{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN Architecture Analysis (Execution-Ready, Updated 2026-02-18)\n",
    "\n",
    "This notebook is aligned with the current TAPE environment updates:\n",
    "- drawdown lambda carry-over + decay\n",
    "- rebalanced penalties with penalty budget cap\n",
    "- intra-step TAPE delta shaping (rolling potential difference)\n",
    "- step-level Sharpe checkpointing\n",
    "- updated episode-length curriculum schedule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0029fb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /content\n",
      "python: /usr/bin/python3\n",
      "hostname: 01c598c115af\n",
      "has /content: True\n",
      "has /mnt/c: False\n"
     ]
    }
   ],
   "source": [
    "import os, sys, socket, pathlib\n",
    "print(\"cwd:\", pathlib.Path.cwd())\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"hostname:\", socket.gethostname())\n",
    "print(\"has /content:\", os.path.exists(\"/content\"))\n",
    "print(\"has /mnt/c:\", os.path.exists(\"/mnt/c\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7338f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/adaptive_portfolio_rl # or /content/drive/MyDrive/adaptive_portfolio_rl'\n",
      "/content\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    }
   ],
   "source": [
    "%cd /content/adaptive_portfolio_rl   # or /content/drive/MyDrive/adaptive_portfolio_rl\n",
    "!git status\n",
    "!git add src/environment_tape_rl.py src/config.py src/train_rl.py src/notebook_helpers/tcn_phase1.py\n",
    "!git commit -m \"Reward shaping: signed terminal TAPE + neutral band + Gate A wiring/logging\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60545e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd /content/adaptive_portfolio_rl\n",
    "#!ls -la data\n",
    "#!find . -type d -name \"__pycache__\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd /content/adaptive_portfolio_rl\n",
    "#!rm -f data/daily_ohlcv_assets.csv\n",
    "#!rm -f data/master_features_NORMALIZED.csv\n",
    "#!rm -f data/processed_daily_macro_features.csv\n",
    "#!find . -type d -name \"__pycache__\" -prune -exec rm -rf {} +\n",
    "#!ls -la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell A: sync repo\n",
    "import os\n",
    "if not os.path.exists(\"/content/adaptive_portfolio_rl/.git\"):\n",
    "    !git clone https://github.com/Dave-DKings/tape_tcn_project.git /content/adaptive_portfolio_rl\n",
    "%cd /content/adaptive_portfolio_rl\n",
    "!git fetch origin\n",
    "!git reset --hard origin/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf54c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell B: install project requirements (Colab-safe pinned stack)\n",
    "#%pip install -q -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f36d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -q jedi==0.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe992e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, tensorflow as tf\n",
    "import pandas_ta_classic as ta_classic\n",
    "print('Versions:', np.__version__, pd.__version__, tf.__version__)\n",
    "print('TF GPUs visible:', tf.config.list_physical_devices('GPU'))\n",
    "!pip check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP: PROJECT ROOT, IMPORTS, REPRODUCIBILITY\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import importlib\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Resolve project root robustly\n",
    "project_root = Path.cwd()\n",
    "if project_root.name != 'adaptive_portfolio_rl':\n",
    "    if (project_root / 'adaptive_portfolio_rl').exists():\n",
    "        project_root = project_root / 'adaptive_portfolio_rl'\n",
    "    elif (project_root.parent / 'adaptive_portfolio_rl').exists():\n",
    "        project_root = project_root.parent / 'adaptive_portfolio_rl'\n",
    "\n",
    "# Ensure imports resolve to this project only\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Scientific stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "# IMPORTANT: force-reload local modules so config edits take effect without stale state\n",
    "import src.config as config_module\n",
    "import src.data_utils as data_utils_module\n",
    "import src.notebook_helpers.tcn_phase1 as tcn_phase1_module\n",
    "importlib.reload(config_module)\n",
    "importlib.reload(data_utils_module)\n",
    "importlib.reload(tcn_phase1_module)\n",
    "\n",
    "# Project imports (from freshly reloaded modules)\n",
    "from src.data_utils import DataProcessor\n",
    "from src.config import get_active_config, PROFILE_BALANCED_GROWTH, ASSET_TICKERS\n",
    "from src.reproducibility_helper import set_all_seeds\n",
    "from src.csv_logger import CSVLogger\n",
    "from src.notebook_helpers.tcn_phase1 import (\n",
    "    identify_covariance_columns,\n",
    "    Phase1Dataset,\n",
    "    run_experiment6_tape,\n",
    "    evaluate_experiment6_checkpoint,\n",
    "    create_experiment6_result_stub,\n",
    "    load_training_metadata_into_config,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_all_seeds(RANDOM_SEED, deterministic=True)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"src.environment_tape_rl\").setLevel(logging.WARNING)\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"TF GPUs:\", gpus)\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print('âœ… Setup complete')\n",
    "print('Project root:', project_root)\n",
    "print('Config module path:', config_module.__file__)\n",
    "print('Active fetch range from module:', config_module.DATA_FETCH_START_DATE, '->', config_module.DATA_FETCH_END_DATE)\n",
    "print('Active analysis range from module:', config_module.ANALYSIS_START_DATE, '->', config_module.ANALYSIS_END_DATE)\n",
    "print('Train split end from module:', config_module.TRAIN_TEST_SPLIT_DATE)\n",
    "print('TensorFlow:', tf.__version__)\n",
    "print('NumPy:', np.__version__)\n",
    "print('Pandas:', pd.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa22329",
   "metadata": {},
   "source": [
    "## 2) Config and Run Controls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD ACTIVE CONFIG + CURRENT CONTROL SNAPSHOT\n",
    "# ============================================================================\n",
    "config = get_active_config('phase1')\n",
    "\n",
    "# Force a single source-of-truth date window for this notebook run\n",
    "# (keeps paper training split, extends test window to ~3 years)\n",
    "config['DATA_FETCH_START_DATE'] = '2003-09-02'\n",
    "config['DATA_FETCH_END_DATE'] = '2024-09-01'\n",
    "config['ANALYSIS_START_DATE'] = '2003-09-02'\n",
    "config['ANALYSIS_END_DATE'] = '2024-09-01'\n",
    "config['TRAIN_TEST_SPLIT_DATE'] = '2021-09-01'\n",
    "\n",
    "print('Forced notebook date window:', config['DATA_FETCH_START_DATE'], '->', config['DATA_FETCH_END_DATE'])\n",
    "print('Forced analysis window:', config['ANALYSIS_START_DATE'], '->', config['ANALYSIS_END_DATE'])\n",
    "print('Forced train split end:', config['TRAIN_TEST_SPLIT_DATE'])\n",
    "\n",
    "# Keep defaults from config unless explicitly changed below\n",
    "config['agent_params']['actor_critic_type'] = 'TCN'\n",
    "config['agent_params']['evaluation_mode'] = config['agent_params'].get('evaluation_mode', 'mode')\n",
    "config['training_params']['update_log_interval'] = 1\n",
    "\n",
    "ppo = config['agent_params'].get('ppo_params', {})\n",
    "env = config.get('environment_params', {})\n",
    "dd = env.get('drawdown_constraint', {})\n",
    "tp = config.get('training_params', {})\n",
    "\n",
    "print('CONFIG SNAPSHOT')\n",
    "print('Phase:', config['phase_name'])\n",
    "print('Tickers:', config['ASSET_TICKERS'])\n",
    "print('Date range:', config['ANALYSIS_START_DATE'], '->', config['ANALYSIS_END_DATE'])\n",
    "print('Architecture:', config['agent_params']['actor_critic_type'])\n",
    "print('TCN filters:', config['agent_params'].get('tcn_filters'))\n",
    "print('Dirichlet activation:', config['agent_params'].get('dirichlet_alpha_activation'))\n",
    "print('PPO: epochs=', ppo.get('num_ppo_epochs'), 'clip=', ppo.get('policy_clip'), 'actor_lr=', ppo.get('actor_lr'), 'critic_lr=', ppo.get('critic_lr'), 'target_kl=', ppo.get('target_kl'))\n",
    "\n",
    "print()\n",
    "print('REWARD + RISK CONTROLS')\n",
    "print('target_turnover=', env.get('target_turnover'), 'turnover_penalty_scalar=', env.get('turnover_penalty_scalar'))\n",
    "print('concentration_penalty_scalar=', env.get('concentration_penalty_scalar'), 'top_weight_penalty_scalar=', env.get('top_weight_penalty_scalar'))\n",
    "print('action_realization_penalty_scalar=', env.get('action_realization_penalty_scalar'))\n",
    "print('penalty_budget_ratio=', env.get('penalty_budget_ratio'))\n",
    "print('drawdown: penalty_coef=', dd.get('penalty_coef'), 'lambda_floor=', dd.get('lambda_floor'), 'lambda_carry_decay=', dd.get('lambda_carry_decay'))\n",
    "print('drawdown: target=', dd.get('target'), 'tolerance=', dd.get('tolerance'), 'lambda_max=', dd.get('lambda_max'))\n",
    "\n",
    "print()\n",
    "print('INTRA-STEP TAPE DELTA')\n",
    "print('enabled=', env.get('intra_step_tape_delta_enabled'), 'window=', env.get('intra_step_tape_delta_window'), 'min_history=', env.get('intra_step_tape_delta_min_history'))\n",
    "print('beta=', env.get('intra_step_tape_delta_beta'), 'clip=', env.get('intra_step_tape_delta_clip'))\n",
    "\n",
    "print()\n",
    "print('CHECKPOINT RULES')\n",
    "print('tape_checkpoint_threshold=', tp.get('tape_checkpoint_threshold'))\n",
    "print('periodic_checkpoint_every_steps=', tp.get('periodic_checkpoint_every_steps'))\n",
    "print('high_watermark_checkpoint_enabled=', tp.get('high_watermark_checkpoint_enabled'), 'threshold=', tp.get('high_watermark_sharpe_threshold'))\n",
    "print('step_sharpe_checkpoint_enabled=', tp.get('step_sharpe_checkpoint_enabled'), 'threshold=', tp.get('step_sharpe_checkpoint_threshold'))\n",
    "\n",
    "print()\n",
    "print('CURRICULUM')\n",
    "print('episode_length_curriculum_schedule=', tp.get('episode_length_curriculum_schedule'))\n",
    "print('turnover_penalty_curriculum=', tp.get('turnover_penalty_curriculum'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL OVERRIDES (SAFE, CURRENT-ALIGNED)\n",
    "# ============================================================================\n",
    "APPLY_OVERRIDES = False\n",
    "\n",
    "if APPLY_OVERRIDES:\n",
    "    # Compact run controls\n",
    "    config['training_params']['max_total_timesteps'] = 30_000\n",
    "    config['training_params']['timesteps_per_ppo_update'] = 504\n",
    "\n",
    "    # PPO stability controls\n",
    "    ppo = config['agent_params']['ppo_params']\n",
    "    ppo.update({\n",
    "        'policy_clip': 0.10,\n",
    "        'num_ppo_epochs': 4,\n",
    "        'actor_lr': 2e-5,\n",
    "        'critic_lr': 3e-4,\n",
    "        'target_kl': 0.015,\n",
    "        'kl_stop_multiplier': 1.2,\n",
    "        'minibatches_before_kl_stop': 1,\n",
    "    })\n",
    "\n",
    "    # Full-horizon episodes (disable episode-length curriculum)\n",
    "    config['training_params']['use_episode_length_curriculum'] = False\n",
    "    config['training_params']['episode_length_curriculum_schedule'] = [\n",
    "        {'threshold': 0, 'limit': None},\n",
    "    ]\n",
    "\n",
    "    # Turnover / reward controls\n",
    "    env = config['environment_params']\n",
    "    env['target_turnover'] = 0.60\n",
    "    env['turnover_target_band'] = 0.20\n",
    "    env['turnover_penalty_scalar'] = 1.5\n",
    "    env['dsr_scalar'] = 2.0\n",
    "    config['training_params']['evaluation_turnover_penalty_scalar'] = 1.5\n",
    "\n",
    "    # Penalty rebalancing\n",
    "    env['concentration_penalty_scalar'] = 2.0\n",
    "    env['concentration_target_hhi'] = 0.14\n",
    "    env['top_weight_penalty_scalar'] = 1.5\n",
    "    env['target_top_weight'] = 0.22\n",
    "    env['action_realization_penalty_scalar'] = 0.5\n",
    "    env['penalty_budget_ratio'] = 1.25\n",
    "\n",
    "    # Drawdown controller (gentler early adaptation)\n",
    "    dd = env['drawdown_constraint']\n",
    "    dd.update({\n",
    "        'enabled': True,\n",
    "        'target': 0.18,\n",
    "        'penalty_coef': 1.5,\n",
    "        'dual_learning_rate': 0.10,\n",
    "        'lambda_init': 0.50,\n",
    "        'lambda_floor': 0.00,\n",
    "        'lambda_max': 5.0,\n",
    "        'lambda_carry_decay': 0.7,\n",
    "        'tolerance': -0.015,\n",
    "        'penalty_reference': 'trigger_boundary',\n",
    "        'cooling_rate': 0.35,\n",
    "    })\n",
    "\n",
    "    # Intra-step TAPE delta shaping (reduced)\n",
    "    env['intra_step_tape_delta_enabled'] = True\n",
    "    env['intra_step_tape_delta_window'] = 60\n",
    "    env['intra_step_tape_delta_min_history'] = 20\n",
    "    env['intra_step_tape_delta_beta'] = 0.01\n",
    "    env['intra_step_tape_delta_clip'] = 0.20\n",
    "\n",
    "    # Step-level Sharpe checkpointing (save any step where Sharpe >= 0.5)\n",
    "    config['training_params']['step_sharpe_checkpoint_enabled'] = True\n",
    "    config['training_params']['step_sharpe_checkpoint_threshold'] = 0.5\n",
    "\n",
    "    print('Overrides applied')\n",
    "else:\n",
    "    print('APPLY_OVERRIDES=False (using config defaults)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0319556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VARIANT SETTINGS (TCN FAMILY)\n",
    "# ============================================================================\n",
    "VARIANT_SETTINGS = {\n",
    "    'TCN': {\n",
    "        'actor_critic_type': 'TCN',\n",
    "        'use_attention': False,\n",
    "        'use_fusion': False,\n",
    "        'results_root': Path('tcn_results'),\n",
    "    },\n",
    "    'TCN_ATTENTION': {\n",
    "        'actor_critic_type': 'TCN_ATTENTION',\n",
    "        'use_attention': True,\n",
    "        'use_fusion': False,\n",
    "        'results_root': Path('tcn_att_results'),\n",
    "    },\n",
    "    'TCN_FUSION': {\n",
    "        'actor_critic_type': 'TCN_FUSION',\n",
    "        'use_attention': False,\n",
    "        'use_fusion': True,\n",
    "        'results_root': Path('tcn_fusion_results'),\n",
    "    },\n",
    "}\n",
    "\n",
    "ACTIVE_VARIANT = 'TCN_FUSION'  # change to: TCN, TCN_ATTENTION, TCN_FUSION\n",
    "\n",
    "if ACTIVE_VARIANT not in VARIANT_SETTINGS:\n",
    "    raise ValueError(f'Unsupported ACTIVE_VARIANT: {ACTIVE_VARIANT}')\n",
    "\n",
    "v = VARIANT_SETTINGS[ACTIVE_VARIANT]\n",
    "config['agent_params']['actor_critic_type'] = v['actor_critic_type']\n",
    "config['agent_params']['use_attention'] = v['use_attention']\n",
    "config['agent_params']['use_fusion'] = v['use_fusion']\n",
    "\n",
    "config['training_params']['use_episode_length_curriculum'] = False\n",
    "config['training_params']['episode_length_curriculum_schedule'] = [\n",
    "    {'threshold': 0, 'limit': None},\n",
    "]\n",
    "\n",
    "\n",
    "LATEST_VARIANT = ACTIVE_VARIANT\n",
    "LATEST_RESULTS_ROOT = str(v['results_root'])\n",
    "\n",
    "print('âœ… Variant applied:', ACTIVE_VARIANT)\n",
    "print('results_root:', LATEST_RESULTS_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a236e",
   "metadata": {},
   "source": [
    "## 3) Data Pipeline (Features + Actuarial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be89b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_active_config(\"phase1\")\n",
    "\n",
    "# Optional guard for legacy/invalid macro codes in old configs.\n",
    "# Current config is already updated, so this is usually a no-op.\n",
    "macro_cfg = config[\"feature_params\"][\"macro_data\"]\n",
    "invalid_legacy_codes = {\"NAPM\", \"MOVEINDEX\", \"ISM/MAN_PMI\", \"MOVE\"}\n",
    "before = len(macro_cfg.get(\"fred_series_config\", []))\n",
    "macro_cfg[\"fred_series_config\"] = [\n",
    "    s for s in macro_cfg.get(\"fred_series_config\", [])\n",
    "    if s.get(\"code\") not in invalid_legacy_codes\n",
    "]\n",
    "after = len(macro_cfg.get(\"fred_series_config\", []))\n",
    "print(f\"FRED series config: {before} -> {after} (removed legacy invalid codes: {before - after})\")\n",
    "\n",
    "# Keep legacy engineered-column disables for backward-compatible runs.\n",
    "legacy_macro_columns = {\"ISM_MAN_PMI_level\", \"ISM_MAN_PMI_diff\", \"MOVE_level\", \"MOVE_zscore\"}\n",
    "disabled = set(config[\"feature_params\"][\"feature_selection\"][\"disabled_features\"])\n",
    "newly_added = sorted(list(legacy_macro_columns - disabled))\n",
    "disabled.update(legacy_macro_columns)\n",
    "config[\"feature_params\"][\"feature_selection\"][\"disabled_features\"] = sorted(disabled)\n",
    "if newly_added:\n",
    "    print(\"Added legacy macro disables:\", newly_added)\n",
    "else:\n",
    "    print(\"Legacy macro disable set already present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING + FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "processor = DataProcessor(config)\n",
    "\n",
    "print('=' * 80)\n",
    "print('LOADING AND PROCESSING DATA')\n",
    "print('=' * 80)\n",
    "\n",
    "FORCE_DATA_REFRESH = False  # Set True once when you want to rebuild cache for new date bounds\n",
    "raw_df = processor.load_ohlcv_data(\n",
    "    start_date=config['DATA_FETCH_START_DATE'],\n",
    "    end_date=config['DATA_FETCH_END_DATE'],\n",
    "    force_download=FORCE_DATA_REFRESH,\n",
    ")\n",
    "print('Raw shape:', raw_df.shape)\n",
    "print('Raw dates:', raw_df['Date'].min(), 'â†’', raw_df['Date'].max())\n",
    "\n",
    "# Core feature pipeline\n",
    "df = processor.calculate_log_returns(raw_df, periods=[1, 5, 10, 21])\n",
    "df = processor.calculate_return_statistics(df, window=21)\n",
    "df = processor.calculate_technical_indicators(df)\n",
    "df = processor.calculate_dynamic_covariance_features(df)\n",
    "df = processor.add_regime_features(df)\n",
    "df = processor.add_fundamental_features(df)\n",
    "\n",
    "macro_cfg = config.get('feature_params', {}).get('macro_data')\n",
    "if macro_cfg is not None:\n",
    "    macro_df, macro_cols = processor._build_macro_feature_frame(macro_cfg, df['Date'].min(), df['Date'].max())\n",
    "    if macro_df is not None and macro_cols:\n",
    "        df = df.merge(macro_df, on='Date', how='left')\n",
    "        print(f'Macro features added: {len(macro_cols)}')\n",
    "\n",
    "df = processor.add_quant_alpha_features(df)\n",
    "df = processor.add_cross_sectional_features(df)\n",
    "df = processor.add_actuarial_features(df)\n",
    "\n",
    "master_df = df.copy()\n",
    "feature_cols = processor.get_feature_columns('phase1')\n",
    "present_feature_cols = [c for c in feature_cols if c in master_df.columns]\n",
    "\n",
    "print('Final master_df shape:', master_df.shape)\n",
    "print('Expected feature cols:', len(feature_cols))\n",
    "print('Present feature cols :', len(present_feature_cols))\n",
    "\n",
    "nan_counts = master_df[present_feature_cols].isna().sum()\n",
    "nan_cols = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
    "print('Feature columns with NaN:', len(nan_cols))\n",
    "if len(nan_cols) > 0:\n",
    "    display(nan_cols.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cdab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_cols = [c for c in master_df.columns if c.startswith(\"Actuarial_\")]\n",
    "print(\"Actuarial columns present:\", len(act_cols))\n",
    "print(act_cols[:10])\n",
    "\n",
    "if act_cols:\n",
    "    print(master_df[act_cols].describe().T[[\"mean\",\"std\",\"min\",\"max\"]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db599ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIXED DATE SPLIT (TRAIN/TEST)\n",
    "# ============================================================================\n",
    "analysis_start = pd.Timestamp(config.get('ANALYSIS_START_DATE', '2003-09-02'))\n",
    "analysis_end = pd.Timestamp(config.get('ANALYSIS_END_DATE', '2024-09-01'))\n",
    "\n",
    "train_end_date = pd.Timestamp(config.get('TRAIN_TEST_SPLIT_DATE', '2021-09-01'))\n",
    "test_start_date = train_end_date + pd.Timedelta(days=1)\n",
    "test_end_date = analysis_end\n",
    "\n",
    "all_dates = pd.to_datetime(master_df['Date'])\n",
    "master_df = master_df[(all_dates >= analysis_start) & (all_dates <= analysis_end)].copy()\n",
    "all_dates = pd.to_datetime(master_df['Date'])\n",
    "\n",
    "train_mask = all_dates <= train_end_date\n",
    "test_mask = (all_dates >= test_start_date) & (all_dates <= test_end_date)\n",
    "\n",
    "train_df = master_df[train_mask].copy()\n",
    "test_df = master_df[test_mask].copy()\n",
    "\n",
    "print('Analysis range:', analysis_start.date(), 'â†’', analysis_end.date())\n",
    "print('Train split end:', train_end_date.date(), '| Test start:', test_start_date.date())\n",
    "print('Train:', train_df['Date'].min(), 'â†’', train_df['Date'].max(), f'({len(train_df):,} rows)')\n",
    "print('Test :', test_df['Date'].min(), 'â†’', test_df['Date'].max(), f'({len(test_df):,} rows)')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NORMALIZATION (TRAIN-FIT, TEST-TRANSFORM)\n",
    "# ============================================================================\n",
    "from collections import Counter\n",
    "\n",
    "feature_cols = processor.get_feature_columns('phase1')\n",
    "\n",
    "master_df_normalized, scalers = processor.normalize_features(\n",
    "    master_df,\n",
    "    feature_cols=feature_cols,\n",
    "    train_end_date=train_end_date,\n",
    "    test_start_date=test_start_date,\n",
    "    existing_scalers=None,\n",
    "    scaler_type='standard',\n",
    ")\n",
    "\n",
    "actuarial_cols = [c for c in master_df_normalized.columns if c.startswith('Actuarial_')]\n",
    "\n",
    "method_counter = Counter()\n",
    "for c in feature_cols:\n",
    "    spec = scalers.get(c)\n",
    "    if isinstance(spec, dict):\n",
    "        method_counter[str(spec.get('method', 'standard'))] += 1\n",
    "    elif spec is None:\n",
    "        method_counter['missing'] += 1\n",
    "    else:\n",
    "        method_counter['legacy_scaler'] += 1\n",
    "\n",
    "print('âœ… Normalization complete')\n",
    "print('Normalized shape:', master_df_normalized.shape)\n",
    "print('Actuarial columns:', actuarial_cols)\n",
    "print('Normalization strategy counts:', dict(method_counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BUILD PHASE1 DATASET CONTAINER\n",
    "# ============================================================================\n",
    "all_dates_norm = pd.to_datetime(master_df_normalized['Date'])\n",
    "train_df_norm = master_df_normalized[all_dates_norm <= train_end_date].copy()\n",
    "test_df_norm = master_df_normalized[(all_dates_norm >= test_start_date) & (all_dates_norm <= test_end_date)].copy()\n",
    "\n",
    "covariance_columns = identify_covariance_columns(master_df_normalized.columns)\n",
    "\n",
    "phase1_data = Phase1Dataset(\n",
    "    master_df=master_df_normalized,\n",
    "    train_df=train_df_norm,\n",
    "    test_df=test_df_norm,\n",
    "    scalers=scalers,\n",
    "    train_end_date=train_end_date,\n",
    "    test_start_date=test_start_date,\n",
    "    covariance_columns=covariance_columns,\n",
    "    data_processor=processor,\n",
    ")\n",
    "\n",
    "print('âœ… Phase1Dataset ready')\n",
    "print('Train shape:', phase1_data.train_df.shape)\n",
    "print('Test shape :', phase1_data.test_df.shape)\n",
    "print('Covariance features:', len(covariance_columns))\n",
    "print('Assets:', sorted(phase1_data.master_df['Ticker'].dropna().unique().tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01baf8e",
   "metadata": {},
   "source": [
    "## 4) Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional stability tweaks (keep commented unless needed)\n",
    "# config['training_params']['timesteps_per_ppo_update'] = 504\n",
    "# ppo = config['agent_params']['ppo_params']\n",
    "# ppo['batch_size_ppo'] = 512\n",
    "# ppo['num_ppo_epochs'] = 5\n",
    "# ppo['actor_lr'] = 5e-5\n",
    "# ppo['critic_lr'] = 1e-4\n",
    "# ppo['policy_clip'] = 0.15\n",
    "# ppo['target_kl'] = 0.02\n",
    "# ppo['entropy_coef'] = 0.01\n",
    "# config['environment_params']['penalty_budget_ratio'] = 1.25\n",
    "# config['environment_params']['intra_step_tape_delta_beta'] = 0.10\n",
    "# config['environment_params']['intra_step_tape_delta_clip'] = 0.20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN ACTIVE VARIANT\n",
    "# ============================================================================\n",
    "RUN_TRAINING = True\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    train_cfg = config['training_params']\n",
    "    print('ðŸš€ Starting training')\n",
    "    print('Variant:', ACTIVE_VARIANT)\n",
    "    print('max_total_timesteps:', train_cfg['max_total_timesteps'])\n",
    "    print('timesteps_per_ppo_update:', train_cfg['timesteps_per_ppo_update'])\n",
    "\n",
    "    experiment6 = run_experiment6_tape(\n",
    "        phase1_data=phase1_data,\n",
    "        config=config,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        csv_logger_cls=CSVLogger,\n",
    "        use_covariance=True,\n",
    "        architecture=config['agent_params']['actor_critic_type'],\n",
    "        timesteps_per_update=train_cfg['timesteps_per_ppo_update'],\n",
    "        max_total_timesteps=train_cfg['max_total_timesteps'],\n",
    "    )\n",
    "\n",
    "    print('âœ… Training complete')\n",
    "    print('checkpoint_prefix:', experiment6.checkpoint_path)\n",
    "else:\n",
    "    print('â„¹ï¸ RUN_TRAINING=False (set True to train)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ab856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QUICK TRAINING LOG INSPECTION (LATEST)\n",
    "# ============================================================================\n",
    "logs_dir = Path(LATEST_RESULTS_ROOT) / 'logs'\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "episodes_files = sorted(logs_dir.glob('*episodes*.csv'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not episodes_files:\n",
    "    print(f'No episodes CSV found in {logs_dir} yet.')\n",
    "else:\n",
    "    epis_path = episodes_files[0]\n",
    "    episodes_df = pd.read_csv(epis_path)\n",
    "    print('Episodes file:', epis_path)\n",
    "    print('Rows:', len(episodes_df))\n",
    "    display(episodes_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bacaf2",
   "metadata": {},
   "source": [
    "## 5) Evaluation (Unified Multi-Track)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: RELOAD TRAINING METADATA (POST-RESTART)\n",
    "# ============================================================================\n",
    "USE_METADATA_RELOAD = False\n",
    "METADATA_PATH = None  # e.g., Path('tcn_results/logs/Exp6_TCN_Enhanced_TAPE_training_YYYYMMDD_HHMMSS_metadata.json')\n",
    "\n",
    "if USE_METADATA_RELOAD:\n",
    "    if METADATA_PATH is None:\n",
    "        logs_dir = Path(LATEST_RESULTS_ROOT) / 'logs'\n",
    "        cand = sorted(logs_dir.glob('*metadata*.json'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        METADATA_PATH = cand[0] if cand else None\n",
    "\n",
    "    if METADATA_PATH and Path(METADATA_PATH).exists():\n",
    "        config = load_training_metadata_into_config(Path(METADATA_PATH), config, verbose=True)\n",
    "        print('âœ… Metadata reloaded from:', METADATA_PATH)\n",
    "    else:\n",
    "        print('âš ï¸ Metadata file not found; continuing with current config.')\n",
    "else:\n",
    "    print('â„¹ï¸ USE_METADATA_RELOAD=False')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdbf5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UNIFIED EVALUATION: det_mode + det_mean + stochastic\n",
    "# ============================================================================\n",
    "RUN_EVAL = True\n",
    "\n",
    "# Model selection\n",
    "MODEL_FAMILY = 'normal'           # normal | rare | clip\n",
    "NORMAL_MODEL_STRATEGY = 'latest'  # latest | final\n",
    "RARE_MODEL_STRATEGY = 'best'      # best | episode\n",
    "CHECKPOINT_EPISODE = 83           # used when rare_model_strategy='episode'\n",
    "CLIP_EPISODE = 83                 # used when model_family='clip'\n",
    "CHECKPOINT_PREFIX_OVERRIDE = None # e.g., 'tcn_fusion_results/exp6_tape_ep83'\n",
    "\n",
    "if RUN_EVAL:\n",
    "    experiment6_stub = create_experiment6_result_stub(\n",
    "        random_seed=RANDOM_SEED,\n",
    "        use_covariance=True,\n",
    "        architecture=config['agent_params']['actor_critic_type'],\n",
    "        checkpoint_path=None,\n",
    "        base_agent_params=config.get('agent_params'),\n",
    "    )\n",
    "\n",
    "    evaluation_stub = evaluate_experiment6_checkpoint(\n",
    "        experiment6_stub,\n",
    "        phase1_data=phase1_data,\n",
    "        config=config,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        model_family=MODEL_FAMILY,\n",
    "        normal_model_strategy=NORMAL_MODEL_STRATEGY,\n",
    "        rare_model_strategy=RARE_MODEL_STRATEGY,\n",
    "        checkpoint_episode=CHECKPOINT_EPISODE,\n",
    "        clip_episode=CLIP_EPISODE,\n",
    "        checkpoint_path_override=CHECKPOINT_PREFIX_OVERRIDE,\n",
    "        num_eval_runs=30,\n",
    "        compare_deterministic_modes=['mode', 'mean'],\n",
    "        stochastic_eval_mode='sample',\n",
    "        sample_actions_stochastic=True,\n",
    "        sample_actions=None,\n",
    "        stochastic_episode_length_limit=252,\n",
    "        save_eval_logs=True,\n",
    "        save_eval_artifacts=True,\n",
    "    )\n",
    "\n",
    "    print('âœ… Evaluation complete')\n",
    "    print('Checkpoint:', evaluation_stub.actor_weights_path)\n",
    "    print('Eval CSV  :', evaluation_stub.eval_results_path)\n",
    "else:\n",
    "    print('â„¹ï¸ RUN_EVAL=False (set True to evaluate)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION ARTIFACT EXPORTS (VARIANT-SCOPED)\n",
    "# ============================================================================\n",
    "from datetime import datetime\n",
    "\n",
    "if 'evaluation_stub' not in globals():\n",
    "    print('Run evaluation first (RUN_EVAL=True).')\n",
    "else:\n",
    "    assets = ASSET_TICKERS + ['Cash']\n",
    "\n",
    "    results_root = Path(globals().get('LATEST_RESULTS_ROOT', 'tcn_results'))\n",
    "    stamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    out_root = results_root / 'model_outputs' / f'eval_{stamp}'\n",
    "    det_out = out_root / 'deterministic'\n",
    "    sto_out = out_root / 'stochastic'\n",
    "    det_out.mkdir(parents=True, exist_ok=True)\n",
    "    sto_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Deterministic artifacts\n",
    "    det_dates = pd.DatetimeIndex(evaluation_stub.env_test_deterministic.dates)\n",
    "\n",
    "    if len(evaluation_stub.deterministic_alphas) > 0:\n",
    "        df_alpha = pd.DataFrame(evaluation_stub.deterministic_alphas, columns=assets)\n",
    "        df_alpha.index = det_dates[:len(df_alpha)]\n",
    "        df_alpha.index.name = 'date'\n",
    "        df_alpha.to_csv(det_out / 'alphas.csv')\n",
    "\n",
    "    if len(evaluation_stub.deterministic_weights) > 0:\n",
    "        df_w = pd.DataFrame(evaluation_stub.deterministic_weights, columns=assets)\n",
    "        df_w.index = det_dates[:len(df_w)]\n",
    "        df_w.index.name = 'date'\n",
    "        df_w.to_csv(det_out / 'weights.csv')\n",
    "\n",
    "    if len(evaluation_stub.deterministic_actions) > 0:\n",
    "        df_a = pd.DataFrame(evaluation_stub.deterministic_actions, columns=assets)\n",
    "        df_a.index = det_dates[:len(df_a)]\n",
    "        df_a.index.name = 'date'\n",
    "        df_a.to_csv(det_out / 'actions.csv')\n",
    "\n",
    "    # Copy eval summary CSV into output root for traceability\n",
    "    eval_csv_path = Path(evaluation_stub.eval_results_path) if evaluation_stub.eval_results_path else None\n",
    "    if eval_csv_path and eval_csv_path.exists():\n",
    "        df_eval = pd.read_csv(eval_csv_path)\n",
    "        df_eval.to_csv(out_root / 'evaluation_summary.csv', index=False)\n",
    "    else:\n",
    "        df_eval = pd.DataFrame()\n",
    "\n",
    "    # Stochastic artifacts\n",
    "    all_dates = pd.DatetimeIndex(evaluation_stub.env_test_random.dates)\n",
    "    actions_rows, weights_rows, alphas_rows = [], [], []\n",
    "\n",
    "    if isinstance(evaluation_stub.stochastic_results, pd.DataFrame) and not evaluation_stub.stochastic_results.empty:\n",
    "        stochastic_results_df = evaluation_stub.stochastic_results.copy()\n",
    "        stochastic_results_df.to_csv(sto_out / 'stochastic_results.csv', index=False)\n",
    "\n",
    "        for i in range(len(stochastic_results_df)):\n",
    "            run_id = int(stochastic_results_df.iloc[i].get('run', i + 1))\n",
    "            start_date = pd.Timestamp(stochastic_results_df.iloc[i]['start_date'])\n",
    "            start_idx = all_dates.get_loc(start_date)\n",
    "\n",
    "            run_actions = evaluation_stub.stochastic_actions[i] if i < len(evaluation_stub.stochastic_actions) else []\n",
    "            run_weights = evaluation_stub.stochastic_weights[i] if i < len(evaluation_stub.stochastic_weights) else []\n",
    "            run_alphas = evaluation_stub.stochastic_alphas[i] if i < len(evaluation_stub.stochastic_alphas) else []\n",
    "\n",
    "            run_dates = all_dates[start_idx:start_idx + len(run_weights)]\n",
    "\n",
    "            if len(run_actions):\n",
    "                dfa = pd.DataFrame(run_actions, columns=assets)\n",
    "                dfa['run'] = run_id\n",
    "                dfa['date'] = run_dates[:len(dfa)]\n",
    "                actions_rows.append(dfa)\n",
    "\n",
    "            if len(run_weights):\n",
    "                dfw = pd.DataFrame(run_weights, columns=assets)\n",
    "                dfw['run'] = run_id\n",
    "                dfw['date'] = run_dates[:len(dfw)]\n",
    "                weights_rows.append(dfw)\n",
    "\n",
    "            if len(run_alphas):\n",
    "                dfl = pd.DataFrame(run_alphas, columns=assets)\n",
    "                dfl['run'] = run_id\n",
    "                dfl['date'] = run_dates[:len(dfl)]\n",
    "                alphas_rows.append(dfl)\n",
    "\n",
    "    if actions_rows:\n",
    "        pd.concat(actions_rows, ignore_index=True).set_index(['run', 'date']).to_csv(sto_out / 'actions_all_runs.csv')\n",
    "    if weights_rows:\n",
    "        pd.concat(weights_rows, ignore_index=True).set_index(['run', 'date']).to_csv(sto_out / 'weights_all_runs.csv')\n",
    "    if alphas_rows:\n",
    "        pd.concat(alphas_rows, ignore_index=True).set_index(['run', 'date']).to_csv(sto_out / 'alphas_all_runs.csv')\n",
    "\n",
    "    # README with current run context\n",
    "    readme_lines = [\n",
    "        '# Evaluation Artifact Export',\n",
    "        '',\n",
    "        f'- Variant results root: `{results_root}`',\n",
    "        f'- Export root: `{out_root}`',\n",
    "        f'- Checkpoint actor: `{evaluation_stub.actor_weights_path}`',\n",
    "        f'- Checkpoint critic: `{evaluation_stub.critic_weights_path}`',\n",
    "        f'- Eval summary CSV: `{evaluation_stub.eval_results_path}`',\n",
    "        f'- Export timestamp: `{stamp}`',\n",
    "        '',\n",
    "        '## Included Files',\n",
    "        '- `deterministic/weights.csv`',\n",
    "        '- `deterministic/actions.csv`',\n",
    "        '- `deterministic/alphas.csv`',\n",
    "        '- `stochastic/stochastic_results.csv` (if stochastic runs were executed)',\n",
    "        '- `stochastic/weights_all_runs.csv`',\n",
    "        '- `stochastic/actions_all_runs.csv`',\n",
    "        '- `stochastic/alphas_all_runs.csv`',\n",
    "        '- `evaluation_summary.csv`',\n",
    "    ]\n",
    "\n",
    "    if not df_eval.empty:\n",
    "        cols = [\n",
    "            'eval_track', 'evaluation_type', 'start_date', 'market_regime',\n",
    "            'mean_concentration_hhi', 'mean_top_weight',\n",
    "            'mean_action_realization_l1', 'max_action_realization_l1'\n",
    "        ]\n",
    "        present = [c for c in cols if c in df_eval.columns]\n",
    "        readme_lines += ['', '## Key Logged Diagnostics (present in summary CSV)', *(f'- `{c}`' for c in present)]\n",
    "\n",
    "    (out_root / 'README.md').write_text('\\n'.join(readme_lines), encoding='utf-8')\n",
    "\n",
    "    print('âœ… Export complete')\n",
    "    print('Export root:', out_root)\n",
    "    print('Deterministic dir:', det_out)\n",
    "    print('Stochastic dir   :', sto_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c70cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVAL CSV DIAGNOSTIC COLUMN CHECK\n",
    "# ============================================================================\n",
    "required_cols = [\n",
    "    'start_date',\n",
    "    'market_regime',\n",
    "    'mean_concentration_hhi',\n",
    "    'mean_top_weight',\n",
    "    'mean_action_realization_l1',\n",
    "    'max_action_realization_l1',\n",
    "]\n",
    "\n",
    "csv_path = None\n",
    "\n",
    "if 'evaluation_stub' in globals() and getattr(evaluation_stub, 'eval_results_path', None):\n",
    "    p = Path(evaluation_stub.eval_results_path)\n",
    "    if p.exists():\n",
    "        csv_path = p\n",
    "\n",
    "if csv_path is None:\n",
    "    root = Path(globals().get('LATEST_RESULTS_ROOT', 'tcn_results'))\n",
    "    logs_dir = root / 'logs'\n",
    "    candidates = sorted(logs_dir.glob('*_eval_*.csv'), key=lambda x: x.stat().st_mtime, reverse=True) if logs_dir.exists() else []\n",
    "    csv_path = candidates[0] if candidates else None\n",
    "\n",
    "if csv_path is None:\n",
    "    print('âš ï¸ No evaluation CSV found. Run evaluation first.')\n",
    "else:\n",
    "    df_eval = pd.read_csv(csv_path)\n",
    "    present = [c for c in required_cols if c in df_eval.columns]\n",
    "    missing = [c for c in required_cols if c not in df_eval.columns]\n",
    "\n",
    "    print('ðŸ“‚ Eval CSV:', csv_path)\n",
    "    print('Rows:', len(df_eval))\n",
    "    print('Required columns present:', len(present), '/', len(required_cols))\n",
    "\n",
    "    if missing:\n",
    "        print('âŒ Missing columns:', missing)\n",
    "    else:\n",
    "        print('âœ… All required diagnostic columns are present.')\n",
    "\n",
    "    show_cols = ['eval_track', 'evaluation_type'] + [c for c in required_cols if c in df_eval.columns]\n",
    "    show_cols = [c for c in show_cols if c in df_eval.columns]\n",
    "    if show_cols:\n",
    "        display(df_eval[show_cols].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTICS SUMMARY\n",
    "# ============================================================================\n",
    "if 'evaluation_stub' not in globals():\n",
    "    print('Run evaluation first (RUN_EVAL=True).')\n",
    "else:\n",
    "    # stochastic summary\n",
    "    stoch = evaluation_stub.stochastic_results.copy()\n",
    "    if stoch is not None and not stoch.empty:\n",
    "        cols = [\n",
    "            'total_return', 'annualized_return', 'sharpe_ratio', 'sortino_ratio',\n",
    "            'max_drawdown', 'volatility', 'turnover', 'win_rate'\n",
    "        ]\n",
    "        cols = [c for c in cols if c in stoch.columns]\n",
    "        print('Stochastic summary:')\n",
    "        display(stoch[cols].describe().T)\n",
    "\n",
    "    # deterministic diagnostics\n",
    "    acts = np.asarray(evaluation_stub.deterministic_actions)\n",
    "    alps = np.asarray(evaluation_stub.deterministic_alphas)\n",
    "\n",
    "    action_uniques = int(np.unique(np.round(acts, 6), axis=0).shape[0]) if acts.size else 0\n",
    "    alpha_le1_frac = float(np.mean(alps <= 1.0)) if alps.size else 0.0\n",
    "    argmax_uniques = int(np.unique(np.argmax(alps, axis=1)).shape[0]) if (alps.ndim == 2 and len(alps) > 0) else 0\n",
    "\n",
    "    print('Deterministic diagnostics:')\n",
    "    print(' action_uniques      =', action_uniques)\n",
    "    print(' alpha<=1 fraction   =', alpha_le1_frac)\n",
    "    print(' argmax_alpha_uniques=', argmax_uniques)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIXED STRESS-WINDOW EVALUATION (2 WINDOWS)\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "from dataclasses import replace\n",
    "\n",
    "STRESS_WINDOWS = [\n",
    "    ('2020-02-20', '2020-05-29', 'COVID crash + rebound'),\n",
    "    ('2022-01-03', '2022-12-30', 'Rate-hike bear year'),\n",
    "]\n",
    "\n",
    "def subset_phase1_test_window(phase1_data, start_date, end_date):\n",
    "    s = pd.Timestamp(start_date)\n",
    "    e = pd.Timestamp(end_date)\n",
    "    df = phase1_data.test_df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    win = df[(df['Date'] >= s) & (df['Date'] <= e)].copy()\n",
    "    if win.empty:\n",
    "        raise ValueError(f'No rows in test_df for {start_date} -> {end_date}')\n",
    "    return replace(\n",
    "        phase1_data,\n",
    "        test_df=win,\n",
    "        test_start_date=win['Date'].min(),\n",
    "        train_end_date=win['Date'].max(),\n",
    "    )\n",
    "\n",
    "fixed_rows = []\n",
    "for start, end, label in STRESS_WINDOWS:\n",
    "    phase_win = subset_phase1_test_window(phase1_data, start, end)\n",
    "\n",
    "    ev = evaluate_experiment6_checkpoint(\n",
    "        experiment6_stub,\n",
    "        phase1_data=phase_win,\n",
    "        config=config,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        checkpoint_path_override=CHECKPOINT_PREFIX_OVERRIDE,\n",
    "        deterministic_eval_mode='mode',\n",
    "        num_eval_runs=0,\n",
    "        stochastic_eval_mode='sample',\n",
    "        save_eval_logs=False,\n",
    "        save_eval_artifacts=False,\n",
    "    )\n",
    "\n",
    "    m = ev.deterministic_metrics or {}\n",
    "    fixed_rows.append({\n",
    "        'window_label': label,\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'days_traded': len(ev.deterministic_portfolio) - 1 if len(ev.deterministic_portfolio) else 0,\n",
    "        'total_return': m.get('total_return'),\n",
    "        'annualized_return': m.get('annualized_return'),\n",
    "        'sharpe': m.get('sharpe_ratio'),\n",
    "        'sortino': m.get('sortino_ratio'),\n",
    "        'max_drawdown': m.get('max_drawdown_abs', m.get('max_drawdown')),\n",
    "        'volatility': m.get('volatility'),\n",
    "        'turnover': m.get('turnover'),\n",
    "        'win_rate': m.get('win_rate'),\n",
    "    })\n",
    "\n",
    "fixed_df = pd.DataFrame(fixed_rows)\n",
    "display(fixed_df.sort_values('start'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b0711",
   "metadata": {},
   "source": [
    "## 6) Checkpoint Scan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7107fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHECKPOINT SCANNER (DETERMINISTIC)\n",
    "# ============================================================================\n",
    "import re\n",
    "\n",
    "\n",
    "def evaluate_checkpoint_range_deterministic(\n",
    "    episode_range=(2, 300),\n",
    "    results_root=None,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    deterministic_eval_mode='mode',\n",
    "):\n",
    "    low, high = episode_range\n",
    "    base_root = Path(results_root) if results_root else Path(LATEST_RESULTS_ROOT)\n",
    "\n",
    "    checkpoints = {}\n",
    "    for root in [base_root, base_root / 'rare_models']:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        for actor_path in root.glob('*_actor.weights.h5'):\n",
    "            m = re.search(r'_ep(\\d+)', actor_path.name)\n",
    "            if not m:\n",
    "                continue\n",
    "            ep = int(m.group(1))\n",
    "            if low <= ep <= high:\n",
    "                checkpoints[ep] = actor_path\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(f'No checkpoints found in {base_root} for range {episode_range}.')\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "    for ep, actor_path in sorted(checkpoints.items()):\n",
    "        prefix = str(actor_path).replace('_actor.weights.h5', '')\n",
    "\n",
    "        stub = create_experiment6_result_stub(\n",
    "            random_seed=random_seed,\n",
    "            use_covariance=True,\n",
    "            architecture=config['agent_params']['actor_critic_type'],\n",
    "            checkpoint_path=prefix,\n",
    "            base_agent_params=config.get('agent_params'),\n",
    "        )\n",
    "\n",
    "        ev = evaluate_experiment6_checkpoint(\n",
    "            experiment6=stub,\n",
    "            phase1_data=phase1_data,\n",
    "            config=config,\n",
    "            random_seed=random_seed,\n",
    "            checkpoint_path_override=prefix,\n",
    "            model_family='normal',\n",
    "            normal_model_strategy='latest',\n",
    "            num_eval_runs=0,\n",
    "            deterministic_eval_mode=deterministic_eval_mode,\n",
    "            save_eval_logs=False,\n",
    "            save_eval_artifacts=False,\n",
    "        )\n",
    "\n",
    "        m = ev.deterministic_metrics or {}\n",
    "        rows.append({\n",
    "            'episode': ep,\n",
    "            'checkpoint_prefix': prefix,\n",
    "            'sharpe': m.get('sharpe_ratio', float('nan')),\n",
    "            'total_return': m.get('total_return', float('nan')),\n",
    "            'max_drawdown': m.get('max_drawdown_abs', m.get('max_drawdown', float('nan'))),\n",
    "            'turnover': m.get('turnover', float('nan')),\n",
    "        })\n",
    "\n",
    "    df_scores = pd.DataFrame(rows).sort_values('sharpe', ascending=False)\n",
    "    return df_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a68bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN CHECKPOINT SCAN\n",
    "# ============================================================================\n",
    "RUN_SCAN = True\n",
    "\n",
    "if RUN_SCAN:\n",
    "    results_root = Path(globals().get('LATEST_RESULTS_ROOT', 'tcn_results'))\n",
    "    print('Scanning:', results_root)\n",
    "    scan_df = evaluate_checkpoint_range_deterministic(\n",
    "        episode_range=(8, 100),\n",
    "        results_root=results_root,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        deterministic_eval_mode='mean',\n",
    "    )\n",
    "    display(scan_df.head(20) if scan_df is not None else None)\n",
    "\n",
    "    step_dir = results_root / 'step_sharpe_checkpoints'\n",
    "    if step_dir.exists():\n",
    "        step_files = sorted(step_dir.glob('*_actor.weights.h5'))\n",
    "        print(f'Step-Sharpe checkpoints found: {len(step_files)}')\n",
    "        for p in step_files[:10]:\n",
    "            print(' -', p.name)\n",
    "    else:\n",
    "        print('No step_sharpe_checkpoints directory yet.')\n",
    "else:\n",
    "    print('â„¹ï¸ RUN_SCAN=False')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03e629",
   "metadata": {},
   "source": [
    "## 7) Overfit Monitor (Train-Test Gap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OVERFIT MONITOR HELPERS\n",
    "# ============================================================================\n",
    "import re\n",
    "from dataclasses import replace\n",
    "\n",
    "\n",
    "def _infer_results_root_for_notebook(cfg):\n",
    "    arch = cfg.get('agent_params', {}).get('actor_critic_type', 'TCN').upper()\n",
    "    use_attention = bool(cfg.get('agent_params', {}).get('use_attention', False))\n",
    "    use_fusion = bool(cfg.get('agent_params', {}).get('use_fusion', False))\n",
    "    if arch.startswith('TCN'):\n",
    "        if use_fusion:\n",
    "            return Path('tcn_fusion_results')\n",
    "        if use_attention:\n",
    "            return Path('tcn_att_results')\n",
    "        return Path('tcn_results')\n",
    "    return Path('tcn_results')\n",
    "\n",
    "\n",
    "def _discover_checkpoint_prefixes(results_root, episode_range=(1, 9999), include_rare=True):\n",
    "    lo, hi = episode_range\n",
    "    roots = [Path(results_root)]\n",
    "    if include_rare:\n",
    "        roots.append(Path(results_root) / 'rare_models')\n",
    "\n",
    "    prefixes = {}\n",
    "    for root in roots:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        for actor in root.glob('*_actor.weights.h5'):\n",
    "            m = re.search(r'_ep(\\d+)', actor.name)\n",
    "            if not m:\n",
    "                continue\n",
    "            ep = int(m.group(1))\n",
    "            if lo <= ep <= hi:\n",
    "                prefixes[ep] = str(actor).replace('_actor.weights.h5', '')\n",
    "\n",
    "    return [(ep, prefixes[ep]) for ep in sorted(prefixes.keys())]\n",
    "\n",
    "\n",
    "def _subset_phase1_for_eval(phase1_data, split='test'):\n",
    "    split = str(split).lower().strip()\n",
    "    if split not in {'train', 'test'}:\n",
    "        raise ValueError(f'split must be train or test, got: {split}')\n",
    "\n",
    "    eval_df = phase1_data.train_df.copy() if split == 'train' else phase1_data.test_df.copy()\n",
    "    start_date = pd.to_datetime(eval_df['Date']).min()\n",
    "    end_date = pd.to_datetime(eval_df['Date']).max()\n",
    "\n",
    "    return replace(\n",
    "        phase1_data,\n",
    "        test_df=eval_df,\n",
    "        test_start_date=start_date,\n",
    "        train_end_date=end_date,\n",
    "    )\n",
    "\n",
    "\n",
    "def _diagnostics_from_eval(ev):\n",
    "    acts = np.asarray(ev.deterministic_actions)\n",
    "    alps = np.asarray(ev.deterministic_alphas)\n",
    "    action_uniques = int(np.unique(np.round(acts, 6), axis=0).shape[0]) if acts.size else 0\n",
    "    alpha_le1_fraction = float(np.mean(alps <= 1.0)) if alps.size else 0.0\n",
    "    argmax_alpha_uniques = int(np.unique(np.argmax(alps, axis=1)).shape[0]) if (alps.ndim == 2 and len(alps) > 0) else 0\n",
    "    return action_uniques, alpha_le1_fraction, argmax_alpha_uniques\n",
    "\n",
    "\n",
    "def run_checkpoint_overfit_monitor(\n",
    "    phase1_data,\n",
    "    config,\n",
    "    random_seed,\n",
    "    episode_range=(1, 300),\n",
    "    deterministic_modes=('mode', 'mean'),\n",
    "    eval_splits=('train', 'test'),\n",
    "    results_root=None,\n",
    "    include_rare=False,\n",
    "    save_csv=True,\n",
    "):\n",
    "    results_root = Path(results_root) if results_root else _infer_results_root_for_notebook(config)\n",
    "    ckpts = _discover_checkpoint_prefixes(results_root, episode_range=episode_range, include_rare=include_rare)\n",
    "    if not ckpts:\n",
    "        raise RuntimeError(f'No checkpoints found in {results_root} for range {episode_range}.')\n",
    "\n",
    "    if isinstance(deterministic_modes, str):\n",
    "        deterministic_modes = (deterministic_modes,)\n",
    "    if isinstance(eval_splits, str):\n",
    "        eval_splits = (eval_splits,)\n",
    "\n",
    "    eval_splits = tuple(str(s).lower().strip() for s in eval_splits)\n",
    "    bad = [s for s in eval_splits if s not in {'train', 'test'}]\n",
    "    if bad:\n",
    "        raise ValueError(f'Invalid eval_splits entries: {bad}. Allowed: train, test')\n",
    "\n",
    "    stub = create_experiment6_result_stub(\n",
    "        random_seed=random_seed,\n",
    "        use_covariance=True,\n",
    "        architecture=config['agent_params']['actor_critic_type'],\n",
    "        checkpoint_path=ckpts[0][1],\n",
    "        base_agent_params=config.get('agent_params'),\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for ep, prefix in ckpts:\n",
    "        for split in eval_splits:\n",
    "            phase_eval = _subset_phase1_for_eval(phase1_data, split=split)\n",
    "            split_start = pd.to_datetime(phase_eval.test_df['Date']).min()\n",
    "            split_end = pd.to_datetime(phase_eval.test_df['Date']).max()\n",
    "\n",
    "            for mode in deterministic_modes:\n",
    "                ev = evaluate_experiment6_checkpoint(\n",
    "                    stub,\n",
    "                    phase1_data=phase_eval,\n",
    "                    config=config,\n",
    "                    random_seed=random_seed,\n",
    "                    checkpoint_path_override=prefix,\n",
    "                    deterministic_eval_mode=mode,\n",
    "                    num_eval_runs=0,\n",
    "                    stochastic_eval_mode='sample',\n",
    "                    save_eval_logs=False,\n",
    "                    save_eval_artifacts=False,\n",
    "                )\n",
    "\n",
    "                m = ev.deterministic_metrics or {}\n",
    "                action_uniques, alpha_le1_fraction, argmax_alpha_uniques = _diagnostics_from_eval(ev)\n",
    "\n",
    "                rows.append({\n",
    "                    'checkpoint_prefix': prefix,\n",
    "                    'episode': ep,\n",
    "                    'architecture': config['agent_params']['actor_critic_type'],\n",
    "                    'split': split,\n",
    "                    'deterministic_mode': mode,\n",
    "                    'seed': random_seed,\n",
    "                    'window_start': split_start,\n",
    "                    'window_end': split_end,\n",
    "                    'days_traded': int(len(ev.deterministic_portfolio) - 1) if len(ev.deterministic_portfolio) else 0,\n",
    "                    'total_return': float(m.get('total_return', np.nan)),\n",
    "                    'annualized_return': float(m.get('annualized_return', np.nan)),\n",
    "                    'sharpe_ratio': float(m.get('sharpe_ratio', np.nan)),\n",
    "                    'sortino_ratio': float(m.get('sortino_ratio', np.nan)),\n",
    "                    'max_drawdown': float(m.get('max_drawdown_abs', m.get('max_drawdown', np.nan))),\n",
    "                    'volatility': float(m.get('volatility', np.nan)),\n",
    "                    'turnover': float(m.get('turnover', np.nan)),\n",
    "                    'win_rate': float(m.get('win_rate', np.nan)),\n",
    "                    'action_uniques': action_uniques,\n",
    "                    'alpha_le1_fraction': alpha_le1_fraction,\n",
    "                    'argmax_alpha_uniques': argmax_alpha_uniques,\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise RuntimeError('Monitor produced no rows.')\n",
    "\n",
    "    left = df[df['split'] == 'train'].copy()\n",
    "    right = df[df['split'] == 'test'].copy()\n",
    "    if left.empty or right.empty:\n",
    "        raise RuntimeError(\"Overfit summary requires BOTH train and test rows. Use eval_splits=('train','test').\")\n",
    "\n",
    "    keys = ['checkpoint_prefix', 'episode', 'architecture', 'deterministic_mode', 'seed']\n",
    "    summary = left.merge(right, on=keys, suffixes=('_train', '_test'))\n",
    "\n",
    "    summary['sharpe_gap'] = summary['sharpe_ratio_train'] - summary['sharpe_ratio_test']\n",
    "    summary['mdd_gap'] = summary['max_drawdown_test'] - summary['max_drawdown_train']\n",
    "    summary['return_gap'] = summary['annualized_return_train'] - summary['annualized_return_test']\n",
    "\n",
    "    summary['flag_overfit'] = (\n",
    "        (summary['sharpe_gap'] > 0.40)\n",
    "        | (summary['mdd_gap'] > 0.05)\n",
    "        | (summary['return_gap'] > 0.10)\n",
    "    )\n",
    "\n",
    "    summary = summary.sort_values(['flag_overfit', 'sharpe_ratio_test'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    out_path = None\n",
    "    if save_csv:\n",
    "        out_dir = Path(results_root) / 'logs'\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        out_path = out_dir / f'checkpoint_overfit_monitor_{ts}.csv'\n",
    "        summary.to_csv(out_path, index=False)\n",
    "        print('ðŸ’¾ Overfit monitor saved:', out_path)\n",
    "\n",
    "    return df, summary, out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ba808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN OVERFIT MONITOR\n",
    "# ============================================================================\n",
    "RUN_OVERFIT_MONITOR = True\n",
    "\n",
    "if RUN_OVERFIT_MONITOR:\n",
    "    results_root = Path(globals().get('LATEST_RESULTS_ROOT', _infer_results_root_for_notebook(config)))\n",
    "    print('Using results root:', results_root)\n",
    "\n",
    "    monitor_rows_df, monitor_summary_df, monitor_csv_path = run_checkpoint_overfit_monitor(\n",
    "        phase1_data=phase1_data,\n",
    "        config=config,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        episode_range=(79, 100),\n",
    "        deterministic_modes=('mode', 'mean'),\n",
    "        eval_splits=('train', 'test'),\n",
    "        results_root=results_root,\n",
    "        include_rare=True,\n",
    "        save_csv=True,\n",
    "    )\n",
    "\n",
    "    display(monitor_summary_df.head(20))\n",
    "\n",
    "    if not monitor_summary_df.empty:\n",
    "        best = (\n",
    "            monitor_summary_df[monitor_summary_df['flag_overfit'] == False]\n",
    "            .sort_values('sharpe_ratio_test', ascending=False)\n",
    "            .head(10)\n",
    "        )\n",
    "        print('Top non-overfit candidates (by test Sharpe):')\n",
    "        display(best[[\n",
    "            'episode', 'deterministic_mode', 'sharpe_ratio_test',\n",
    "            'max_drawdown_test', 'turnover_test', 'sharpe_gap', 'mdd_gap', 'return_gap'\n",
    "        ]])\n",
    "else:\n",
    "    print('â„¹ï¸ RUN_OVERFIT_MONITOR=False')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Optional Analysis Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: ABLATION TABLE + TRACK SUMMARY\n",
    "# ============================================================================\n",
    "RUN_OPTIONAL_ANALYSIS = False\n",
    "\n",
    "if RUN_OPTIONAL_ANALYSIS:\n",
    "    try:\n",
    "        from src.notebook_helpers.tcn_phase1 import build_ablation_table, build_evaluation_track_summary\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'Optional analysis helpers unavailable: {e}')\n",
    "\n",
    "    available = {k: v for k, v in globals().items() if k.startswith('evaluation_') and hasattr(v, 'deterministic_metrics')}\n",
    "    if 'evaluation_stub' in globals():\n",
    "        available.setdefault('current_eval', evaluation_stub)\n",
    "\n",
    "    if not available:\n",
    "        print('No evaluation objects found. Run evaluation first.')\n",
    "    else:\n",
    "        display(build_ablation_table(available))\n",
    "        if 'evaluation_stub' in globals():\n",
    "            print('Track summary for current evaluation:')\n",
    "            display(build_evaluation_track_summary(evaluation_stub))\n",
    "else:\n",
    "    print('â„¹ï¸ RUN_OPTIONAL_ANALYSIS=False')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Run Checklist\n",
    "\n",
    "Before running heavy jobs:\n",
    "- Confirm `ACTIVE_VARIANT`\n",
    "- Confirm `max_total_timesteps` and `timesteps_per_ppo_update`\n",
    "- Confirm curriculum schedule (1500 -> 2000 -> 2500 -> full)\n",
    "- Confirm step-Sharpe checkpoint rule (`>= 0.5`)\n",
    "- Confirm intra-step TAPE delta settings (`beta`, `clip`, `window`)\n",
    "- Set exactly one expensive toggle at a time (`RUN_TRAINING`, `RUN_EVAL`, `RUN_SCAN`, `RUN_OVERFIT_MONITOR`)\n",
    "- Keep artifact exports on after successful eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15960f6c",
   "metadata": {},
   "source": [
    "## 10) Feature Manifest Audit (Latest Artifacts)\n",
    "\n",
    "Use this section to inspect the latest active-feature manifests without hardcoded timestamps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3481dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from src.notebook_helpers.tcn_phase1 import summarize_active_feature_manifest\n",
    "\n",
    "def latest_path(paths):\n",
    "    paths = [p for p in paths if p.exists()]\n",
    "    if not paths:\n",
    "        return None\n",
    "    return sorted(paths, key=lambda p: p.stat().st_mtime, reverse=True)[0]\n",
    "\n",
    "trainrl_manifest = latest_path(Path('results').glob('**/active_feature_manifest.json'))\n",
    "notebook_manifest = latest_path(Path(LATEST_RESULTS_ROOT).glob('logs/*_active_feature_manifest.json'))\n",
    "\n",
    "print('Latest train_rl manifest:', trainrl_manifest)\n",
    "print('Latest notebook manifest:', notebook_manifest)\n",
    "\n",
    "if trainrl_manifest:\n",
    "    summarize_active_feature_manifest(str(trainrl_manifest))\n",
    "if notebook_manifest:\n",
    "    summarize_active_feature_manifest(str(notebook_manifest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2203ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect missing requested columns + group counts from latest notebook manifest\n",
    "manifest_path = notebook_manifest\n",
    "if manifest_path is None:\n",
    "    print('No notebook manifest found. Run training first.')\n",
    "else:\n",
    "    m = json.loads(Path(manifest_path).read_text(encoding='utf-8'))\n",
    "    train_env = m.get('train_env', m)\n",
    "    print('Manifest path:', manifest_path)\n",
    "    print('Missing requested columns:', train_env.get('missing_requested_columns', []))\n",
    "    print('Group counts:', train_env.get('group_counts', {}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178280a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print active variable names from latest notebook manifest\n",
    "manifest_path = notebook_manifest\n",
    "if manifest_path is None:\n",
    "    print('No notebook manifest found. Run training first.')\n",
    "else:\n",
    "    m = json.loads(Path(manifest_path).read_text(encoding='utf-8'))\n",
    "    train_env = m.get('train_env', m)\n",
    "    active = train_env.get('active_feature_columns', [])\n",
    "    print(f'Active variable count: {len(active)}')\n",
    "    for v in active:\n",
    "        print(v)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
