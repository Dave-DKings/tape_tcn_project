{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64dd4dbc",
   "metadata": {},
   "source": [
    "# TCN Training Only (Clean)\n",
    "\n",
    "This notebook is for **training only**.\n",
    "It uses isolated `train_*` variables and a Sharpe-based checkpoint policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d402f683",
   "metadata": {},
   "source": [
    "## 1) Connect to Colab VM and Sync Repo\n",
    "Run this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d81df77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fresh start complete\n",
      "Repo: /content/adaptive_portfolio_rl\n",
      "Deleted paths: 3\n",
      " - /content/adaptive_portfolio_rl/tcn_fusion_results\n",
      " - /content/adaptive_portfolio_rl/data/master_features_NORMALIZED.csv\n",
      " - /content/adaptive_portfolio_rl/data/daily_ohlcv_assets.csv\n"
     ]
    }
   ],
   "source": [
    "# Fresh-start cleanup cell (run before importing project modules)\n",
    "import gc\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "TRAIN_REPO_URL = \"https://github.com/Dave-DKings/tape_tcn_project.git\"\n",
    "TRAIN_REPO_DIR = Path(\"/content/adaptive_portfolio_rl\")\n",
    "\n",
    "# 1) Sync repo to latest main\n",
    "if not (TRAIN_REPO_DIR / \".git\").exists():\n",
    "    subprocess.run([\"git\", \"clone\", TRAIN_REPO_URL, str(TRAIN_REPO_DIR)], check=True)\n",
    "\n",
    "subprocess.run([\"git\", \"-C\", str(TRAIN_REPO_DIR), \"fetch\", \"origin\"], check=True)\n",
    "subprocess.run([\"git\", \"-C\", str(TRAIN_REPO_DIR), \"reset\", \"--hard\", \"origin/main\"], check=True)\n",
    "\n",
    "# 2) Remove old experiment outputs/checkpoints/cached data\n",
    "purge_paths = [\n",
    "    TRAIN_REPO_DIR / \"tcn_fusion_results\",\n",
    "    TRAIN_REPO_DIR / \"tcn_results\",\n",
    "    TRAIN_REPO_DIR / \"tcn_att_results\",\n",
    "    TRAIN_REPO_DIR / \"output_logs\",\n",
    "    TRAIN_REPO_DIR / \"data\" / \"phase1_preparation_artifacts\",\n",
    "    TRAIN_REPO_DIR / \"data\" / \"master_features_NORMALIZED.csv\",\n",
    "    TRAIN_REPO_DIR / \"data\" / \"daily_ohlcv_assets.csv\",              # forces fresh OHLCV download\n",
    "    TRAIN_REPO_DIR / \"data\" / \"processed_daily_macro_features.csv\",   # forces fresh macro cache build\n",
    "]\n",
    "\n",
    "deleted = []\n",
    "for p in purge_paths:\n",
    "    if p.is_dir():\n",
    "        shutil.rmtree(p, ignore_errors=True)\n",
    "        deleted.append(str(p))\n",
    "    elif p.is_file():\n",
    "        p.unlink(missing_ok=True)\n",
    "        deleted.append(str(p))\n",
    "\n",
    "# 3) Remove Python/Jupyter cache folders\n",
    "for cache_dir in TRAIN_REPO_DIR.rglob(\"__pycache__\"):\n",
    "    shutil.rmtree(cache_dir, ignore_errors=True)\n",
    "for ckpt_dir in TRAIN_REPO_DIR.rglob(\".ipynb_checkpoints\"):\n",
    "    shutil.rmtree(ckpt_dir, ignore_errors=True)\n",
    "\n",
    "# 4) Clear loaded project modules from kernel memory\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if mod.startswith(\"src.\") or mod.startswith(\"src_\"):\n",
    "        del sys.modules[mod]\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Fresh start complete\")\n",
    "print(f\"Repo: {TRAIN_REPO_DIR}\")\n",
    "print(f\"Deleted paths: {len(deleted)}\")\n",
    "for d in deleted:\n",
    "    print(\" -\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9325a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "CWD: /content\n",
      "\n",
      "Top-level:\n",
      " - [DIR ] .git\n",
      " - [FILE] .gitignore\n",
      " - [FILE] RL Portfolio Optimization Feature Engineering.md\n",
      " - [FILE] RL_Portfolio_Optimization_Feature_Engineering.ipynb\n",
      " - [FILE] USAGE_GUIDE_ACTUARIAL.py\n",
      " - [FILE] __init__.py\n",
      " - [FILE] convert_md_to_ipynb.py\n",
      " - [DIR ] data\n",
      " - [FILE] data1.zip\n",
      " - [DIR ] data_exports\n",
      " - [FILE] debug_attention_weights.py\n",
      " - [DIR ] docs\n",
      " - [DIR ] eval\n",
      " - [DIR ] paper\n",
      " - [DIR ] prompts\n",
      " - [FILE] ra_kl_research_writeup.ipynb\n",
      " - [FILE] rcdcc_research_writeup.ipynb\n",
      " - [FILE] requirements.txt\n",
      " - [FILE] run_tcn_eval.py\n",
      " - [DIR ] src\n",
      " - [FILE] tcn_architecture_analysis.ipynb\n",
      " - [DIR ] tcn_documentation\n",
      " - [FILE] tcn_evaluation_only.ipynb\n",
      " - [FILE] technical_deep_dive_presentation.ipynb\n",
      " - [DIR ] tests\n",
      " - [FILE] traditional_portfolio_benchmarks.ipynb\n",
      " - [DIR ] training_scripts\n",
      "\n",
      "Target paths:\n",
      " - tcn_fusion_results: MISSING\n",
      " - tcn_results: MISSING\n",
      " - tcn_att_results: MISSING\n",
      " - output_logs: MISSING\n",
      " - data/phase1_preparation_artifacts: MISSING\n",
      " - data/master_features_NORMALIZED.csv: MISSING\n",
      " - data/daily_ohlcv_assets.csv: MISSING\n",
      " - data/processed_daily_macro_features.csv: MISSING\n"
     ]
    }
   ],
   "source": [
    "#from pathlib import Path\n",
    "import os\n",
    "\n",
    "root = Path(\"/content/adaptive_portfolio_rl\")\n",
    "print(\"Exists:\", root.exists())\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "print(\"\\nTop-level:\")\n",
    "for p in sorted(root.iterdir()):\n",
    "    kind = \"DIR \" if p.is_dir() else \"FILE\"\n",
    "    print(f\" - [{kind}] {p.name}\")\n",
    "\n",
    "# Quick check for outputs/caches you expected to be deleted\n",
    "targets = [\n",
    "    \"tcn_fusion_results\",\n",
    "    \"tcn_results\",\n",
    "    \"tcn_att_results\",\n",
    "    \"output_logs\",\n",
    "    \"data/phase1_preparation_artifacts\",\n",
    "    \"data/master_features_NORMALIZED.csv\",\n",
    "    \"data/daily_ohlcv_assets.csv\",\n",
    "    \"data/processed_daily_macro_features.csv\",\n",
    "]\n",
    "print(\"\\nTarget paths:\")\n",
    "for t in targets:\n",
    "    p = root / t\n",
    "    print(f\" - {t}: {'EXISTS' if p.exists() else 'MISSING'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!find /content/adaptive_portfolio_rl -maxdepth 3 | head -n 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af84a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install project requirements in Colab VM\n",
    "#import subprocess, sys\n",
    "#from pathlib import Path\n",
    "\n",
    "REPO_DIR = Path(\"/content/adaptive_portfolio_rl\")\n",
    "REQ_FILE = REPO_DIR / \"requirements.txt\"\n",
    "\n",
    "if not REQ_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Missing requirements file: {REQ_FILE}\")\n",
    "\n",
    "print(\"Using python:\", sys.executable)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(REQ_FILE)], check=True)\n",
    "\n",
    "print(\"‚úÖ Requirements installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f880e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-63e896b9-315e-1d28-078b-5c8272c3e435)\n",
      "TF GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Mixed precision policy: <DTypePolicy \"mixed_float16\">\n",
      "Matmul device: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Default GPU device name: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# --- GPU sanity/setup for TensorFlow ---\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) Confirm Colab sees an NVIDIA GPU\n",
    "!nvidia-smi -L\n",
    "\n",
    "# 2) Confirm TensorFlow sees GPU(s)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"TF GPUs:\", gpus)\n",
    "if not gpus:\n",
    "    raise RuntimeError(\"No GPU visible to TensorFlow. In Colab: Runtime -> Change runtime type -> GPU\")\n",
    "\n",
    "# 3) Safer GPU memory behavior\n",
    "for g in gpus:\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "# 4) Optional: speed boost on modern GPUs\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "print(\"Mixed precision policy:\", tf.keras.mixed_precision.global_policy())\n",
    "\n",
    "# 5) Quick proof op runs on GPU\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    a = tf.random.normal((4096, 4096))\n",
    "    b = tf.random.normal((4096, 4096))\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "print(\"Matmul device:\", c.device)\n",
    "print(\"Default GPU device name:\", tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14187b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, pandas, tensorflow\n",
    "print(\"numpy\", numpy.__version__)\n",
    "print(\"pandas\", pandas.__version__)\n",
    "print(\"tensorflow\", tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49d0f2",
   "metadata": {},
   "source": [
    "## 2) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641959e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /content/adaptive_portfolio_rl\n",
      "sys.path[0]: /content/adaptive_portfolio_rl\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_DIR = Path(\"/content/adaptive_portfolio_rl\")\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Repo not found: {REPO_DIR}\")\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "# Add repo root to Python path\n",
    "if str(REPO_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"sys.path[0]:\", sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b477656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.config import get_active_config\n",
    "from src.csv_logger import CSVLogger\n",
    "from src.notebook_helpers.tcn_phase1 import prepare_phase1_dataset, run_experiment6_tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b538f",
   "metadata": {},
   "source": [
    "## 3) Base Config and Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b71d4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Global feature-audit plan enforcement (49 + 4 actuarial = 53)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def enforce_feature_audit_plan(cfg):\n",
    "    fs = cfg.setdefault(\"feature_params\", {}).setdefault(\"feature_selection\", {})\n",
    "    fs[\"enforce_allowlist\"] = True\n",
    "    fs[\"allowlist_apply_to_phase2\"] = False\n",
    "\n",
    "    allowlist = list(dict.fromkeys(fs.get(\"active_features_allowlist\", []) or []))\n",
    "    fs[\"active_features_allowlist\"] = allowlist\n",
    "\n",
    "    plan_name = fs.get(\"feature_audit_plan_name\", \"feature_audit_allowlist\")\n",
    "    expected_total = int(fs.get(\"feature_audit_expected_total_count\", len(allowlist)))\n",
    "    act_cols = [c for c in allowlist if str(c).startswith(\"Actuarial_\")]\n",
    "\n",
    "    print(\"‚úÖ Feature audit plan configured\")\n",
    "    print(\"   plan:\", plan_name)\n",
    "    print(\"   allowlist count:\", len(allowlist))\n",
    "    print(\"   expected total:\", expected_total)\n",
    "    print(\"   actuarial in allowlist:\", len(act_cols), act_cols)\n",
    "\n",
    "    if len(allowlist) != expected_total:\n",
    "        print(\"‚ö†Ô∏è Allowlist count differs from expected total. Check src/config.py\")\n",
    "\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ca80e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature audit plan configured\n",
      "   plan: exp6_feature_audit_20260221_v2\n",
      "   allowlist count: 53\n",
      "   expected total: 53\n",
      "   actuarial in allowlist: 4 ['Actuarial_Expected_Recovery', 'Actuarial_Prob_30d', 'Actuarial_Prob_60d', 'Actuarial_Reserve_Severity']\n",
      "üìä Loading raw market data...\n",
      "   ‚úÖ Raw data shape: (55107, 7)\n",
      "   ‚úÖ Date range: 2003-09-02 00:00:00 ‚Üí 2025-08-29 00:00:00\n",
      "\n",
      "üîß Computing multi-horizon log returns: [1, 5, 10, 21]\n",
      "   ‚úÖ Shape after returns: (54897, 11)\n",
      "\n",
      "üìà Calculating 21-day rolling statistics\n",
      "\n",
      "üßÆ Computing technical indicators\n",
      "\n",
      "üìä Computing dynamic covariance features\n",
      "\n",
      "üéØ Adding regime awareness features\n",
      "   ‚úÖ Master DF shape: (54897, 47)\n",
      "   ‚úÖ Total features: 47\n",
      "\n",
      "üìä Integrating fundamental features (if enabled)...\n",
      "   ‚úÖ Fundamental columns in dataset: 6 (enabled=True)\n",
      "   üßæ Sample fundamental cols: ['Fundamental_FCFE_Delta', 'Fundamental_Revenue_Delta', 'Fundamental_NCFO_Delta', 'Fundamental_FCFE_Sign', 'Fundamental_Staleness_Days', 'Fundamental_Staleness_Quarters']\n",
      "\n",
      "üìä Integrating macroeconomic features (if enabled)...\n",
      "   ‚úÖ Macro features added - 43 columns: ['EFFR_diff', 'EFFR_zscore', 'SOFR_level', 'SOFR_diff', 'FEDFUNDS_diff', 'FEDFUNDS_zscore', 'DGS10_level', 'DGS10_diff', 'DGS10_slope', 'DGS2_level', 'DGS2_diff', 'T10Y2Y_level', 'TIPS10Y_level', 'TIPS10Y_diff', 'BreakevenInf10Y_level', 'BreakevenInf10Y_diff', 'BreakevenInf5Y_level', 'BreakevenInf5Y_diff', 'FedBalanceSheet_level', 'FedBalanceSheet_diff', 'ON_RRP_level', 'ON_RRP_diff', 'CPI_yoy', 'CPI_mom', 'PPI_yoy', 'PPI_mom', 'UNRATE_level', 'UNRATE_diff', 'UNRATE_zscore', 'PAYEMS_level', 'PAYEMS_diff', 'PAYEMS_yoy', 'INDPRO_level', 'INDPRO_diff', 'INDPRO_yoy', 'IG_Credit_level', 'IG_Credit_diff', 'IG_Credit_zscore', 'HY_Credit_level', 'HY_Credit_diff', 'HY_Credit_zscore', 'VIX_level', 'VIX_zscore']\n",
      "\n",
      "üìä Integrating Alpha features (if enabled)...\n",
      "\n",
      "üìä Integrating actuarial features (if enabled)...\n",
      "   ‚úÖ Actuarial columns in dataset: 4 (enabled=True)\n",
      "   üìã Non-null counts: {'Actuarial_Expected_Recovery': 54897, 'Actuarial_Prob_30d': 54897, 'Actuarial_Prob_60d': 54897, 'Actuarial_Reserve_Severity': 54897}\n",
      "\n",
      "‚úÖ Final master DF shape: (54897, 109)\n",
      "   ‚úÖ Total features: 109\n",
      "üß≠ Feature audit plan: exp6_feature_audit_20260221_v2 (allowlist enabled)\n",
      "   active feature count (phase1): 53\n",
      "   ‚úÖ expected active features: 53\n",
      "\n",
      "================================================================================\n",
      "‚úÇÔ∏è FILTERING TO ANALYSIS PERIOD\n",
      "================================================================================\n",
      "   Filtering data to: 2003-09-02 ‚Üí 2025-09-01\n",
      "   ‚úÖ Dates after filter: 5514 trading days\n",
      "   ‚úÖ Date range: 2003-10-01 00:00:00 to 2025-08-29 00:00:00\n",
      "================================================================================\n",
      "================================================================================\n",
      "‚úÇÔ∏è  TIME-BASED TRAIN/TEST SPLIT (80/20 split)\n",
      "   Train: 2003-10-01 ‚Üí 2021-04-09 (4411 days, 17.5 years, 43867 rows)\n",
      "   Test:  2021-04-12 ‚Üí 2025-08-29 (1103 days, 4.4 years, 11030 rows)\n",
      "================================================================================\n",
      "\n",
      "üîß NORMALISING FEATURES (standard scaler)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.data_utils:Found 1566 NaN values after normalization, applying forward-fill only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving NORMALISED master dataframe to '/content/adaptive_portfolio_rl/data/master_features_NORMALIZED.csv'\n",
      "\n",
      "üíæ Saved preparation artifacts:\n",
      "   raw OHLCV: /content/adaptive_portfolio_rl/data_exports/phase1_prep_20260224_084140_raw_ohlcv.csv\n",
      "   full engineered: /content/adaptive_portfolio_rl/data_exports/phase1_prep_20260224_084140_feature_engineered_full.csv\n",
      "   analysis-window engineered: /content/adaptive_portfolio_rl/data_exports/phase1_prep_20260224_084140_feature_engineered_analysis_window.csv\n",
      "   normalized master: /content/adaptive_portfolio_rl/data_exports/phase1_prep_20260224_084140_feature_engineered_normalized.csv\n",
      "   train normalized: /content/adaptive_portfolio_rl/data_exports/phase1_prep_20260224_084140_train_normalized.csv\n",
      "   test normalized: /content/adaptive_portfolio_rl/data_exports/phase1_prep_20260224_084140_test_normalized.csv\n",
      "   scalers: /content/adaptive_portfolio_rl/data_exports/phase1_prep_20260224_084140_scalers.joblib\n",
      "   audit report: /content/adaptive_portfolio_rl/data_exports/phase1_prep_20260224_084140_preparation_audit.json\n"
     ]
    }
   ],
   "source": [
    "TRAIN_RANDOM_SEED = 42\n",
    "\n",
    "train_config = deepcopy(get_active_config(\"phase1\"))\n",
    "\n",
    "# Optional: override analysis horizon\n",
    "# train_config[\"ANALYSIS_END_DATE\"] = \"2025-09-01\"\n",
    "\n",
    "train_config = enforce_feature_audit_plan(train_config)\n",
    "\n",
    "# Force fresh dataset build and market data re-download\n",
    "if \"train_phase1_data\" in globals():\n",
    "    del train_phase1_data\n",
    "\n",
    "train_phase1_data = prepare_phase1_dataset(\n",
    "    train_config,\n",
    "    force_download=True,\n",
    "    preparation_artifacts_dir=\"/content/adaptive_portfolio_rl/data_exports\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32011606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (43867, 109)\n",
      "Test shape: (11030, 109)\n",
      "Total columns: 109\n",
      "Potential redundant raw/unscaled cols: 0\n",
      "[]\n",
      "Model feature count (phase1): 53\n",
      "Actuarial feature count: 4 ['Actuarial_Expected_Recovery', 'Actuarial_Prob_30d', 'Actuarial_Prob_60d', 'Actuarial_Reserve_Severity']\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape:\", train_phase1_data.train_df.shape)\n",
    "print(\"Test shape:\", train_phase1_data.test_df.shape)\n",
    "\n",
    "cols = train_phase1_data.train_df.columns\n",
    "print(\"Total columns:\", len(cols))\n",
    "\n",
    "# quick sanity for common redundant groups\n",
    "dup_like = [c for c in cols if c.endswith(\"_raw\") or c.endswith(\"_unscaled\")]\n",
    "print(\"Potential redundant raw/unscaled cols:\", len(dup_like))\n",
    "print(dup_like[:20])\n",
    "\n",
    "used_now = list(dict.fromkeys(train_phase1_data.data_processor.get_feature_columns(\"phase1\")))\n",
    "act_now = [c for c in used_now if c.startswith(\"Actuarial_\")]\n",
    "print(\"Model feature count (phase1):\", len(used_now))\n",
    "print(\"Actuarial feature count:\", len(act_now), act_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee488d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f32ef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used feature count: 53\n",
      "Actuarial used: 4 ['Actuarial_Expected_Recovery', 'Actuarial_Prob_30d', 'Actuarial_Prob_60d', 'Actuarial_Reserve_Severity']\n",
      "Disabled that still in used: []\n",
      "VIX_zscore used? True\n"
     ]
    }
   ],
   "source": [
    "used = set(train_phase1_data.data_processor.get_feature_columns(\"phase1\"))\n",
    "disabled = set(train_config[\"feature_params\"][\"feature_selection\"][\"disabled_features\"])\n",
    "act_used = sorted([c for c in used if c.startswith(\"Actuarial_\")])\n",
    "\n",
    "print(\"Used feature count:\", len(used))\n",
    "print(\"Actuarial used:\", len(act_used), act_used)\n",
    "print(\"Disabled that still in used:\", sorted(disabled & used))  # should be []\n",
    "print(\"VIX_zscore used?\", \"VIX_zscore\" in used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e85bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = [\"Date\", \"Ticker\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "keep = [c for c in base_cols + list(used) if c in train_phase1_data.master_df.columns]\n",
    "\n",
    "train_phase1_data.master_df = train_phase1_data.master_df[keep].copy()\n",
    "train_phase1_data.train_df = train_phase1_data.train_df[keep].copy()\n",
    "train_phase1_data.test_df  = train_phase1_data.test_df[keep].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e8d0e",
   "metadata": {},
   "source": [
    "## 4) Training Overrides (Sharpe-Only Checkpoint Policy)\n",
    "\n",
    "This policy keeps only Sharpe-threshold high-watermark checkpointing (`>= 0.5`) and disables rare/step/periodic/TAPE checkpoint routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NEXT RUN OVERRIDES (post-mortem tuned: KL stability + turnover control)\n",
    "# ============================================================================\n",
    "from copy import deepcopy\n",
    "\n",
    "train_config = deepcopy(train_config)  # or deepcopy(config) if that's your active object\n",
    "\n",
    "tp = train_config[\"training_params\"]\n",
    "ap = train_config[\"agent_params\"]\n",
    "ppo = ap[\"ppo_params\"]\n",
    "env = train_config[\"environment_params\"]\n",
    "\n",
    "\n",
    "ap[\"actor_critic_type\"] = \"TCN\"\n",
    "ap[\"use_fusion\"] = False\n",
    "ap[\"use_attention\"] = False\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1) Core run shape\n",
    "# ----------------------------------------------------------------------------\n",
    "tp[\"max_total_timesteps\"] = 20_000\n",
    "tp[\"timesteps_per_ppo_update\"] = 384  # fallback\n",
    "\n",
    "tp[\"timesteps_per_ppo_update_schedule\"] = [\n",
    "    {\"threshold\": 0, \"timesteps_per_update\": 384},\n",
    "    {\"threshold\": 50_000, \"timesteps_per_update\": 448},\n",
    "]\n",
    "\n",
    "tp[\"batch_size_ppo_schedule\"] = [\n",
    "    {\"threshold\": 0, \"batch_size\": 96},\n",
    "    {\"threshold\": 50_000, \"batch_size\": 112},\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2) A2: deeper temporal receptive field (1D TCN only; 2D variant skipped)\n",
    "# ----------------------------------------------------------------------------\n",
    "ap[\"tcn_filters\"] = [64, 96, 128, 128, 128]\n",
    "ap[\"tcn_kernel_size\"] = 5\n",
    "ap[\"tcn_dilations\"] = [1, 2, 4, 8, 16]\n",
    "ap[\"tcn_dropout\"] = 0.15\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3) PPO stability (reduce aggressiveness)\n",
    "# ----------------------------------------------------------------------------\n",
    "ppo[\"num_ppo_epochs\"] = 1\n",
    "ppo[\"policy_clip\"] = 0.08\n",
    "ppo[\"target_kl\"] = 0.050\n",
    "ppo[\"kl_stop_multiplier\"] = 1.25\n",
    "ppo[\"minibatches_before_kl_stop\"] = 2\n",
    "ppo[\"max_grad_norm\"] = 0.30\n",
    "\n",
    "ppo[\"actor_lr\"] = 2e-5\n",
    "ppo[\"critic_lr\"] = 1.2e-4\n",
    "ppo[\"entropy_coef\"] = 0.0020\n",
    "\n",
    "tp[\"actor_lr_schedule\"] = [\n",
    "    {\"threshold\": 0, \"lr\": 8e-6},\n",
    "    {\"threshold\": 30_000, \"lr\": 7e-6},\n",
    "    {\"threshold\": 60_000, \"lr\": 6e-6},\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4) RA-KL (less aggressive, prevent floor lock)\n",
    "# ----------------------------------------------------------------------------\n",
    "tp[\"ra_kl_enabled\"] = True\n",
    "tp[\"ra_kl_target_ratio\"] = 1.0\n",
    "tp[\"ra_kl_ema_alpha\"] = 0.25\n",
    "tp[\"ra_kl_gain\"] = 0.03\n",
    "tp[\"ra_kl_deadband\"] = 0.20\n",
    "tp[\"ra_kl_max_change_fraction\"] = 0.05\n",
    "tp[\"ra_kl_min_target_kl\"] = 0.016\n",
    "tp[\"ra_kl_max_target_kl\"] = 0.030\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5) Dirichlet + concentration controls\n",
    "# ----------------------------------------------------------------------------\n",
    "ap[\"dirichlet_alpha_activation\"] = \"softplus\"\n",
    "ap[\"dirichlet_logit_temperature\"] = 1.0 # Keep static temperature neutral when adaptive is on\n",
    "ap[\"dirichlet_alpha_cap\"] = 20.0\n",
    "ap[\"dirichlet_epsilon\"] = {\"max\": 0.2, \"min\": 0.02}\n",
    "\n",
    "ap[\"dirichlet_adaptive_temperature_enabled\"] = True\n",
    "ap[\"dirichlet_adaptive_temperature_base\"] = 0.9\n",
    "ap[\"dirichlet_adaptive_temperature_slope\"] = 0.6\n",
    "ap[\"dirichlet_adaptive_temperature_min\"] = 0.8\n",
    "ap[\"dirichlet_adaptive_temperature_max\"] = 2.5\n",
    "\n",
    "\n",
    "# A3/A4: richer alpha head + optional cross-asset mixer\n",
    "ap[\"fusion_cross_asset_mixer_enabled\"] = True\n",
    "ap[\"fusion_cross_asset_mixer_layers\"] = 2\n",
    "ap[\"fusion_cross_asset_mixer_expansion\"] = 2.0\n",
    "ap[\"fusion_cross_asset_mixer_dropout\"] = 0.10\n",
    "ap[\"fusion_alpha_head_hidden_dims\"] = [128, 64]\n",
    "ap[\"fusion_alpha_head_dropout\"] = 0.05\n",
    "\n",
    "env[\"concentration_penalty_scalar\"] = 3.0\n",
    "env[\"concentration_target_hhi\"] = 0.12\n",
    "env[\"top_weight_penalty_scalar\"] = 2.0\n",
    "env[\"action_realization_penalty_scalar\"] = 0.5\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6) Turnover + execution smoothing\n",
    "# ----------------------------------------------------------------------------\n",
    "env[\"target_turnover\"] = 0.35\n",
    "env[\"turnover_penalty_scalar\"] = 0.00\n",
    "env[\"transaction_cost_pct\"] = 0.001\n",
    "\n",
    "tp[\"action_execution_beta_curriculum\"] = {\n",
    "    0: 0.15,\n",
    "    30_000: 0.25,\n",
    "}\n",
    "tp[\"evaluation_action_execution_beta\"] = 0.15\n",
    "\n",
    "tp[\"turnover_penalty_curriculum\"] = {\n",
    "    0: 0.00,\n",
    "    10_000: 0.0,\n",
    "    25_000: 0.0,\n",
    "    40_000: 0.2,\n",
    "}\n",
    "tp[\"evaluation_turnover_penalty_scalar\"] = 0.2\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7) Episode horizon curriculum (keep cap late)\n",
    "# ----------------------------------------------------------------------------\n",
    "tp[\"use_episode_length_curriculum\"] = True\n",
    "tp[\"episode_length_curriculum_schedule\"] = [\n",
    "    {\"threshold\": 0, \"limit\": 252},\n",
    "    {\"threshold\": 10_000, \"limit\": 504},\n",
    "    {\"threshold\": 25_000, \"limit\": 756},\n",
    "    {\"threshold\": 90_000, \"limit\": 1008},\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 8) Logging + checkpoints\n",
    "# ----------------------------------------------------------------------------\n",
    "tp[\"log_step_diagnostics\"] = True\n",
    "tp[\"update_log_interval\"] = 5\n",
    "tp[\"alpha_diversity_log_interval\"] = 1\n",
    "tp[\"alpha_diversity_warning_after_updates\"] = 120\n",
    "tp[\"alpha_diversity_warning_std_threshold\"] = 0.25\n",
    "\n",
    "tp[\"deterministic_validation_checkpointing_enabled\"] = True\n",
    "tp[\"deterministic_validation_eval_every_episodes\"] = 3\n",
    "tp[\"deterministic_validation_mode\"] = \"mode\"\n",
    "tp[\"deterministic_validation_episode_length_limit\"] = 504\n",
    "tp[\"deterministic_validation_sharpe_min\"] = 0.5\n",
    "tp[\"deterministic_validation_sharpe_min_delta\"] = 0.005\n",
    "tp[\"deterministic_validation_seed_offset\"] = 10_000\n",
    "tp[\"deterministic_validation_log_alpha_stats\"] = True\n",
    "tp[\"deterministic_validation_checkpointing_only\"] = True\n",
    "\n",
    "tp[\"high_watermark_checkpoint_enabled\"] = False\n",
    "tp[\"high_watermark_sharpe_threshold\"] = 0.5\n",
    "tp[\"step_sharpe_checkpoint_enabled\"] = False\n",
    "tp[\"periodic_checkpoint_every_steps\"] = 0\n",
    "tp[\"rare_checkpoint_params\"] = {\"enable\": False}\n",
    "tp[\"tape_checkpoint_threshold\"] = 999.0\n",
    "\n",
    "print(\"‚úÖ Applied next-run override (KL-stable + smoother execution + moderate turnover control)\")\n",
    "print(\"num_ppo_epochs:\", ppo[\"num_ppo_epochs\"])\n",
    "print(\"target_kl:\", ppo[\"target_kl\"], \"| kl_stop_multiplier:\", ppo[\"kl_stop_multiplier\"])\n",
    "print(\"RA-KL:\", {k: tp[k] for k in [\n",
    "    \"ra_kl_enabled\", \"ra_kl_gain\", \"ra_kl_deadband\",\n",
    "    \"ra_kl_max_change_fraction\", \"ra_kl_min_target_kl\", \"ra_kl_max_target_kl\"\n",
    "]})\n",
    "print(\"action_execution_beta_curriculum:\", tp[\"action_execution_beta_curriculum\"])\n",
    "print(\"turnover_penalty_curriculum:\", tp[\"turnover_penalty_curriculum\"])\n",
    "print(\"concentration:\", env[\"concentration_penalty_scalar\"], env[\"concentration_target_hhi\"], env[\"top_weight_penalty_scalar\"])\n",
    "\n",
    "\n",
    "print(\"TCN stack:\", ap[\"tcn_filters\"], \"| dilations:\", ap[\"tcn_dilations\"], \"| dropout:\", ap[\"tcn_dropout\"])\n",
    "print(\"Fusion mixer:\", {k: ap[k] for k in [\"fusion_cross_asset_mixer_enabled\", \"fusion_cross_asset_mixer_layers\", \"fusion_cross_asset_mixer_expansion\", \"fusion_cross_asset_mixer_dropout\"]})\n",
    "print(\"Fusion alpha head:\", ap[\"fusion_alpha_head_hidden_dims\"], \"| dropout:\", ap[\"fusion_alpha_head_dropout\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1706d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional experimental override (OFF by default)\n",
    "# Keep this OFF for the aligned default pipeline.\n",
    "EXPERIMENT_DISABLE_KL_GUARDS = True\n",
    "\n",
    "if EXPERIMENT_DISABLE_KL_GUARDS:\n",
    "    tp = train_config[\"training_params\"]\n",
    "    ppo = train_config[\"agent_params\"][\"ppo_params\"]\n",
    "\n",
    "    # Disable RA-KL controller (otherwise it keeps adjusting target_kl)\n",
    "    tp[\"ra_kl_enabled\"] = False\n",
    "\n",
    "    # Disable KL early-stop gate in PPOAgentTF\n",
    "    ppo[\"target_kl\"] = 0.0\n",
    "\n",
    "    # Optional (irrelevant once target_kl=0, but explicit)\n",
    "    ppo[\"kl_stop_multiplier\"] = 999.0\n",
    "    ppo[\"minibatches_before_kl_stop\"] = 9999\n",
    "    print(\"‚ö†Ô∏è EXPERIMENT_DISABLE_KL_GUARDS=True (non-default experimental mode)\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è EXPERIMENT_DISABLE_KL_GUARDS=False (keeping RA-KL + KL safeguards)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09063801",
   "metadata": {},
   "source": [
    "## 5) Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TRAINING = True\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    tp = train_config[\"training_params\"]\n",
    "    print(\"üöÄ Starting training\")\n",
    "    print(\"Architecture:\", train_config[\"agent_params\"].get(\"actor_critic_type\"))\n",
    "    print(\"max_total_timesteps:\", tp[\"max_total_timesteps\"])\n",
    "\n",
    "    train_experiment6 = run_experiment6_tape(\n",
    "        phase1_data=train_phase1_data,\n",
    "        config=train_config,\n",
    "        random_seed=TRAIN_RANDOM_SEED,\n",
    "        csv_logger_cls=CSVLogger,\n",
    "        use_covariance=True,\n",
    "        architecture=train_config[\"agent_params\"].get(\"actor_critic_type\"),\n",
    "        timesteps_per_update=tp.get(\"timesteps_per_ppo_update\", 384),\n",
    "        max_total_timesteps=tp[\"max_total_timesteps\"],\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Training complete\")\n",
    "    print(\"checkpoint_prefix:\", train_experiment6.checkpoint_path)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è RUN_TRAINING=False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ffd0df",
   "metadata": {},
   "source": [
    "## 6) Inspect Latest Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fd0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RESULTS_ROOT = Path(\"/content/adaptive_portfolio_rl/tcn_fusion_results\")\n",
    "TRAIN_LOGS_DIR = TRAIN_RESULTS_ROOT / \"logs\"\n",
    "\n",
    "episodes_files = sorted(TRAIN_LOGS_DIR.glob(\"*episodes*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not episodes_files:\n",
    "    print(f\"No episodes CSV found in {TRAIN_LOGS_DIR}\")\n",
    "else:\n",
    "    train_episodes_path = episodes_files[0]\n",
    "    train_episodes_df = pd.read_csv(train_episodes_path)\n",
    "    print(\"Episodes file:\", train_episodes_path)\n",
    "    print(\"Rows:\", len(train_episodes_df))\n",
    "    display(train_episodes_df.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb04a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0e0e1",
   "metadata": {},
   "source": [
    "## 7) Export Results Folder (Optional)\n",
    "Creates a zip for download from Colab VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "EXPORT_RESULTS_ZIP = True\n",
    "EXPORT_PATH = Path(\"/content/tcn_fusion_results_run4.zip\")\n",
    "ROOT = Path(\"/content/adaptive_portfolio_rl\")\n",
    "\n",
    "if EXPORT_RESULTS_ZIP:\n",
    "    # Core items\n",
    "    include_paths = [\n",
    "        ROOT / \"tcn_fusion_results\",\n",
    "        ROOT / \"data\" / \"phase1_preparation_artifacts\",\n",
    "        ROOT / \"data\" / \"master_features_NORMALIZED.csv\",\n",
    "        ROOT / \"data_exports\",  # include all prep exports like phase1_prep_* artifacts\n",
    "    ]\n",
    "\n",
    "    # Also include latest phase1_prep_* files (explicitly, if present)\n",
    "    data_exports_dir = ROOT / \"data_exports\"\n",
    "    if data_exports_dir.exists():\n",
    "        latest_prep_files = sorted(\n",
    "            data_exports_dir.glob(\"phase1_prep_*\"),\n",
    "            key=lambda p: p.stat().st_mtime,\n",
    "            reverse=True,\n",
    "        )\n",
    "        include_paths.extend(latest_prep_files)\n",
    "\n",
    "    # De-dup + existence check\n",
    "    seen = set()\n",
    "    existing = []\n",
    "    for p in include_paths:\n",
    "        p = p.resolve()\n",
    "        if p.exists() and p not in seen:\n",
    "            seen.add(p)\n",
    "            existing.append(p)\n",
    "\n",
    "    if not existing:\n",
    "        print(\"‚ö†Ô∏è Nothing to export.\")\n",
    "    else:\n",
    "        if EXPORT_PATH.exists():\n",
    "            EXPORT_PATH.unlink()\n",
    "\n",
    "        rel_items = [str(p.relative_to(ROOT)) for p in existing if str(p).startswith(str(ROOT))]\n",
    "        if not rel_items:\n",
    "            print(\"‚ö†Ô∏è No export items are under ROOT.\")\n",
    "        else:\n",
    "            cmd = f\"cd {ROOT} && zip -qr {EXPORT_PATH} \" + \" \".join(f'\"{x}\"' for x in rel_items)\n",
    "            subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "            print(f\"‚úÖ Created: {EXPORT_PATH}\")\n",
    "            print(\"Included:\")\n",
    "            for p in rel_items:\n",
    "                print(\" -\", p)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è EXPORT_RESULTS_ZIP=False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp /content/tcn_fusion_results_run4.zip /content/drive/MyDrive/\n",
    "print(\"‚úÖ Copied to Drive: /content/drive/MyDrive/tcn_fusion_results_run4.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf11d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
