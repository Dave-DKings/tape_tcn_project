{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "64dd4dbc",
      "metadata": {},
      "source": [
        "# TCN Training Only (Clean)\n",
        "\n",
        "This notebook is for **training only**.\n",
        "It uses isolated `train_*` variables and a Sharpe-based checkpoint policy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d402f683",
      "metadata": {},
      "source": [
        "## 1) Connect to Colab VM and Sync Repo\n",
        "Run this first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d81df77",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fresh-start cleanup cell (run before importing project modules)\n",
        "import gc\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_REPO_URL = \"https://github.com/Dave-DKings/tape_tcn_project.git\"\n",
        "TRAIN_REPO_DIR = Path(\"/content/adaptive_portfolio_rl\")\n",
        "\n",
        "# 1) Sync repo to latest main\n",
        "if not (TRAIN_REPO_DIR / \".git\").exists():\n",
        "    subprocess.run([\"git\", \"clone\", TRAIN_REPO_URL, str(TRAIN_REPO_DIR)], check=True)\n",
        "\n",
        "subprocess.run([\"git\", \"-C\", str(TRAIN_REPO_DIR), \"fetch\", \"origin\"], check=True)\n",
        "subprocess.run([\"git\", \"-C\", str(TRAIN_REPO_DIR), \"reset\", \"--hard\", \"origin/main\"], check=True)\n",
        "\n",
        "# 2) Remove old experiment outputs/checkpoints/cached data\n",
        "purge_paths = [\n",
        "    TRAIN_REPO_DIR / \"tcn_fusion_results\",\n",
        "    TRAIN_REPO_DIR / \"tcn_results\",\n",
        "    TRAIN_REPO_DIR / \"tcn_att_results\",\n",
        "    TRAIN_REPO_DIR / \"output_logs\",\n",
        "    TRAIN_REPO_DIR / \"data\" / \"phase1_preparation_artifacts\",\n",
        "    TRAIN_REPO_DIR / \"data\" / \"master_features_NORMALIZED.csv\",\n",
        "    TRAIN_REPO_DIR / \"data\" / \"daily_ohlcv_assets.csv\",              # forces fresh OHLCV download\n",
        "    TRAIN_REPO_DIR / \"data\" / \"processed_daily_macro_features.csv\",   # forces fresh macro cache build\n",
        "]\n",
        "\n",
        "deleted = []\n",
        "for p in purge_paths:\n",
        "    if p.is_dir():\n",
        "        shutil.rmtree(p, ignore_errors=True)\n",
        "        deleted.append(str(p))\n",
        "    elif p.is_file():\n",
        "        p.unlink(missing_ok=True)\n",
        "        deleted.append(str(p))\n",
        "\n",
        "# 3) Remove Python/Jupyter cache folders\n",
        "for cache_dir in TRAIN_REPO_DIR.rglob(\"__pycache__\"):\n",
        "    shutil.rmtree(cache_dir, ignore_errors=True)\n",
        "for ckpt_dir in TRAIN_REPO_DIR.rglob(\".ipynb_checkpoints\"):\n",
        "    shutil.rmtree(ckpt_dir, ignore_errors=True)\n",
        "\n",
        "# 4) Clear loaded project modules from kernel memory\n",
        "for mod in list(sys.modules.keys()):\n",
        "    if mod.startswith(\"src.\") or mod.startswith(\"src_\"):\n",
        "        del sys.modules[mod]\n",
        "gc.collect()\n",
        "\n",
        "print(\"‚úÖ Fresh start complete\")\n",
        "print(f\"Repo: {TRAIN_REPO_DIR}\")\n",
        "print(f\"Deleted paths: {len(deleted)}\")\n",
        "for d in deleted:\n",
        "    print(\" -\", d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b49d0f2",
      "metadata": {},
      "source": [
        "## 2) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b477656",
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from src.config import get_active_config\n",
        "from src.csv_logger import CSVLogger\n",
        "from src.notebook_helpers.tcn_phase1 import prepare_phase1_dataset, run_experiment6_tape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5b538f",
      "metadata": {},
      "source": [
        "## 3) Base Config and Dataset Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b71d4646",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Feature lock from CORE project pipeline (no metadata/manifest)\n",
        "# ------------------------------------------------------------------\n",
        "from src.data_utils import DataProcessor\n",
        "\n",
        "\n",
        "def build_core_active_feature_columns(cfg):\n",
        "    probe = DataProcessor(cfg)\n",
        "    # DataProcessor.get_feature_columns(\"phase1\") is the canonical feature list\n",
        "    # used by env construction during training.\n",
        "    return list(dict.fromkeys(probe.get_feature_columns(\"phase1\")))\n",
        "\n",
        "\n",
        "def apply_core_feature_lock(cfg, active_feature_columns):\n",
        "    fp = cfg.setdefault(\"feature_params\", {})\n",
        "    fs = fp.setdefault(\"feature_selection\", {})\n",
        "\n",
        "    probe = DataProcessor(cfg)\n",
        "    core_all_cols = list(dict.fromkeys(probe.get_feature_columns(\"phase1\")))\n",
        "\n",
        "    active_set = set(active_feature_columns)\n",
        "    disabled = sorted([c for c in core_all_cols if c not in active_set])\n",
        "\n",
        "    fs[\"disable_features\"] = True\n",
        "    fs[\"disabled_features\"] = disabled\n",
        "\n",
        "    return core_all_cols, disabled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ca80e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_RANDOM_SEED = 42\n",
        "\n",
        "train_config = deepcopy(get_active_config(\"phase1\"))\n",
        "\n",
        "# Optional: override analysis horizon\n",
        "# train_config[\"ANALYSIS_END_DATE\"] = \"2025-09-01\"\n",
        "\n",
        "# Build active features from core project files/pipeline.\n",
        "train_active_feature_columns = build_core_active_feature_columns(train_config)\n",
        "_, train_disabled_features = apply_core_feature_lock(train_config, train_active_feature_columns)\n",
        "\n",
        "print(\"‚úÖ Core feature lock applied\")\n",
        "print(\"   active_feature_columns:\", len(train_active_feature_columns))\n",
        "print(\"   disabled_features:\", len(train_disabled_features))\n",
        "\n",
        "# Force fresh dataset build and market data re-download\n",
        "if \"train_phase1_data\" in globals():\n",
        "    del train_phase1_data\n",
        "\n",
        "train_phase1_data = prepare_phase1_dataset(\n",
        "    train_config,\n",
        "    force_download=True,\n",
        "    preparation_artifacts_dir=\"/content/adaptive_portfolio_rl/data_exports\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32011606",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Train shape:\", train_phase1_data.train_df.shape)\n",
        "print(\"Test shape:\", train_phase1_data.test_df.shape)\n",
        "\n",
        "cols = train_phase1_data.train_df.columns\n",
        "print(\"Total columns:\", len(cols))\n",
        "\n",
        "# quick sanity for common redundant groups\n",
        "dup_like = [c for c in cols if c.endswith(\"_raw\") or c.endswith(\"_unscaled\")]\n",
        "print(\"Potential redundant raw/unscaled cols:\", len(dup_like))\n",
        "print(dup_like[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a81e8d0e",
      "metadata": {},
      "source": [
        "## 4) Training Overrides (Sharpe-Only Checkpoint Policy)\n",
        "\n",
        "This policy keeps only Sharpe-threshold high-watermark checkpointing (`>= 0.5`) and disables rare/step/periodic/TAPE checkpoint routes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2720c6fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "train_config = deepcopy(train_config)  # or deepcopy(config) if that is your active object\n",
        "\n",
        "tp = train_config[\"training_params\"]\n",
        "ap = train_config[\"agent_params\"]\n",
        "ppo = ap[\"ppo_params\"]\n",
        "env = train_config[\"environment_params\"]\n",
        "\n",
        "# Core setup\n",
        "tp[\"max_total_timesteps\"] = 100_000\n",
        "tp[\"timesteps_per_ppo_update\"] = 384  # fallback\n",
        "tp[\"timesteps_per_ppo_update_schedule\"] = [\n",
        "    {\"threshold\": 0, \"timesteps_per_update\": 384},\n",
        "    {\"threshold\": 30_000, \"timesteps_per_update\": 512},\n",
        "]\n",
        "tp[\"batch_size_ppo_schedule\"] = [\n",
        "    {\"threshold\": 0, \"batch_size\": 128},\n",
        "    {\"threshold\": 30_000, \"batch_size\": 128},\n",
        "]\n",
        "\n",
        "# PPO KL management\n",
        "ppo[\"target_kl\"] = 0.020\n",
        "ppo[\"kl_stop_multiplier\"] = 1.25\n",
        "ppo[\"minibatches_before_kl_stop\"] = 2\n",
        "ppo[\"policy_clip\"] = 0.08\n",
        "ppo[\"num_ppo_epochs\"] = 1\n",
        "ppo[\"max_grad_norm\"] = 0.30\n",
        "\n",
        "# LR + entropy\n",
        "ppo[\"actor_lr\"] = 8e-6\n",
        "ppo[\"critic_lr\"] = 1.2e-4\n",
        "ppo[\"entropy_coef\"] = 0.0015\n",
        "\n",
        "# Dirichlet controls\n",
        "ap[\"dirichlet_alpha_activation\"] = \"elu\"\n",
        "ap[\"dirichlet_logit_temperature\"] = 1.20\n",
        "ap[\"dirichlet_alpha_cap\"] = 40.0\n",
        "ap[\"dirichlet_epsilon\"] = {\"max\": 0.9, \"min\": 0.3}\n",
        "\n",
        "# Keep execution/turnover controls\n",
        "tp[\"action_execution_beta_curriculum\"] = {\n",
        "    0: 0.20,\n",
        "    30_000: 0.35,\n",
        "}\n",
        "tp[\"turnover_penalty_curriculum\"] = {\n",
        "    0: 1.50,\n",
        "    10_000: 2.00,\n",
        "    25_000: 2.50,\n",
        "    40_000: 3.00,\n",
        "}\n",
        "env[\"target_turnover\"] = 0.35\n",
        "env[\"turnover_penalty_scalar\"] = 1.50\n",
        "env[\"transaction_cost_pct\"] = 0.001\n",
        "\n",
        "# Sharpe-only checkpoint policy\n",
        "tp[\"high_watermark_checkpoint_enabled\"] = True\n",
        "tp[\"high_watermark_sharpe_threshold\"] = 0.5\n",
        "tp[\"step_sharpe_checkpoint_enabled\"] = False\n",
        "tp[\"periodic_checkpoint_every_steps\"] = 0\n",
        "tp[\"tape_checkpoint_threshold\"] = 999.0\n",
        "tp[\"rare_checkpoint_params\"] = {\"enable\": False}\n",
        "\n",
        "print(\"‚úÖ KL tuning + Sharpe-only checkpoint policy applied\")\n",
        "print(\"target_kl:\", ppo[\"target_kl\"])\n",
        "print(\"rollout schedule:\", tp[\"timesteps_per_ppo_update_schedule\"])\n",
        "print(\"batch schedule:\", tp[\"batch_size_ppo_schedule\"])\n",
        "print(\"dirichlet:\", ap[\"dirichlet_alpha_activation\"], ap[\"dirichlet_logit_temperature\"], ap[\"dirichlet_alpha_cap\"], ap[\"dirichlet_epsilon\"])\n",
        "print(\"high_watermark_sharpe_threshold:\", tp[\"high_watermark_sharpe_threshold\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09063801",
      "metadata": {},
      "source": [
        "## 5) Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0fd91a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_TRAINING = True\n",
        "\n",
        "if RUN_TRAINING:\n",
        "    tp = train_config[\"training_params\"]\n",
        "    print(\"üöÄ Starting training\")\n",
        "    print(\"Architecture:\", train_config[\"agent_params\"].get(\"actor_critic_type\"))\n",
        "    print(\"max_total_timesteps:\", tp[\"max_total_timesteps\"])\n",
        "\n",
        "    train_experiment6 = run_experiment6_tape(\n",
        "        phase1_data=train_phase1_data,\n",
        "        config=train_config,\n",
        "        random_seed=TRAIN_RANDOM_SEED,\n",
        "        csv_logger_cls=CSVLogger,\n",
        "        use_covariance=True,\n",
        "        architecture=train_config[\"agent_params\"].get(\"actor_critic_type\"),\n",
        "        timesteps_per_update=tp.get(\"timesteps_per_ppo_update\", 384),\n",
        "        max_total_timesteps=tp[\"max_total_timesteps\"],\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Training complete\")\n",
        "    print(\"checkpoint_prefix:\", train_experiment6.checkpoint_path)\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è RUN_TRAINING=False\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40ffd0df",
      "metadata": {},
      "source": [
        "## 6) Inspect Latest Training Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "348fd0c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_RESULTS_ROOT = Path(\"/content/adaptive_portfolio_rl/tcn_fusion_results\")\n",
        "TRAIN_LOGS_DIR = TRAIN_RESULTS_ROOT / \"logs\"\n",
        "\n",
        "episodes_files = sorted(TRAIN_LOGS_DIR.glob(\"*episodes*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "if not episodes_files:\n",
        "    print(f\"No episodes CSV found in {TRAIN_LOGS_DIR}\")\n",
        "else:\n",
        "    train_episodes_path = episodes_files[0]\n",
        "    train_episodes_df = pd.read_csv(train_episodes_path)\n",
        "    print(\"Episodes file:\", train_episodes_path)\n",
        "    print(\"Rows:\", len(train_episodes_df))\n",
        "    display(train_episodes_df.tail(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a0e0e1",
      "metadata": {},
      "source": [
        "## 7) Export Results Folder (Optional)\n",
        "Creates a zip for download from Colab VM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366d8cc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "EXPORT_RESULTS_ZIP = False\n",
        "EXPORT_PATH = Path(\"/content/tcn_fusion_results_export.zip\")\n",
        "ROOT = Path(\"/content/adaptive_portfolio_rl\")\n",
        "\n",
        "if EXPORT_RESULTS_ZIP:\n",
        "    include_paths = [\n",
        "        ROOT / \"tcn_fusion_results\",\n",
        "        ROOT / \"data\" / \"phase1_preparation_artifacts\",\n",
        "        ROOT / \"data\" / \"master_features_NORMALIZED.csv\",\n",
        "    ]\n",
        "\n",
        "    existing = [p for p in include_paths if p.exists()]\n",
        "    if not existing:\n",
        "        print(\"‚ö†Ô∏è Nothing to export.\")\n",
        "    else:\n",
        "        if EXPORT_PATH.exists():\n",
        "            EXPORT_PATH.unlink()\n",
        "\n",
        "        rel_items = [str(p.relative_to(ROOT)) for p in existing]\n",
        "        cmd = f\"cd {ROOT} && zip -qr {EXPORT_PATH} \" + \" \".join(rel_items)\n",
        "        subprocess.run(cmd, shell=True, check=True)\n",
        "\n",
        "        print(f\"‚úÖ Created: {EXPORT_PATH}\")\n",
        "        print(\"Included:\")\n",
        "        for p in rel_items:\n",
        "            print(\" -\", p)\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è EXPORT_RESULTS_ZIP=False\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d3f4d04",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/tcn_fusion_results_export.zip /content/drive/MyDrive/\n",
        "print(\"‚úÖ Copied to Drive: /content/drive/MyDrive/tcn_fusion_results_export.zip\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
