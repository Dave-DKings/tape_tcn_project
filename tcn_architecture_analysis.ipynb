{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "64dd4dbc",
      "metadata": {},
      "source": [
        "# TCN Training Only (Clean)\n",
        "\n",
        "This notebook is for **training only**.\n",
        "It uses isolated `train_*` variables and a Sharpe-based checkpoint policy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d402f683",
      "metadata": {},
      "source": [
        "## 1) Connect to Colab VM and Sync Repo\n",
        "Run this first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d81df77",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fresh-start cleanup cell (run before importing project modules)\n",
        "import gc\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_REPO_URL = \"https://github.com/Dave-DKings/tape_tcn_project.git\"\n",
        "TRAIN_REPO_DIR = Path(\"/content/adaptive_portfolio_rl\")\n",
        "\n",
        "# 1) Sync repo to latest main\n",
        "if not (TRAIN_REPO_DIR / \".git\").exists():\n",
        "    subprocess.run([\"git\", \"clone\", TRAIN_REPO_URL, str(TRAIN_REPO_DIR)], check=True)\n",
        "\n",
        "subprocess.run([\"git\", \"-C\", str(TRAIN_REPO_DIR), \"fetch\", \"origin\"], check=True)\n",
        "subprocess.run([\"git\", \"-C\", str(TRAIN_REPO_DIR), \"reset\", \"--hard\", \"origin/main\"], check=True)\n",
        "\n",
        "# 2) Remove old experiment outputs/checkpoints/cached data\n",
        "purge_paths = [\n",
        "    TRAIN_REPO_DIR / \"tcn_fusion_results\",\n",
        "    TRAIN_REPO_DIR / \"tcn_results\",\n",
        "    TRAIN_REPO_DIR / \"tcn_att_results\",\n",
        "    TRAIN_REPO_DIR / \"output_logs\",\n",
        "    TRAIN_REPO_DIR / \"data\" / \"phase1_preparation_artifacts\",\n",
        "    TRAIN_REPO_DIR / \"data\" / \"master_features_NORMALIZED.csv\",\n",
        "    TRAIN_REPO_DIR / \"data\" / \"daily_ohlcv_assets.csv\",              # forces fresh OHLCV download\n",
        "    TRAIN_REPO_DIR / \"data\" / \"processed_daily_macro_features.csv\",   # forces fresh macro cache build\n",
        "]\n",
        "\n",
        "deleted = []\n",
        "for p in purge_paths:\n",
        "    if p.is_dir():\n",
        "        shutil.rmtree(p, ignore_errors=True)\n",
        "        deleted.append(str(p))\n",
        "    elif p.is_file():\n",
        "        p.unlink(missing_ok=True)\n",
        "        deleted.append(str(p))\n",
        "\n",
        "# 3) Remove Python/Jupyter cache folders\n",
        "for cache_dir in TRAIN_REPO_DIR.rglob(\"__pycache__\"):\n",
        "    shutil.rmtree(cache_dir, ignore_errors=True)\n",
        "for ckpt_dir in TRAIN_REPO_DIR.rglob(\".ipynb_checkpoints\"):\n",
        "    shutil.rmtree(ckpt_dir, ignore_errors=True)\n",
        "\n",
        "# 4) Clear loaded project modules from kernel memory\n",
        "for mod in list(sys.modules.keys()):\n",
        "    if mod.startswith(\"src.\") or mod.startswith(\"src_\"):\n",
        "        del sys.modules[mod]\n",
        "gc.collect()\n",
        "\n",
        "print(\"‚úÖ Fresh start complete\")\n",
        "print(f\"Repo: {TRAIN_REPO_DIR}\")\n",
        "print(f\"Deleted paths: {len(deleted)}\")\n",
        "for d in deleted:\n",
        "    print(\" -\", d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9325a176",
      "metadata": {},
      "outputs": [],
      "source": [
        "#from pathlib import Path\n",
        "import os\n",
        "\n",
        "root = Path(\"/content/adaptive_portfolio_rl\")\n",
        "print(\"Exists:\", root.exists())\n",
        "print(\"CWD:\", os.getcwd())\n",
        "\n",
        "print(\"\\nTop-level:\")\n",
        "for p in sorted(root.iterdir()):\n",
        "    kind = \"DIR \" if p.is_dir() else \"FILE\"\n",
        "    print(f\" - [{kind}] {p.name}\")\n",
        "\n",
        "# Quick check for outputs/caches you expected to be deleted\n",
        "targets = [\n",
        "    \"tcn_fusion_results\",\n",
        "    \"tcn_results\",\n",
        "    \"tcn_att_results\",\n",
        "    \"output_logs\",\n",
        "    \"data/phase1_preparation_artifacts\",\n",
        "    \"data/master_features_NORMALIZED.csv\",\n",
        "    \"data/daily_ohlcv_assets.csv\",\n",
        "    \"data/processed_daily_macro_features.csv\",\n",
        "]\n",
        "print(\"\\nTarget paths:\")\n",
        "for t in targets:\n",
        "    p = root / t\n",
        "    print(f\" - {t}: {'EXISTS' if p.exists() else 'MISSING'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c7cda1",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!find /content/adaptive_portfolio_rl -maxdepth 3 | head -n 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09af84a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install project requirements in Colab VM\n",
        "#import subprocess, sys\n",
        "#from pathlib import Path\n",
        "\n",
        "REPO_DIR = Path(\"/content/adaptive_portfolio_rl\")\n",
        "REQ_FILE = REPO_DIR / \"requirements.txt\"\n",
        "\n",
        "if not REQ_FILE.exists():\n",
        "    raise FileNotFoundError(f\"Missing requirements file: {REQ_FILE}\")\n",
        "\n",
        "print(\"Using python:\", sys.executable)\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(REQ_FILE)], check=True)\n",
        "\n",
        "print(\"‚úÖ Requirements installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f880e90",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- GPU sanity/setup for TensorFlow ---\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1) Confirm Colab sees an NVIDIA GPU\n",
        "!nvidia-smi -L\n",
        "\n",
        "# 2) Confirm TensorFlow sees GPU(s)\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "print(\"TF GPUs:\", gpus)\n",
        "if not gpus:\n",
        "    raise RuntimeError(\"No GPU visible to TensorFlow. In Colab: Runtime -> Change runtime type -> GPU\")\n",
        "\n",
        "# 3) Safer GPU memory behavior\n",
        "for g in gpus:\n",
        "    tf.config.experimental.set_memory_growth(g, True)\n",
        "\n",
        "# 4) Optional: speed boost on modern GPUs\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "print(\"Mixed precision policy:\", tf.keras.mixed_precision.global_policy())\n",
        "\n",
        "# 5) Quick proof op runs on GPU\n",
        "with tf.device(\"/GPU:0\"):\n",
        "    a = tf.random.normal((4096, 4096))\n",
        "    b = tf.random.normal((4096, 4096))\n",
        "    c = tf.matmul(a, b)\n",
        "\n",
        "print(\"Matmul device:\", c.device)\n",
        "print(\"Default GPU device name:\", tf.test.gpu_device_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14187b25",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy, pandas, tensorflow\n",
        "print(\"numpy\", numpy.__version__)\n",
        "print(\"pandas\", pandas.__version__)\n",
        "print(\"tensorflow\", tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b49d0f2",
      "metadata": {},
      "source": [
        "## 2) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "641959e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = Path(\"/content/adaptive_portfolio_rl\")\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Repo not found: {REPO_DIR}\")\n",
        "\n",
        "# Set working directory\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# Add repo root to Python path\n",
        "if str(REPO_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_DIR))\n",
        "\n",
        "print(\"cwd:\", os.getcwd())\n",
        "print(\"sys.path[0]:\", sys.path[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b477656",
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from src.config import get_active_config\n",
        "from src.csv_logger import CSVLogger\n",
        "from src.notebook_helpers.tcn_phase1 import prepare_phase1_dataset, run_experiment6_tape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5b538f",
      "metadata": {},
      "source": [
        "## 3) Base Config and Dataset Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b71d4646",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Feature lock from CORE project pipeline (no metadata/manifest)\n",
        "# ------------------------------------------------------------------\n",
        "from src.data_utils import DataProcessor\n",
        "\n",
        "\n",
        "def build_core_active_feature_columns(cfg):\n",
        "    # Active list as defined by core config + current selection rules.\n",
        "    probe = DataProcessor(cfg)\n",
        "    return list(dict.fromkeys(probe.get_feature_columns(\"phase1\")))\n",
        "\n",
        "\n",
        "def apply_core_feature_lock(cfg, active_feature_columns):\n",
        "    # Compute full candidate pool with selection filter temporarily disabled,\n",
        "    # then enforce active-only by setting disabled_features = full - active.\n",
        "    probe_cfg = deepcopy(cfg)\n",
        "    probe_fp = probe_cfg.setdefault(\"feature_params\", {})\n",
        "    probe_fs = probe_fp.setdefault(\"feature_selection\", {})\n",
        "    probe_fs[\"disable_features\"] = False\n",
        "    probe_fs[\"disabled_features\"] = []\n",
        "\n",
        "    probe = DataProcessor(probe_cfg)\n",
        "    core_all_cols = list(dict.fromkeys(probe.get_feature_columns(\"phase1\")))\n",
        "\n",
        "    active_set = set(active_feature_columns)\n",
        "    disabled = sorted([c for c in core_all_cols if c not in active_set])\n",
        "\n",
        "    fp = cfg.setdefault(\"feature_params\", {})\n",
        "    fs = fp.setdefault(\"feature_selection\", {})\n",
        "    fs[\"disable_features\"] = True\n",
        "    fs[\"disabled_features\"] = disabled\n",
        "\n",
        "    return core_all_cols, disabled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ca80e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_RANDOM_SEED = 42\n",
        "\n",
        "train_config = deepcopy(get_active_config(\"phase1\"))\n",
        "\n",
        "# Optional: override analysis horizon\n",
        "# train_config[\"ANALYSIS_END_DATE\"] = \"2025-09-01\"\n",
        "\n",
        "# Build active features from core project files/pipeline.\n",
        "train_active_feature_columns = build_core_active_feature_columns(train_config)\n",
        "_, train_disabled_features = apply_core_feature_lock(train_config, train_active_feature_columns)\n",
        "\n",
        "print(\"‚úÖ Core feature lock applied\")\n",
        "print(\"   active_feature_columns:\", len(train_active_feature_columns))\n",
        "print(\"   disabled_features:\", len(train_disabled_features))\n",
        "\n",
        "# Force fresh dataset build and market data re-download\n",
        "if \"train_phase1_data\" in globals():\n",
        "    del train_phase1_data\n",
        "\n",
        "train_phase1_data = prepare_phase1_dataset(\n",
        "    train_config,\n",
        "    force_download=True,\n",
        "    preparation_artifacts_dir=\"/content/adaptive_portfolio_rl/data_exports\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32011606",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Train shape:\", train_phase1_data.train_df.shape)\n",
        "print(\"Test shape:\", train_phase1_data.test_df.shape)\n",
        "\n",
        "cols = train_phase1_data.train_df.columns\n",
        "print(\"Total columns:\", len(cols))\n",
        "\n",
        "# quick sanity for common redundant groups\n",
        "dup_like = [c for c in cols if c.endswith(\"_raw\") or c.endswith(\"_unscaled\")]\n",
        "print(\"Potential redundant raw/unscaled cols:\", len(dup_like))\n",
        "print(dup_like[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee488d70",
      "metadata": {},
      "outputs": [],
      "source": [
        "cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f32ef47",
      "metadata": {},
      "outputs": [],
      "source": [
        "used = set(train_phase1_data.data_processor.get_feature_columns(\"phase1\"))\n",
        "disabled = set(train_config[\"feature_params\"][\"feature_selection\"][\"disabled_features\"])\n",
        "\n",
        "print(\"Used feature count:\", len(used))\n",
        "print(\"Disabled that still in used:\", sorted(disabled & used))  # should be []\n",
        "print(\"VIX_zscore used?\", \"VIX_zscore\" in used)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e85bb7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "base_cols = [\"Date\", \"Ticker\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "keep = [c for c in base_cols + list(used) if c in train_phase1_data.master_df.columns]\n",
        "\n",
        "train_phase1_data.master_df = train_phase1_data.master_df[keep].copy()\n",
        "train_phase1_data.train_df = train_phase1_data.train_df[keep].copy()\n",
        "train_phase1_data.test_df  = train_phase1_data.test_df[keep].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a81e8d0e",
      "metadata": {},
      "source": [
        "## 4) Training Overrides (Sharpe-Only Checkpoint Policy)\n",
        "\n",
        "This policy keeps only Sharpe-threshold high-watermark checkpointing (`>= 0.5`) and disables rare/step/periodic/TAPE checkpoint routes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2720c6fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# NEXT RUN OVERRIDES (post-mortem tuned: KL stability + turnover control)\n",
        "# ============================================================================\n",
        "from copy import deepcopy\n",
        "\n",
        "train_config = deepcopy(train_config)  # or deepcopy(config) if that's your active object\n",
        "\n",
        "tp = train_config[\"training_params\"]\n",
        "ap = train_config[\"agent_params\"]\n",
        "ppo = ap[\"ppo_params\"]\n",
        "env = train_config[\"environment_params\"]\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1) Core run shape\n",
        "# ----------------------------------------------------------------------------\n",
        "tp[\"max_total_timesteps\"] = 100_000\n",
        "tp[\"timesteps_per_ppo_update\"] = 384  # fallback\n",
        "\n",
        "tp[\"timesteps_per_ppo_update_schedule\"] = [\n",
        "    {\"threshold\": 0, \"timesteps_per_update\": 384},\n",
        "    {\"threshold\": 50_000, \"timesteps_per_update\": 448},\n",
        "]\n",
        "\n",
        "tp[\"batch_size_ppo_schedule\"] = [\n",
        "    {\"threshold\": 0, \"batch_size\": 96},\n",
        "    {\"threshold\": 50_000, \"batch_size\": 112},\n",
        "]\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 2) PPO stability (reduce aggressiveness)\n",
        "# ----------------------------------------------------------------------------\n",
        "ppo[\"num_ppo_epochs\"] = 1\n",
        "ppo[\"policy_clip\"] = 0.08\n",
        "ppo[\"target_kl\"] = 0.020\n",
        "ppo[\"kl_stop_multiplier\"] = 1.25\n",
        "ppo[\"minibatches_before_kl_stop\"] = 2\n",
        "ppo[\"max_grad_norm\"] = 0.30\n",
        "\n",
        "ppo[\"actor_lr\"] = 8e-6\n",
        "ppo[\"critic_lr\"] = 1.2e-4\n",
        "ppo[\"entropy_coef\"] = 0.0020\n",
        "\n",
        "tp[\"actor_lr_schedule\"] = [\n",
        "    {\"threshold\": 0, \"lr\": 8e-6},\n",
        "    {\"threshold\": 30_000, \"lr\": 7e-6},\n",
        "    {\"threshold\": 60_000, \"lr\": 6e-6},\n",
        "]\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 3) RA-KL (less aggressive, prevent floor lock)\n",
        "# ----------------------------------------------------------------------------\n",
        "tp[\"ra_kl_enabled\"] = True\n",
        "tp[\"ra_kl_target_ratio\"] = 1.0\n",
        "tp[\"ra_kl_ema_alpha\"] = 0.25\n",
        "tp[\"ra_kl_gain\"] = 0.03\n",
        "tp[\"ra_kl_deadband\"] = 0.20\n",
        "tp[\"ra_kl_max_change_fraction\"] = 0.05\n",
        "tp[\"ra_kl_min_target_kl\"] = 0.016\n",
        "tp[\"ra_kl_max_target_kl\"] = 0.030\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 4) Dirichlet + concentration controls\n",
        "# ----------------------------------------------------------------------------\n",
        "ap[\"dirichlet_alpha_activation\"] = \"elu\"\n",
        "ap[\"dirichlet_logit_temperature\"] = 1.15\n",
        "ap[\"dirichlet_alpha_cap\"] = 35.0\n",
        "ap[\"dirichlet_epsilon\"] = {\"max\": 1.0, \"min\": 0.5}\n",
        "\n",
        "env[\"concentration_penalty_scalar\"] = 3.0\n",
        "env[\"concentration_target_hhi\"] = 0.12\n",
        "env[\"top_weight_penalty_scalar\"] = 2.0\n",
        "env[\"action_realization_penalty_scalar\"] = 0.5\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 5) Turnover + execution smoothing\n",
        "# ----------------------------------------------------------------------------\n",
        "env[\"target_turnover\"] = 0.35\n",
        "env[\"turnover_penalty_scalar\"] = 1.5\n",
        "env[\"transaction_cost_pct\"] = 0.001\n",
        "\n",
        "tp[\"action_execution_beta_curriculum\"] = {\n",
        "    0: 0.20,\n",
        "    30_000: 0.35,\n",
        "}\n",
        "tp[\"evaluation_action_execution_beta\"] = 0.35\n",
        "\n",
        "tp[\"turnover_penalty_curriculum\"] = {\n",
        "    0: 1.5,\n",
        "    10_000: 2.0,\n",
        "    25_000: 2.5,\n",
        "    40_000: 3.0,\n",
        "}\n",
        "tp[\"evaluation_turnover_penalty_scalar\"] = 3.0\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6) Episode horizon curriculum (keep cap late)\n",
        "# ----------------------------------------------------------------------------\n",
        "tp[\"use_episode_length_curriculum\"] = True\n",
        "tp[\"episode_length_curriculum_schedule\"] = [\n",
        "    {\"threshold\": 0, \"limit\": 252},\n",
        "    {\"threshold\": 10_000, \"limit\": 504},\n",
        "    {\"threshold\": 25_000, \"limit\": 756},\n",
        "    {\"threshold\": 90_000, \"limit\": 1008},\n",
        "]\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 7) Logging + checkpoints\n",
        "# ----------------------------------------------------------------------------\n",
        "tp[\"log_step_diagnostics\"] = True\n",
        "tp[\"update_log_interval\"] = 5\n",
        "tp[\"alpha_diversity_log_interval\"] = 1\n",
        "tp[\"alpha_diversity_warning_after_updates\"] = 120\n",
        "tp[\"alpha_diversity_warning_std_threshold\"] = 0.25\n",
        "\n",
        "tp[\"high_watermark_checkpoint_enabled\"] = True\n",
        "tp[\"high_watermark_sharpe_threshold\"] = 0.5\n",
        "tp[\"step_sharpe_checkpoint_enabled\"] = False\n",
        "tp[\"periodic_checkpoint_every_steps\"] = 0\n",
        "tp[\"rare_checkpoint_params\"] = {\"enable\": False}\n",
        "tp[\"tape_checkpoint_threshold\"] = 999.0\n",
        "\n",
        "print(\"‚úÖ Applied next-run override (KL-stable + smoother execution + moderate turnover control)\")\n",
        "print(\"num_ppo_epochs:\", ppo[\"num_ppo_epochs\"])\n",
        "print(\"target_kl:\", ppo[\"target_kl\"], \"| kl_stop_multiplier:\", ppo[\"kl_stop_multiplier\"])\n",
        "print(\"RA-KL:\", {k: tp[k] for k in [\n",
        "    \"ra_kl_enabled\", \"ra_kl_gain\", \"ra_kl_deadband\",\n",
        "    \"ra_kl_max_change_fraction\", \"ra_kl_min_target_kl\", \"ra_kl_max_target_kl\"\n",
        "]})\n",
        "print(\"action_execution_beta_curriculum:\", tp[\"action_execution_beta_curriculum\"])\n",
        "print(\"turnover_penalty_curriculum:\", tp[\"turnover_penalty_curriculum\"])\n",
        "print(\"concentration:\", env[\"concentration_penalty_scalar\"], env[\"concentration_target_hhi\"], env[\"top_weight_penalty_scalar\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09063801",
      "metadata": {},
      "source": [
        "## 5) Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0fd91a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_TRAINING = True\n",
        "\n",
        "if RUN_TRAINING:\n",
        "    tp = train_config[\"training_params\"]\n",
        "    print(\"üöÄ Starting training\")\n",
        "    print(\"Architecture:\", train_config[\"agent_params\"].get(\"actor_critic_type\"))\n",
        "    print(\"max_total_timesteps:\", tp[\"max_total_timesteps\"])\n",
        "\n",
        "    train_experiment6 = run_experiment6_tape(\n",
        "        phase1_data=train_phase1_data,\n",
        "        config=train_config,\n",
        "        random_seed=TRAIN_RANDOM_SEED,\n",
        "        csv_logger_cls=CSVLogger,\n",
        "        use_covariance=True,\n",
        "        architecture=train_config[\"agent_params\"].get(\"actor_critic_type\"),\n",
        "        timesteps_per_update=tp.get(\"timesteps_per_ppo_update\", 384),\n",
        "        max_total_timesteps=tp[\"max_total_timesteps\"],\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Training complete\")\n",
        "    print(\"checkpoint_prefix:\", train_experiment6.checkpoint_path)\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è RUN_TRAINING=False\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40ffd0df",
      "metadata": {},
      "source": [
        "## 6) Inspect Latest Training Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "348fd0c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_RESULTS_ROOT = Path(\"/content/adaptive_portfolio_rl/tcn_fusion_results\")\n",
        "TRAIN_LOGS_DIR = TRAIN_RESULTS_ROOT / \"logs\"\n",
        "\n",
        "episodes_files = sorted(TRAIN_LOGS_DIR.glob(\"*episodes*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "if not episodes_files:\n",
        "    print(f\"No episodes CSV found in {TRAIN_LOGS_DIR}\")\n",
        "else:\n",
        "    train_episodes_path = episodes_files[0]\n",
        "    train_episodes_df = pd.read_csv(train_episodes_path)\n",
        "    print(\"Episodes file:\", train_episodes_path)\n",
        "    print(\"Rows:\", len(train_episodes_df))\n",
        "    display(train_episodes_df.tail(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb04a33",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_episodes_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a0e0e1",
      "metadata": {},
      "source": [
        "## 7) Export Results Folder (Optional)\n",
        "Creates a zip for download from Colab VM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366d8cc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "EXPORT_RESULTS_ZIP = True\n",
        "EXPORT_PATH = Path(\"/content/tcn_fusion_results_export2.zip\")\n",
        "ROOT = Path(\"/content/adaptive_portfolio_rl\")\n",
        "\n",
        "if EXPORT_RESULTS_ZIP:\n",
        "    include_paths = [\n",
        "        ROOT / \"tcn_fusion_results\",\n",
        "        ROOT / \"data\" / \"phase1_preparation_artifacts\",\n",
        "        ROOT / \"data\" / \"master_features_NORMALIZED.csv\",\n",
        "    ]\n",
        "\n",
        "    existing = [p for p in include_paths if p.exists()]\n",
        "    if not existing:\n",
        "        print(\"‚ö†Ô∏è Nothing to export.\")\n",
        "    else:\n",
        "        if EXPORT_PATH.exists():\n",
        "            EXPORT_PATH.unlink()\n",
        "\n",
        "        rel_items = [str(p.relative_to(ROOT)) for p in existing]\n",
        "        cmd = f\"cd {ROOT} && zip -qr {EXPORT_PATH} \" + \" \".join(rel_items)\n",
        "        subprocess.run(cmd, shell=True, check=True)\n",
        "\n",
        "        print(f\"‚úÖ Created: {EXPORT_PATH}\")\n",
        "        print(\"Included:\")\n",
        "        for p in rel_items:\n",
        "            print(\" -\", p)\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è EXPORT_RESULTS_ZIP=False\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d3f4d04",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/tcn_fusion_results_export2.zip /content/drive/MyDrive/\n",
        "print(\"‚úÖ Copied to Drive: /content/drive/MyDrive/tcn_fusion_results_export2.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bf11d2",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
