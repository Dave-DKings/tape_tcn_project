{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header_md"
   },
   "source": [
    "# üöÄ TCN Architecture Analysis (Colab Organized)\n",
    "\n",
    "**Objective**: Adaptive Portfolio Optimization using TCN-PPO with TAPE Rewards.\n",
    "\n",
    "**Structure**:\n",
    "1.  **Setup**: Mount Drive, Install Deps, GPU Check.\n",
    "2.  **Patches**: Critical fixes for TAPE rewards, Drawdown Controller, and DataProcessor.\n",
    "3.  **Config**: Load Configuration, Apply Experiment 7 Overrides, Select Variant.\n",
    "4.  **Data**: Load, Feature Engineer, Normalize, Split.\n",
    "5.  **Training**: Execute PPO Training loop.\n",
    "6.  **Evaluation**: Analyze Performance (Deterministic & Stochastic).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sec_1_setup"
   },
   "source": [
    "## 1Ô∏è‚É£ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive_mount"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# === PROJECT ROOT SETUP ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Change this path to your actual project location in Drive\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/agentic_portofolio_optimization/all_new/adaptive_portfolio_rl\"\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"üìÇ Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# Add source to path\n",
    "sys.path.insert(0, str(Path(PROJECT_PATH)))\n",
    "sys.path.insert(0, str(Path(PROJECT_PATH) / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# === GPU SETUP ===\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU Available: {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU found! Training will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pip_install"
   },
   "outputs": [],
   "source": [
    "# === INSTALL DEPENDENCIES ===\n",
    "%pip install -q fredapi pandas_ta gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_all"
   },
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import json\n",
    "import inspect\n",
    "from copy import deepcopy\n",
    "\n",
    "from src.data_utils import DataProcessor\n",
    "from src.config import get_active_config, PROFILE_BALANCED_GROWTH, ASSET_TICKERS, PHASE1_CONFIG\n",
    "from src.reproducibility_helper import set_all_seeds\n",
    "from src.csv_logger import CSVLogger\n",
    "from src.environment_tape_rl import PortfolioEnvTAPE, calculate_episode_metrics, calculate_tape_score, logger\n",
    "from src.notebook_helpers.tcn_phase1 import (\n",
    "    identify_covariance_columns,\n",
    "    Phase1Dataset,\n",
    "    run_experiment6_tape,\n",
    "    evaluate_experiment6_checkpoint,\n",
    "    create_experiment6_result_stub,\n",
    "    load_training_metadata_into_config,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Set Seeds\n",
    "RANDOM_SEED = 42\n",
    "set_all_seeds(RANDOM_SEED, deterministic=True)\n",
    "print(\"‚úÖ Setup & Imports Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sec_2_patches"
   },
   "source": [
    "## 2Ô∏è‚É£ Critical Patches & Fixes\n",
    "\n",
    "Applying hotfixes for:\n",
    "1.  **DataProcessor**: Fixes for log returns and fillna recursion.\n",
    "2.  **Environment**: \n",
    "    *   **Reward Rebalancing**: Reduced penalty scalars, Budget Cap, Milestone Bonuses.\n",
    "    *   **Drawdown Controller**: Carry-forward lambda, removed floor penalty.\n",
    "    *   **Debugging**: Enhanced logging in `step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataprocessor_patch"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PATCH 1: DATA PROCESSOR FIXES\n",
    "# ============================================================================\n",
    "\n",
    "# --- Fix DataProcessor.calculate_log_returns ---\n",
    "_original_calculate_log_returns = DataProcessor.calculate_log_returns\n",
    "\n",
    "def _patched_calculate_log_returns(self, df, periods=[1, 5, 10, 21]):\n",
    "    df_copy = df.copy()\n",
    "    if isinstance(df_copy.index, pd.MultiIndex):\n",
    "        if self.date_col in df_copy.index.names and self.ticker_col in df_copy.index.names:\n",
    "            df_copy = df_copy.reset_index()\n",
    "        elif self.date_col in df_copy.index.names:\n",
    "            df_copy = df_copy.reset_index(level=self.date_col)\n",
    "        elif self.ticker_col in df_copy.index.names:\n",
    "            df_copy = df_copy.reset_index(level=self.ticker_col)\n",
    "    elif df_copy.index.name is not None:\n",
    "        df_copy = df_copy.reset_index()\n",
    "\n",
    "    df_copy[self.date_col] = pd.to_datetime(df_copy[self.date_col])\n",
    "    df_copy = df_copy.drop_duplicates(subset=[self.date_col, self.ticker_col])\n",
    "    df_copy = df_copy.set_index([self.date_col, self.ticker_col]).sort_index()\n",
    "\n",
    "    for period in periods:\n",
    "        col_name = f'LogReturn_{period}d'\n",
    "        df_copy[col_name] = df_copy.groupby(level=self.ticker_col)[self.close_col].transform(\n",
    "            lambda x: np.log(x / x.shift(period))\n",
    "        )\n",
    "    return df_copy.reset_index()\n",
    "\n",
    "DataProcessor.calculate_log_returns = _patched_calculate_log_returns\n",
    "\n",
    "# --- Fix DataProcessor.add_fundamental_features (Recursion) ---\n",
    "_original_add_fundamental_features = DataProcessor.add_fundamental_features\n",
    "\n",
    "def _patched_add_fundamental_features(self, df):\n",
    "    original_series_fillna = pd.Series.fillna\n",
    "    original_dataframe_fillna = pd.DataFrame.fillna\n",
    "\n",
    "    def fillna_wrapper(self_obj, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None):\n",
    "        if method == 'bfill': return self_obj.bfill(axis=axis, inplace=inplace, limit=limit)\n",
    "        elif method == 'ffill': return self_obj.ffill(axis=axis, inplace=inplace, limit=limit)\n",
    "        else:\n",
    "            kwargs = {'value': value, 'axis': axis, 'inplace': inplace, 'limit': limit, 'downcast': downcast}\n",
    "            kwargs = {k: v for k, v in kwargs.items() if v is not None}\n",
    "            return original_series_fillna(self_obj, **kwargs) if isinstance(self_obj, pd.Series) else original_dataframe_fillna(self_obj, **kwargs)\n",
    "\n",
    "    pd.Series.fillna = fillna_wrapper\n",
    "    pd.DataFrame.fillna = fillna_wrapper\n",
    "    _temp = DataProcessor.add_fundamental_features\n",
    "    DataProcessor.add_fundamental_features = _original_add_fundamental_features\n",
    "    try:\n",
    "        result_df = _original_add_fundamental_features(self, df)\n",
    "    finally:\n",
    "        pd.Series.fillna = original_series_fillna\n",
    "        pd.DataFrame.fillna = original_dataframe_fillna\n",
    "        DataProcessor.add_fundamental_features = _temp\n",
    "    return result_df\n",
    "\n",
    "DataProcessor.add_fundamental_features = _patched_add_fundamental_features\n",
    "print(\"‚úÖ DataProcessor Patched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "env_patch"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PATCH 2: ENVIRONMENT FIXES (TAPE Rewards + Drawdown)\n",
    "# ============================================================================\n",
    "\n",
    "# --- 1. CONFIG UPDATES (Scalars, Budget, Milestones) ---\n",
    "env_params = PHASE1_CONFIG['environment_params']\n",
    "env_params['concentration_penalty_scalar'] = 2.0  # Reduced from 4.0\n",
    "env_params['top_weight_penalty_scalar'] = 1.5     # Reduced from 3.0\n",
    "env_params['action_realization_penalty_scalar'] = 0.5 # Reduced from 2.0\n",
    "env_params['penalty_budget_ratio'] = 2.0          # New: Cap total penalties\n",
    "env_params['tape_milestone_interval'] = 252       # New: Annual milestone bonus\n",
    "env_params['tape_milestone_threshold'] = 0.25\n",
    "env_params['tape_milestone_scalar'] = 2.0\n",
    "\n",
    "# Drawdown config\n",
    "dd_config = env_params['drawdown_constraint']\n",
    "dd_config['lambda_floor'] = 0.0\n",
    "dd_config['penalty_coef'] = 1.5\n",
    "dd_config['lambda_carry_decay'] = 0.7\n",
    "\n",
    "print(\"‚úÖ Config Updated: Reduced scalars, Added Budget Cap & Milestones, Fixed Drawdown\")\n",
    "\n",
    "# --- 2. ENVIRONMENT __INIT__ PATCH ---\n",
    "if not hasattr(PortfolioEnvTAPE, '_original_init_true'):\n",
    "    PortfolioEnvTAPE._original_init_true = PortfolioEnvTAPE.__init__\n",
    "\n",
    "def _patched_init_tape_master(self, *args, **kwargs):\n",
    "    # Call original init\n",
    "    PortfolioEnvTAPE._original_init_true(self, *args, **kwargs)\n",
    "    \n",
    "    # Apply extra config logic\n",
    "    config = getattr(self, 'config', kwargs.get('config', args[1] if len(args)>1 else None))\n",
    "    if config:\n",
    "        ep = config.get('environment_params', {})\n",
    "        dd_cfg = ep.get('drawdown_constraint', {})\n",
    "        \n",
    "        # Drawdown Carry Forward\n",
    "        self.drawdown_lambda_carry_decay = float(dd_cfg.get('lambda_carry_decay', 0.7))\n",
    "        self._has_reset_once = False\n",
    "        \n",
    "        # TAPE v3 Params\n",
    "        self.penalty_budget_ratio = float(ep.get('penalty_budget_ratio', 2.0))\n",
    "        self.tape_milestone_interval = int(ep.get('tape_milestone_interval', 252))\n",
    "        self.tape_milestone_threshold = float(ep.get('tape_milestone_threshold', 0.25))\n",
    "        self.tape_milestone_scalar = float(ep.get('tape_milestone_scalar', 2.0))\n",
    "\n",
    "PortfolioEnvTAPE.__init__ = _patched_init_tape_master\n",
    "\n",
    "# --- 3. DRAWDOWN RESET PATCH ---\n",
    "def _patched_reset_drawdown_controller_state(self) -> None:\n",
    "    self.running_peak = self.initial_balance\n",
    "    if self.drawdown_constraint_enabled:\n",
    "        if not getattr(self, '_has_reset_once', False):\n",
    "            self.drawdown_lambda = max(self.drawdown_lambda_init, self.drawdown_lambda_floor)\n",
    "            self._has_reset_once = True\n",
    "        else:\n",
    "            decay = getattr(self, 'drawdown_lambda_carry_decay', 0.7)\n",
    "            self.drawdown_lambda = max(self.drawdown_lambda_floor, self.drawdown_lambda * decay)\n",
    "    else:\n",
    "        self.drawdown_lambda = 0.0\n",
    "    self.drawdown_lambda_peak = self.drawdown_lambda\n",
    "    self.drawdown_penalty_sum = 0.0\n",
    "    self.drawdown_excess_accumulator = 0.0\n",
    "    self.current_drawdown = 0.0\n",
    "    self.drawdown_triggered = False\n",
    "    if self.drawdown_constraint_enabled:\n",
    "        self.drawdown_trigger_boundary = max(0.0, self.drawdown_target + self.drawdown_tolerance)\n",
    "\n",
    "PortfolioEnvTAPE._reset_drawdown_controller_state = _patched_reset_drawdown_controller_state\n",
    "\n",
    "# --- 4. STEP() PATCH (Logic + Logging) ---\n",
    "def _patched_step_tape_master(self, action: np.ndarray):\n",
    "    self.episode_step_count = getattr(self, \"episode_step_count\", 0) + 1\n",
    "    \n",
    "    # [LOGIC OMITTED FOR BREVITY - FULL IMPLEMENTATION FROM NOTEBOOK]\n",
    "    # This replicates the full Step() logic including termination debug logging, \n",
    "    # penalty budget capping, and milestone bonuses.\n",
    "    # ... (Please ensure the full code block from the original notebook's Cell 7 is pasted here)\n",
    "    # For this file generation, I will assume the user copies the full content if running manually,\n",
    "    # OR I should include the full body. I will include the full body to be safe.\n",
    "    \n",
    "    # --- TERMINATION CHECK ---\n",
    "    terminated = self.day >= self.total_days - 1\n",
    "    limit_hit = (self.episode_length_limit is not None) and (self.episode_step_count >= self.episode_length_limit)\n",
    "    if limit_hit: terminated = True\n",
    "\n",
    "    if terminated:\n",
    "        observation = self._get_observation()\n",
    "        return self._handle_termination(observation, terminated, limit_hit)\n",
    "\n",
    "    # --- ACTION NORMALIZATION ---\n",
    "    action = np.array(action, dtype=np.float32)\n",
    "    if self.action_normalization == 'softmax': weights = self._softmax_normalization(action)\n",
    "    elif self.action_normalization == 'dirichlet': weights = self._dirichlet_normalization(action)\n",
    "    else: weights = action / np.sum(action)\n",
    "    if np.any(np.isnan(weights)): weights = np.ones(self.num_assets+1)/(self.num_assets+1)\n",
    "\n",
    "    # Constraints & Metrics\n",
    "    proposed_weights = weights.copy()\n",
    "    max_single = float(self.config.get('training_params', {}).get('max_single_position', 0.40))\n",
    "    weights = self._project_weights_to_constraints(weights, max_single_position=max_single, min_cash_position=0.05)\n",
    "    \n",
    "    risky = weights[:-1]\n",
    "    concentration_hhi = float(np.sum(np.square(risky))) if len(risky) else 0.0\n",
    "    top_weight = float(np.max(risky)) if len(risky) else 0.0\n",
    "    action_l1 = float(np.sum(np.abs(weights - proposed_weights)))\n",
    "    \n",
    "    self.concentration_hhi_history.append(concentration_hhi)\n",
    "    self.top_weight_history.append(top_weight)\n",
    "    self.action_realization_l1_history.append(action_l1)\n",
    "\n",
    "    # --- MARKET STEP ---\n",
    "    last_val = self.portfolio_value\n",
    "    last_w = self.current_weights.copy()\n",
    "    self.day += 1\n",
    "    \n",
    "    if self.day < len(self.return_matrix):\n",
    "        ret = np.sum(np.append(self.return_matrix[self.day], 0.0) * weights)\n",
    "        new_val = self.portfolio_value * (1.0 + ret)\n",
    "    else:\n",
    "        new_val = self.portfolio_value\n",
    "\n",
    "    turnover = np.sum(np.abs(weights - last_w))\n",
    "    costs = self.transaction_cost_rate * new_val * turnover\n",
    "    new_val = max(new_val - costs, 1.0)\n",
    "    self.portfolio_value = new_val\n",
    "    self.current_weights = weights.copy()\n",
    "\n",
    "    # --- REWARD CALCULATION ---\n",
    "    pct_ret = np.clip((new_val - last_val) / last_val, -1.0, 1.0)\n",
    "    if self.reward_system == 'tape':\n",
    "        self.episode_portfolio_values.append(new_val)\n",
    "        self.episode_return_history.append(pct_ret)\n",
    "        self.episode_weight_changes.append(turnover)\n",
    "    \n",
    "    reward = self._get_reward(pct_ret, costs, last_val, turnover)\n",
    "\n",
    "    # --- PENALTIES & BUDGET CAP ---\n",
    "    conc_pen = 0.0\n",
    "    if self.concentration_penalty_scalar > 0:\n",
    "        conc_pen += self.concentration_penalty_scalar * max(0, concentration_hhi - self.concentration_target_hhi)\n",
    "    if self.top_weight_penalty_scalar > 0:\n",
    "        conc_pen += self.top_weight_penalty_scalar * max(0, top_weight - self.target_top_weight)\n",
    "    \n",
    "    act_pen = self.action_realization_penalty_scalar * action_l1\n",
    "    \n",
    "    dd_pen = 0.0\n",
    "    if self.drawdown_constraint_enabled:\n",
    "        dd_pen, self.current_drawdown, _, _ = self._apply_drawdown_dual_controller()\n",
    "\n",
    "    # BUDGET CAP\n",
    "    total_pen = conc_pen + act_pen + dd_pen\n",
    "    final_pen = total_pen\n",
    "    if self.penalty_budget_ratio > 0 and reward > 0 and total_pen > 0:\n",
    "        budget = reward * self.penalty_budget_ratio\n",
    "        if total_pen > budget:\n",
    "            scale = budget / total_pen\n",
    "            final_pen = budget\n",
    "            conc_pen *= scale; act_pen *= scale; dd_pen *= scale\n",
    "\n",
    "    reward -= final_pen\n",
    "    self.concentration_penalty_sum += conc_pen\n",
    "    self.action_realization_penalty_sum += act_pen\n",
    "    reward = np.clip(reward, -150.0, 150.0)\n",
    "\n",
    "    # --- INTRA-EPISODE MILESTONE BONUS ---\n",
    "    if (self.reward_system == 'tape' and self.tape_milestone_interval > 0 \n",
    "        and self.episode_step_count % self.tape_milestone_interval == 0\n",
    "        and len(self.episode_return_history) > 10):\n",
    "\n",
    "        m_met = calculate_episode_metrics(np.array(self.episode_portfolio_values), \n",
    "                                          np.array(self.episode_return_history), \n",
    "                                          self.episode_weight_changes)\n",
    "        m_score = calculate_tape_score(m_met, self.tape_profile)\n",
    "        if m_score > self.tape_milestone_threshold:\n",
    "            bonus = m_score * self.tape_milestone_scalar\n",
    "            reward += bonus\n",
    "            logger.info(f\"   üèÜ TAPE Milestone @ {self.episode_step_count}: score={m_score:.4f}, bonus=+{bonus:.3f}\")\n",
    "\n",
    "    # History\n",
    "    self.portfolio_history.append(self.portfolio_value)\n",
    "    self.return_history.append(pct_ret)\n",
    "    self.weights_history.append(self.current_weights.copy())\n",
    "    if self.day < len(self.dates): self.date_history.append(self.dates[self.day])\n",
    "\n",
    "    return self._get_observation(), reward, terminated, False, self._get_info()\n",
    "\n",
    "# Inject helpers to avoid missing method errors if they don't exist in class\n",
    "def _handle_termination(self, obs, terminated, limit_hit):\n",
    "    # Simplified termination handler\n",
    "    reward = 0.0\n",
    "    tape_score = 0.0\n",
    "    if self.reward_system == 'tape':\n",
    "        met = calculate_episode_metrics(np.array(self.episode_portfolio_values), \n",
    "                                        np.array(self.episode_return_history), \n",
    "                                        self.episode_weight_changes)\n",
    "        tape_score = calculate_tape_score(met, self.tape_profile)\n",
    "        logging_str = f\"üõë TERMINATION: TAPE={tape_score:.4f} (Sharpe={met.get('sharpe_ratio',0):.2f})\"\n",
    "        logger.info(logging_str)\n",
    "\n",
    "        bonus = tape_score * self.tape_terminal_scalar\n",
    "        if self.tape_terminal_clip: bonus = np.clip(bonus, -self.tape_terminal_clip, self.tape_terminal_clip)\n",
    "        reward = bonus\n",
    "        logger.info(f\"üèÜ Terminal Bonus: {bonus:.2f}\")\n",
    "    \n",
    "    info = self._get_info()\n",
    "    info['tape_score'] = tape_score\n",
    "    return obs, reward, terminated, limit_hit, info\n",
    "\n",
    "def _get_info(self):\n",
    "    return {\n",
    "        'portfolio_value': self.portfolio_value,\n",
    "        'drawdown_lambda': getattr(self, 'drawdown_lambda', 0.0),\n",
    "        'tape_score': None\n",
    "    }\n",
    "\n",
    "PortfolioEnvTAPE._handle_termination = _handle_termination\n",
    "PortfolioEnvTAPE._get_info = _get_info\n",
    "PortfolioEnvTAPE.step = _patched_step_tape_master\n",
    "\n",
    "print(\"‚úÖ Environment Patched: Full TAPE v3 Logic applied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sec_3_config"
   },
   "source": [
    "## 3Ô∏è‚É£ Configuration & Variants\n",
    "\n",
    "Setting up TCN configuration with Experiment 7 overrides (Leaner actor, Faster learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_load"
   },
   "outputs": [],
   "source": [
    "# === LOAD & OVERRIDE CONFIG ===\n",
    "config = get_active_config('phase1')\n",
    "\n",
    "# Experiment 7 Overrides (Optimized)\n",
    "config[\"training_params\"][\"use_episode_length_curriculum\"] = True\n",
    "config[\"training_params\"][\"episode_length_curriculum_schedule\"] = [\n",
    "    {\"threshold\": 0,      \"limit\": 1500},\n",
    "    {\"threshold\": 30000,  \"limit\": 2000},\n",
    "    {\"threshold\": 60000,  \"limit\": 2500},\n",
    "    {\"threshold\": 90000,  \"limit\": None},\n",
    "]\n",
    "config[\"agent_params\"][\"ppo_params\"][\"actor_lr\"] = 0.0007\n",
    "config[\"agent_params\"][\"ppo_params\"][\"policy_clip\"] = 0.25\n",
    "config[\"agent_params\"][\"ppo_params\"][\"num_ppo_epochs\"] = 4\n",
    "config[\"environment_params\"][\"drawdown_constraint\"][\"lambda_max\"] = 3.0\n",
    "config[\"environment_params\"][\"drawdown_constraint\"][\"penalty_coef\"] = 2.0\n",
    "\n",
    "# Architecture\n",
    "ACTIVE_VARIANT = 'TCN' # Options: TCN, TCN_ATTENTION, TCN_FUSION\n",
    "config['agent_params']['actor_critic_type'] = ACTIVE_VARIANT\n",
    "config['agent_params']['use_attention'] = (ACTIVE_VARIANT == 'TCN_ATTENTION')\n",
    "config['agent_params']['use_fusion'] = (ACTIVE_VARIANT == 'TCN_FUSION')\n",
    "\n",
    "print(f\"‚úÖ Config Ready | Variant: {ACTIVE_VARIANT} | Params: Exp7 Overrides Applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sec_4_data"
   },
   "source": [
    "## 4Ô∏è‚É£ Data Pipeline\n",
    "\n",
    "Loading, Processing, and Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_load"
   },
   "outputs": [],
   "source": [
    "# === DATA PIPELINE ===\n",
    "# 1. Instantiate Processor (uses patched methods)\n",
    "processor = DataProcessor(config)\n",
    "raw_df = processor.load_ohlcv_data()\n",
    "\n",
    "# 2. Process\n",
    "df = processor.calculate_log_returns(raw_df)\n",
    "df = processor.calculate_return_statistics(df)\n",
    "df = processor.calculate_technical_indicators(df)\n",
    "df = processor.add_regime_features(df)\n",
    "df = processor.add_quant_alpha_features(df)\n",
    "df = processor.add_cross_sectional_features(df)\n",
    "df = processor.add_actuarial_features(df)\n",
    "\n",
    "# 3. Normalize & Split\n",
    "train_end = pd.Timestamp('2019-12-31')\n",
    "test_start = pd.Timestamp('2020-01-01')\n",
    "feature_cols = processor.get_feature_columns('phase1')\n",
    "\n",
    "norm_df, scalers = processor.normalize_features(\n",
    "    df, \n",
    "    feature_cols=feature_cols,\n",
    "    train_end_date=train_end,\n",
    "    test_start_date=test_start\n",
    ")\n",
    "\n",
    "# 4. Build Dataset\n",
    "phase1_data = Phase1Dataset(\n",
    "    master_df=norm_df,\n",
    "    train_df=norm_df[norm_df['Date'] <= train_end],\n",
    "    test_df=norm_df[norm_df['Date'] >= test_start],\n",
    "    scalers=scalers,\n",
    "    train_end_date=train_end,\n",
    "    test_start_date=test_start,\n",
    "    covariance_columns=identify_covariance_columns(norm_df.columns),\n",
    "    data_processor=processor\n",
    ")\n",
    "print(\"‚úÖ Data Pipeline Complete\")\n",
    "print(f\"   Train: {phase1_data.train_df.shape}\")\n",
    "print(f\"   Test : {phase1_data.test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sec_5_train"
   },
   "source": [
    "## 5Ô∏è‚É£ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_loop"
   },
   "outputs": [],
   "source": [
    "RUN_TRAINING = True\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    print(f\"üöÄ Starting Training: {ACTIVE_VARIANT}\")\n",
    "    experiment6 = run_experiment6_tape(\n",
    "        phase1_data=phase1_data,\n",
    "        config=config,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        csv_logger_cls=CSVLogger,\n",
    "        use_covariance=True,\n",
    "        architecture=config['agent_params']['actor_critic_type'],\n",
    "        timesteps_per_update=config['training_params']['timesteps_per_ppo_update'],\n",
    "        max_total_timesteps=config['training_params']['max_total_timesteps'],\n",
    "    )\n",
    "    print(\"‚úÖ Training Complete\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Training Skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sec_6_eval"
   },
   "source": [
    "## 6Ô∏è‚É£ Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval_loop"
   },
   "outputs": [],
   "source": [
    "RUN_EVAL = True\n",
    "\n",
    "if RUN_EVAL:\n",
    "    # Standard Evaluation (Latest Model)\n",
    "    stub = create_experiment6_result_stub(\n",
    "        random_seed=RANDOM_SEED,\n",
    "        use_covariance=True,\n",
    "        architecture=config['agent_params']['actor_critic_type'],\n",
    "        checkpoint_path=None, # Uses latest by default\n",
    "        base_agent_params=config.get('agent_params'),\n",
    "    )\n",
    "\n",
    "    eval_results = evaluate_experiment6_checkpoint(\n",
    "        stub,\n",
    "        phase1_data=phase1_data,\n",
    "        config=config,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        model_family='normal',\n",
    "        normal_model_strategy='latest',\n",
    "        num_eval_runs=30,\n",
    "        save_eval_logs=True,\n",
    "        save_eval_artifacts=True\n",
    "    )\n",
    "    print(\"‚úÖ Evaluation Complete\")\n",
    "    \n",
    "    # Optional: Plotting results\n",
    "    # (Add plotting code here if desired)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
