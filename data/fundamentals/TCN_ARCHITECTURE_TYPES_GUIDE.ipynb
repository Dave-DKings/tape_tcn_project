{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN Architecture Types Guide (Project-Aligned)\n",
    "\n",
    "This notebook explains the TCN architecture variants implemented in this project, how they process data, and why you would pick one over another. It is written to be both technical and readable for non-specialists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) What is implemented right now\n",
    "\n",
    "From the current codebase, the TCN family includes:\n",
    "\n",
    "- `TCN` (baseline temporal convolution model)\n",
    "- `TCN_ATTENTION` (TCN + self-attention on the time axis)\n",
    "- `TCN_FUSION` (hierarchical fusion: per-asset temporal encoding + cross-asset attention + global context + gating)\n",
    "\n",
    "There is also a compatibility path where `actor_critic_type='TCN'` can route to fusion or attention based on `use_fusion` or `use_attention` flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Shared input data pipeline (all TCN variants)\n",
    "\n",
    "All TCN variants consume the same prepared feature stream, then differ only in model internals.\n",
    "\n",
    "Core groups in the pipeline include:\n",
    "- Technical indicators and return transforms\n",
    "- Dynamic covariance signals\n",
    "- Fundamental features (daily-aligned from quarterly reports)\n",
    "- Regime and macro features\n",
    "- Actuarial features\n",
    "- Cross-sectional/quant alpha style features\n",
    "\n",
    "After feature construction, normalization is applied on the configured feature list. Sequence models then consume windows of shape:\n",
    "\n",
    "\\[ X \\in \\mathbb{R}^{B \\times T \\times F} \\]",
    "\n",
    "where:\n",
    "- \\(B\\): batch size\n",
    "- \\(T\\): sequence length (lookback window)\n",
    "- \\(F\\): total number of features per timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Common actor output: Dirichlet concentration parameters\n",
    "\n",
    "All TCN actors output logits, then transform logits into positive Dirichlet concentration values \\(\\alpha\\).\n",
    "\n",
    "General form:\n",
    "\\[ \\alpha = g(\\text{logits}/\\tau) + \\epsilon \\]",
    "\n",
    "where:\n",
    "- \\(g(\\cdot)\\) is the selected activation (`elu`, `softplus_shift`, `swish`, `mish`, `exp_clip`, or default softplus)\n",
    "- \\(\\tau\\) is optional logit temperature\n",
    "- \\(\\epsilon\\) is adaptive Dirichlet floor (annealed during training)\n",
    "\n",
    "This guarantees valid positive \\(\\alpha\\), which define the portfolio-weight distribution.\n",
    "\n",
    "Plain language: the network does not output weights directly. It outputs confidence parameters for each asset, and those parameters define how concentrated or diversified the sampled weights are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Baseline TCN (`TCN`)\n",
    "\n",
    "### Core idea\n",
    "Use stacked **dilated causal convolutions** with residual connections to learn temporal patterns efficiently.\n",
    "\n",
    "A simplified block is:\n",
    "\\[ h^{(l)} = \\sigma\\left(\\text{Conv}_{d_l}(\\text{Conv}_{d_l}(h^{(l-1)})) + R(h^{(l-1)})\\right) \\]",
    "\n",
    "- \\(d_l\\): dilation at block \\(l\\)\n",
    "- \\(R\\): residual projection when dimensions differ\n",
    "- \\(\\sigma\\): ReLU in this implementation\n",
    "\n",
    "Then a global average over time produces one vector for allocation output.\n",
    "\n",
    "### Receptive field intuition\n",
    "Approximate temporal reach grows with kernel size and dilations. With kernel \\(k\\) and dilation stack \\((d_1,\\dots,d_L)\\), a common approximation is:\n",
    "\\[ RF \\approx 1 + (k-1)\\sum_{l=1}^{L} d_l \\]",
    "\n",
    "### Pros\n",
    "- Fast and parallel compared with recurrent models\n",
    "- Stable training due to residual design\n",
    "- Strong baseline for medium-horizon pattern extraction\n",
    "\n",
    "### Cons\n",
    "- No explicit mechanism to reweight important timestamps globally\n",
    "- Can under-emphasize sparse but critical events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) TCN + Attention (`TCN_ATTENTION`)\n",
    "\n",
    "### Core idea\n",
    "Run TCN blocks first, then apply multi-head self-attention over time to let the model focus on key timesteps.\n",
    "\n",
    "Attention equations:\n",
    "\\[ Q = XW_Q,\\; K = XW_K,\\; V = XW_V \\]",
    "\\[ \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V \\]",
    "\n",
    "with multiple heads combined and projected.\n",
    "\n",
    "### What changes vs baseline\n",
    "- Baseline TCN asks: \"what local-to-midrange motifs exist?\"\n",
    "- TCN+Attention adds: \"which timesteps matter most for the decision?\"\n",
    "\n",
    "### Pros\n",
    "- Better at emphasizing eventful periods\n",
    "- More expressive than pure convolutional pooling\n",
    "\n",
    "### Cons\n",
    "- More compute and memory than baseline TCN\n",
    "- Can overfit faster if regularization is weak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) TCN Fusion (`TCN_FUSION`)\n",
    "\n",
    "### Core idea\n",
    "This is a structured fusion architecture that separates and recombines signals at two levels:\n",
    "\n",
    "1. **Per-asset temporal encoder** (shared TCN blocks)\n",
    "2. **Cross-asset attention** to model relationships between assets\n",
    "3. **Global context branch** from the whole state\n",
    "4. **Learnable gate** that mixes asset-context and global-context\n",
    "\n",
    "Gated fusion form:\n",
    "\\[ g = \\sigma(W_g[a \\| c]) \\]",
    "\\[ z = g \\odot a + (1-g) \\odot c \\]",
    "\n",
    "- \\(a\\): asset-context embedding\n",
    "- \\(c\\): global-context embedding\n",
    "- \\(z\\): fused representation sent to output head\n",
    "\n",
    "### Why this is different\n",
    "Baseline and attention variants mostly treat the sequence as one unified stream. Fusion explicitly builds an asset-level pathway and a portfolio-level pathway, then learns how much to trust each at each step.\n",
    "\n",
    "### Pros\n",
    "- Best structural fit when you want both asset-specific and market-wide context\n",
    "- Better interpretability of where signal comes from (asset branch vs global branch)\n",
    "\n",
    "### Cons\n",
    "- Highest implementation complexity\n",
    "- Most sensitive to shape/config mismatches (asset count, feature slicing)\n",
    "- Highest compute cost among TCN options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Quick comparison\n",
    "\n",
    "| Variant | Best Use Case | Complexity | Speed | Main Risk |\n",
    "|---|---|---|---|---|\n",
    "| `TCN` | Strong baseline, fast iteration | Low | Fastest | May miss global importance weighting |\n",
    "| `TCN_ATTENTION` | Event-aware temporal weighting | Medium | Medium | Higher overfitting risk |\n",
    "| `TCN_FUSION` | Joint asset-level + market-level reasoning | High | Slowest | Config/shape sensitivity |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Practical selection logic\n",
    "\n",
    "Use this progression in practice:\n",
    "\n",
    "1. Start with `TCN` to validate data pipeline, reward shaping, and stability.\n",
    "2. Move to `TCN_ATTENTION` when baseline converges but misses regime/event timing.\n",
    "3. Use `TCN_FUSION` when you need explicit asset-interaction modeling and have strong data hygiene + compute budget.\n",
    "\n",
    "For publication-quality comparisons, keep all non-architecture factors fixed (data window, reward profile, seed protocol, evaluation protocol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example config toggles for TCN family\n",
    "\n",
    "# Baseline TCN\n",
    "config[\"agent_params\"][\"actor_critic_type\"] = \"TCN\"\n",
    "config[\"agent_params\"][\"use_attention\"] = False\n",
    "config[\"agent_params\"][\"use_fusion\"] = False\n",
    "\n",
    "# TCN + Attention\n",
    "# config[\"agent_params\"][\"actor_critic_type\"] = \"TCN_ATTENTION\"\n",
    "# config[\"agent_params\"][\"use_attention\"] = True\n",
    "# config[\"agent_params\"][\"use_fusion\"] = False\n",
    "\n",
    "# TCN + Fusion\n",
    "# config[\"agent_params\"][\"actor_critic_type\"] = \"TCN_FUSION\"\n",
    "# config[\"agent_params\"][\"use_fusion\"] = True\n",
    "# config[\"agent_params\"][\"use_attention\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Notes for non-technical readers\n",
    "\n",
    "- Think of `TCN` as a fast scanner over recent history.\n",
    "- Think of `TCN_ATTENTION` as a scanner plus a highlighter that marks important time points.\n",
    "- Think of `TCN_FUSION` as two analysts (asset-level and global-level) whose opinions are blended by a learned referee.\n",
    "\n",
    "All three feed into the same portfolio-allocation logic via Dirichlet concentration outputs, so differences in behavior come from representation quality, not from changing the action distribution family."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
