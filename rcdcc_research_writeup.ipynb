{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RCDCC Research Notebook\n",
        "\n",
        "## Risk-Conditioned Dirichlet Concentration Control for PPO Portfolio RL\n",
        "\n",
        "This notebook provides a comprehensive and granular write-up of the Risk-Conditioned Dirichlet Concentration Control (RCDCC) framework, designed to stabilize PPO-based portfolio optimization agents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad04837f",
      "metadata": {},
      "source": [
        "## 1) Motivation and Problem\n",
        "\n",
        "### 1.1 The PPO-Dirichlet Challenge\n",
        "In current PPO-Dirichlet setups, we frequently observe periodic **KL spikes** and early-stop events. These indicate that the policy is making over-updatesâ€”abrupt shifts in allocation that exceed the trust-region boundaries ($target\\text{\\_}kl$).\n",
        "\n",
        "### 1.2 Coupling of Preference and Confidence\n",
        "The fundamental issue is the coupling of:\n",
        "1. **Allocation preference**: Which assets to favor (the direction of the vector).\n",
        "2. **Allocation confidence**: How concentrated or uncertain to be (the intensity of the vector).\n",
        "\n",
        "In a standard parameterization, trying to change preference often inadvertently increases concentration (confidence), leading to instability. This coupling makes it hard to manage update stability, turnover, and risk simultaneously in non-stationary regimes.\n",
        "\n",
        "### 1.3 Static vs. Dynamic Control\n",
        "While static controls (like `alpha_activation`, `alpha_cap`, and `epsilon`) provide a baseline level of stability, they do not adapt to market context. RCDCC addresses this by introducing closed-loop, adaptive concentration governance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b64b6f",
      "metadata": {},
      "source": [
        "## 2) Core Idea & Factorization\n",
        "\n",
        "RCDCC explicitly decouples **Direction** from **Confidence** using a two-part Dirichlet parameterization.\n",
        "\n",
        "### 2.1 The Parameterization\n",
        "Introduce two components:\n",
        "- A **simplex mean vector** $p_t$, representing the portfolio preference.\n",
        "- A **scalar concentration budget** $C_t$, representing the confidence level.\n",
        "\n",
        "The final Dirichlet parameters $\\alpha_{t,i}$ are constructed as:\n",
        "\n",
        "$$\\alpha_{t,i} = \\alpha_{\\text{floor}} + C_t \\cdot p_{t,i}, \\quad \\sum_i p_{t,i} = 1, \\quad p_{t,i} \\ge 0$$\n",
        "\n",
        "### 2.2 Mechanism of Action\n",
        "By modulating $C_t$, we effectively scale the variance of the sampled portfolio weights without altering their expected value. Under high market uncertainty or training instability, shrinking $C_t$ broadens the Dirichlet distribution (increasing exploration/uncertainty), which dampens the impact of abrupt policy shifts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a7b5b06",
      "metadata": {},
      "source": [
        "## 3) Network Parameterization\n",
        "\n",
        "The actor produces two distinct heads from its backbone (e.g., TCN, Attention, or Fusion layer):\n",
        "\n",
        "### 3.1 Preference Head\n",
        "$$p_t = \\text{softmax}(z_t / \\tau_t)$$\n",
        "- $z_t$: logits from actor backbone\n",
        "- $\\tau_t$: temperature (can be scheduled or fixed)\n",
        "\n",
        "### 3.2 Raw Concentration Head\n",
        "$$\\hat{C}_t = \\text{softplus}(u_t) + \\epsilon_C$$\n",
        "- $u_t$: scalar output\n",
        "- Ensures positivity of the raw budget.\n",
        "\n",
        "Then apply controller scaling:\n",
        "$$C_t = \\text{clip}(\\hat{C}_t \\cdot g_t,\\; C_{\\min},\\; C_{\\max})$$\n",
        "where $g_t$ is the adaptive gain driven by the closed-loop controller."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "170c97ac",
      "metadata": {},
      "source": [
        "## 4) Closed-Loop Controller\n",
        "\n",
        "We track Exponential Moving Averages (EMA) of update diagnostics:\n",
        "- $\\bar{k}_t$: Approximate KL divergence\n",
        "- $\\bar{to}_t$: Portfolio turnover\n",
        "- $\\bar{dd}_t$: Drawdown magnitude\n",
        "- $\\bar{s}_t$: KL early-stop incidence rate\n",
        "\n",
        "### 4.1 Error Terms\n",
        "$$e_k = \\frac{\\bar{k}_t}{k^*} - 1, \\quad e_{to} = \\frac{\\bar{to}_t}{to^*} - 1, \\quad e_{dd} = \\frac{\\bar{dd}_t}{dd^*} - 1$$\n",
        "\n",
        "### 4.2 Gain Update (Log-Space)\n",
        "$$\\log g_{t+1} = \\text{clip}(\\log g_t - \\eta_k e_k - \\eta_{to} e_{to} - \\eta_{dd} e_{dd} - \\eta_s \\bar{s}_t, \\; \\log g_{\\min}, \\log g_{\\max})$$\n",
        "$$g_{t+1} = \\exp(\\log g_{t+1})$$\n",
        "\n",
        "If diagnostics are poor (high KL, turnover, or drawdown), $g_t$ falls, shrinking concentration and increasing policy entropy. If healthy, $g_t$ recovers to allow stronger conviction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb86d51e",
      "metadata": {},
      "source": [
        "## 5) Regime-Aware Extension\n",
        "\n",
        "We can further refine the gain using a regime multiplier $\\rho_t$ based on market state:\n",
        "$$g_t \\leftarrow g_t \\cdot \\rho_t$$\n",
        "- **Calm Regime**: $\\rho_t \\in [1.00, 1.10]$ (allow higher conviction)\n",
        "- **Stress Regime**: $\\rho_t \\in [0.75, 0.95]$ (force conservatism / broader exploration)\n",
        "\n",
        "This makes the model's confidence adaptive to both its own optimization state and the external environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96f005bc",
      "metadata": {},
      "source": [
        "## 6) PPO Coupling and Synergy\n",
        "\n",
        "RCDCC is designed to work *with* PPO's existing trust-region mechanisms, not replace them:\n",
        "1. **Upstream Mitigation**: RCDCC reduces the frequency and severity of KL overshoots by modulating concentration *before* the action space is sampled.\n",
        "2. **Complementary Control**: PPO still maintains `target\\_kl`, `policy\\_clip`, and early-stop triggers.\n",
        "3. **Resulting Synergy**: Smoother learning curves, fewer update truncations, and lower churn."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f41706a5",
      "metadata": {},
      "source": [
        "## 7) Practical Integration (Your Codebase)\n",
        "\n",
        "### 7.1 `src/agents/actor_critic_tf.py`\n",
        "- Modify `DirichletActor` to add the dual heads.\n",
        "- Update `_compute_alpha` to use the factorization logic.\n",
        "\n",
        "### 7.2 `src/agents/ppo_agent_tf.py`\n",
        "- Implement the EMA trackers for diagnostics.\n",
        "- Implement the $g_t$ controller update loop within the `update` method.\n",
        "\n",
        "### 7.3 `src/config.py`\n",
        "- Add `dirichlet_controller_params` to the agent configuration (see suggested schema below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.4 Suggested config schema (copy into config override cell)\n",
        "rcdcc_params = {\n",
        "    'enabled': True,\n",
        "    'alpha_floor': 1e-3,\n",
        "    'epsilon_c': 1e-6,\n",
        "    'concentration_min': 2.0,\n",
        "    'concentration_max': 60.0,\n",
        "    'gain_init': 1.0,\n",
        "    'gain_min': 0.4,\n",
        "    'gain_max': 2.0,\n",
        "    'ema_beta': 0.90,\n",
        "    'targets': {\n",
        "        'kl': 0.020,\n",
        "        'turnover': 0.35,\n",
        "        'drawdown': 0.18,\n",
        "    },\n",
        "    'controller_eta': {\n",
        "        'kl': 0.40,\n",
        "        'turnover': 0.20,\n",
        "        'drawdown': 0.15,\n",
        "        'early_stop': 0.10,\n",
        "    },\n",
        "    'regime_gate': {\n",
        "        'enabled': True,\n",
        "        'stress_gain_mult': 0.85,\n",
        "        'calm_gain_mult': 1.00,\n",
        "    },\n",
        "    'temperature': {\n",
        "        'base': 1.20,\n",
        "        'min': 0.90,\n",
        "        'max': 1.60,\n",
        "    },\n",
        "}\n",
        "\n",
        "print('RCDCC template ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Expected Behavioral Effects\n",
        "\n",
        "- **Lower KL Overshoot Rate**: Fewer abrupt policy shifts from overconfident alpha spikes.\n",
        "- **Lower Turnover Volatility**: Allocations become smoother, especially in high-stress regimes.\n",
        "- **Improved Train Stability**: Fewer frequent KL early-stop interruptions, leading to more efficient use of data.\n",
        "- **Better Robustness**: Out-of-sample (OOS) gains from adaptive confidence rather than static heuristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Ablation Plan\n",
        "\n",
        "1. **Baseline**: Static Dirichlet (`elu/cap/epsilon` only).\n",
        "2. **Factorized-only**: Mean-concentration factorization only (no controller).\n",
        "3. **Factorized + KL Controller**: Partial controller using only KL error.\n",
        "4. **Factorized + KL+Turnover Controller**: Multi-objective control.\n",
        "5. **Full RCDCC**: KL + Turnover + Drawdown + Regime-Aware multipliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment registry for ablations\n",
        "from pathlib import Path\n",
        "\n",
        "EXPERIMENTS = [\n",
        "    {\n",
        "        'label': 'baseline_static_dirichlet',\n",
        "        'method': 'Static Dirichlet + static PPO-KL',\n",
        "        'group': 'baseline',\n",
        "        'logs_dir': Path('./results/logs'),  # Update for actual local results\n",
        "        'run_tag': None,\n",
        "    },\n",
        "    {\n",
        "        'label': 'rcdcc_full',\n",
        "        'method': 'RCDCC full',\n",
        "        'group': 'proposed',\n",
        "        'logs_dir': Path('./results/logs'),\n",
        "        'run_tag': None,\n",
        "    },\n",
        "]\n",
        "\n",
        "print('Configured runs:', len(EXPERIMENTS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Novelty and Contribution\n",
        "\n",
        "- **Adaptive Concentration Governance**: Most portfolio RL uses fixed or scheduled Dirichlet controls. RCDCC introduces a **closed-loop feedback system**.\n",
        "- **Stateful Confidence**: Explicitly treats confidence as a stateful, risk-aware process driven by optimization diagnostics.\n",
        "- **Explicit Decoupling**: Provides a cleaner mathematical separation between policy intent (mean) and policy conviction (concentration)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Risks and Mitigations\n",
        "\n",
        "- **Controller Oscillation**: Aggressive $\\eta$ values could cause $g_t$ to oscillate. \n",
        "  - *Mitigation*: use log-space updates and EMA smoothing.\n",
        "- **Policy Flattening**: The controller might over-flatten the policy to satisfy constraints.\n",
        "  - *Mitigation*: set reasonable $g_{\\min}$ and $C_{\\min}$ floors.\n",
        "- **Delayed Feedback**: Reward/drawdown signals are delayed relative to the gradient update.\n",
        "  - *Mitigation*: use EMA to provide a stable, long-horizon view of risk diagnostics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load helper\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "\n",
        "def pick_latest(logs_dir: Path, pattern: str, run_tag=None):\n",
        "    files = sorted(logs_dir.glob(pattern), key=lambda p: p.stat().st_mtime)\n",
        "    if run_tag:\n",
        "        tagged = [p for p in files if run_tag in p.name]\n",
        "        if tagged:\n",
        "            return tagged[-1]\n",
        "    return files[-1] if files else None\n",
        "\n",
        "\n",
        "def load_artifacts(logs_dir: Path, run_tag=None):\n",
        "    ep_p = pick_latest(logs_dir, '*episodes*.csv', run_tag)\n",
        "    st_p = pick_latest(logs_dir, '*step_diagnostics*.csv', run_tag)\n",
        "    sm_p = pick_latest(logs_dir, '*summary*.csv', run_tag)\n",
        "    md_p = pick_latest(logs_dir, '*_metadata.json', run_tag)\n",
        "\n",
        "    return {\n",
        "        'episodes_path': ep_p,\n",
        "        'steps_path': st_p,\n",
        "        'summary_path': sm_p,\n",
        "        'metadata_path': md_p,\n",
        "        'episodes': pd.read_csv(ep_p) if ep_p else None,\n",
        "        'steps': pd.read_csv(st_p) if st_p else None,\n",
        "        'summary': pd.read_csv(sm_p) if sm_p else None,\n",
        "        'metadata': json.loads(md_p.read_text(encoding='utf-8')) if md_p else None,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def col(df, names):\n",
        "    for n in names:\n",
        "        if df is not None and n in df.columns:\n",
        "            return n\n",
        "    return None\n",
        "\n",
        "\n",
        "def summarize_run(art):\n",
        "    out = {}\n",
        "    ep = art['episodes']\n",
        "    st = art['steps']\n",
        "\n",
        "    if st is not None and len(st) > 0:\n",
        "        kl = col(st, ['approx_kl', 'kl'])\n",
        "        es = col(st, ['early_stop_kl_triggered', 'early_stop'])\n",
        "        cf = col(st, ['clip_fraction'])\n",
        "        to = col(st, ['turnover'])\n",
        "\n",
        "        if kl:\n",
        "            out['kl_mean'] = float(st[kl].mean())\n",
        "            out['kl_p95'] = float(st[kl].quantile(0.95))\n",
        "        if es:\n",
        "            out['early_stop_rate'] = float((st[es] > 0).mean())\n",
        "        if cf:\n",
        "            out['clip_fraction_mean'] = float(st[cf].mean())\n",
        "        if to:\n",
        "            out['turnover_mean'] = float(st[to].mean())\n",
        "\n",
        "    if ep is not None and len(ep) > 0:\n",
        "        shp = col(ep, ['Sharpe', 'sharpe', 'sharpe_ratio'])\n",
        "        ret = col(ep, ['Return', 'total_return'])\n",
        "        mdd = col(ep, ['Max_Drawdown', 'max_drawdown'])\n",
        "\n",
        "        if shp:\n",
        "            out['episode_sharpe_mean'] = float(ep[shp].mean())\n",
        "            out['episode_sharpe_p75'] = float(ep[shp].quantile(0.75))\n",
        "        if ret:\n",
        "            out['episode_return_mean'] = float(ep[ret].mean())\n",
        "        if mdd:\n",
        "            out['episode_mdd_mean'] = float(ep[mdd].mean())\n",
        "\n",
        "    return out"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
