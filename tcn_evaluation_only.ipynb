{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TCN Final Evaluation Notebook\n",
        "\n",
        "This notebook is evaluation-only and designed for final model selection, ablations, and benchmarking.\n",
        "All runtime variables are isolated with the `eval_` prefix to avoid conflicts with training notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Connect to Colab VM and sync repository\n",
        "Run this first in a fresh Colab runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "EVAL_REPO_URL = \"https://github.com/Dave-DKings/tape_tcn_project.git\"\n",
        "EVAL_REPO_DIR = \"/content/adaptive_portfolio_rl\"\n",
        "\n",
        "if not os.path.exists(f\"{EVAL_REPO_DIR}/.git\"):\n",
        "    !git clone {EVAL_REPO_URL} {EVAL_REPO_DIR}\n",
        "\n",
        "%cd /content/adaptive_portfolio_rl\n",
        "!git fetch origin\n",
        "!git reset --hard origin/main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Optional: mount Drive and restore saved results zip\n",
        "Set `EVAL_RESTORE_FROM_ZIP=True` only when needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "EVAL_RESTORE_FROM_ZIP = False\n",
        "EVAL_ZIP_PATH = \"/content/drive/MyDrive/tcn_fusion_results_download_new2.zip\"\n",
        "\n",
        "if EVAL_RESTORE_FROM_ZIP:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    zip_path = Path(EVAL_ZIP_PATH)\n",
        "    if not zip_path.exists():\n",
        "        raise FileNotFoundError(f\"Zip not found: {zip_path}\")\n",
        "\n",
        "    !mkdir -p /content/adaptive_portfolio_rl\n",
        "    !unzip -q -o {zip_path} -d /content/adaptive_portfolio_rl\n",
        "    print(\"‚úÖ Restored results from zip\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è EVAL_RESTORE_FROM_ZIP=False\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import json\n",
        "import re\n",
        "from dataclasses import replace\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from src.config import get_active_config\n",
        "from src.notebook_helpers.tcn_phase1 import (\n",
        "    prepare_phase1_dataset,\n",
        "    create_experiment6_result_stub,\n",
        "    evaluate_experiment6_checkpoint,\n",
        "    load_training_metadata_into_config,\n",
        "    build_evaluation_track_summary,\n",
        "    build_ablation_table,\n",
        "    compare_agent_vs_baseline,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Evaluation run settings\n",
        "Adjust once here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EVAL_RANDOM_SEED = 42\n",
        "EVAL_RESULTS_ROOT = Path('/content/adaptive_portfolio_rl/tcn_fusion_results')\n",
        "\n",
        "# Deterministic policy mode: 'mean' is recommended for stable ranking.\n",
        "EVAL_DETERMINISTIC_MODE = 'mean'\n",
        "\n",
        "# Stochastic robustness checks per checkpoint.\n",
        "EVAL_NUM_STOCHASTIC_RUNS = 10\n",
        "EVAL_STOCHASTIC_EPISODE_LIMIT = 252\n",
        "\n",
        "# Selection for ablation basket\n",
        "EVAL_TOP_HW = 8         # high-watermark checkpoints by filename Sharpe tag\n",
        "EVAL_TOP_PERIODIC = 4   # periodic step checkpoints by most recent step\n",
        "EVAL_INCLUDE_ROOT = True\n",
        "EVAL_INCLUDE_RARE = False\n",
        "\n",
        "# Save outputs\n",
        "EVAL_SAVE_LOGS = True\n",
        "EVAL_SAVE_ARTIFACTS = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Build evaluation dataset and load latest metadata config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not EVAL_RESULTS_ROOT.exists():\n",
        "    raise FileNotFoundError(f\"Missing results root: {EVAL_RESULTS_ROOT}\")\n",
        "\n",
        "\n",
        "def eval_build_core_active_feature_columns(cfg):\n",
        "    from src.data_utils import DataProcessor\n",
        "\n",
        "    probe = DataProcessor(cfg)\n",
        "    return list(dict.fromkeys(probe.get_feature_columns(\"phase1\")))\n",
        "\n",
        "\n",
        "def eval_apply_core_feature_lock(cfg, active_feature_columns):\n",
        "    from src.data_utils import DataProcessor\n",
        "\n",
        "    probe_cfg = copy.deepcopy(cfg)\n",
        "    probe_fp = probe_cfg.setdefault(\"feature_params\", {})\n",
        "    probe_fs = probe_fp.setdefault(\"feature_selection\", {})\n",
        "    probe_fs[\"disable_features\"] = False\n",
        "    probe_fs[\"disabled_features\"] = []\n",
        "\n",
        "    probe = DataProcessor(probe_cfg)\n",
        "    core_all_cols = list(dict.fromkeys(probe.get_feature_columns(\"phase1\")))\n",
        "\n",
        "    active_set = set(active_feature_columns)\n",
        "    disabled = sorted([c for c in core_all_cols if c not in active_set])\n",
        "\n",
        "    fp = cfg.setdefault(\"feature_params\", {})\n",
        "    fs = fp.setdefault(\"feature_selection\", {})\n",
        "    fs[\"disable_features\"] = True\n",
        "    fs[\"disabled_features\"] = disabled\n",
        "\n",
        "    return core_all_cols, disabled\n",
        "\n",
        "\n",
        "eval_config = copy.deepcopy(get_active_config('phase1'))\n",
        "\n",
        "eval_logs_dir = EVAL_RESULTS_ROOT / 'logs'\n",
        "meta_files = sorted(eval_logs_dir.glob('*_metadata.json'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "if not meta_files:\n",
        "    raise FileNotFoundError(f\"No metadata JSON in {eval_logs_dir}\")\n",
        "\n",
        "EVAL_METADATA_PATH = meta_files[0]\n",
        "print('üìÑ Using metadata:', EVAL_METADATA_PATH)\n",
        "\n",
        "# Apply run-time training settings (architecture/reward/etc.) from metadata.\n",
        "eval_config = load_training_metadata_into_config(\n",
        "    EVAL_METADATA_PATH,\n",
        "    copy.deepcopy(eval_config),\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Enforce architecture family used by checkpoints\n",
        "eval_config['agent_params']['actor_critic_type'] = 'TCN_FUSION'\n",
        "eval_config['agent_params']['use_fusion'] = True\n",
        "eval_config['agent_params']['use_attention'] = False\n",
        "\n",
        "# IMPORTANT: feature list is derived from core project pipeline, not manifest.\n",
        "eval_active_feature_columns = eval_build_core_active_feature_columns(eval_config)\n",
        "_, eval_disabled_features = eval_apply_core_feature_lock(eval_config, eval_active_feature_columns)\n",
        "\n",
        "print('‚úÖ Eval core feature lock applied')\n",
        "print('   active_feature_columns:', len(eval_active_feature_columns))\n",
        "print('   disabled_features:', len(eval_disabled_features))\n",
        "\n",
        "if 'eval_phase1_data' not in globals():\n",
        "    eval_phase1_data = prepare_phase1_dataset(eval_config, force_download=False)\n",
        "else:\n",
        "    print('‚ÑπÔ∏è Reusing eval_phase1_data from current runtime')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Inspect latest training CSV logs (for diagnostics context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_latest_csv(pattern):\n",
        "    files = sorted(eval_logs_dir.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    return files[0] if files else None\n",
        "\n",
        "eval_latest_episodes_csv = eval_latest_csv('*episodes*.csv')\n",
        "eval_latest_step_diag_csv = eval_latest_csv('*step_diagnostics*.csv')\n",
        "eval_latest_summary_csv = eval_latest_csv('*summary*.csv')\n",
        "\n",
        "print('episodes:', eval_latest_episodes_csv)\n",
        "print('step diagnostics:', eval_latest_step_diag_csv)\n",
        "print('summary:', eval_latest_summary_csv)\n",
        "\n",
        "if eval_latest_episodes_csv:\n",
        "    eval_episodes_df = pd.read_csv(eval_latest_episodes_csv)\n",
        "    display(eval_episodes_df.tail(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Checkpoint discovery and ablation basket construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_parse_sharpe_from_name(path: Path):\n",
        "    m = re.search(r'_sh([pm])(\\d+)p(\\d+)', path.name)\n",
        "    if not m:\n",
        "        return None\n",
        "    sign = 1.0 if m.group(1) == 'p' else -1.0\n",
        "    return sign * float(f\"{m.group(2)}.{m.group(3)}\")\n",
        "\n",
        "\n",
        "def eval_parse_episode(path: Path):\n",
        "    m = re.search(r'_ep(\\d+)', path.name)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "\n",
        "def eval_parse_step(path: Path):\n",
        "    m = re.search(r'_step(\\d+)', path.name)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "\n",
        "def eval_discover_actor_files(results_root: Path):\n",
        "    actors = sorted(results_root.rglob('*_actor.weights.h5'))\n",
        "    rows = []\n",
        "    for actor in actors:\n",
        "        prefix = str(actor).replace('_actor.weights.h5', '')\n",
        "        critic = Path(prefix + '_critic.weights.h5')\n",
        "        if not critic.exists():\n",
        "            continue\n",
        "\n",
        "        parent = actor.parent.name\n",
        "        if parent == 'high_watermark_checkpoints':\n",
        "            kind = 'high_watermark'\n",
        "        elif parent == 'step_sharpe_checkpoints':\n",
        "            kind = 'step_sharpe'\n",
        "        elif parent == 'rare_models':\n",
        "            kind = 'rare'\n",
        "        elif eval_parse_step(actor.name if isinstance(actor, str) else actor):\n",
        "            kind = 'periodic_step'\n",
        "        else:\n",
        "            kind = 'root'\n",
        "\n",
        "        rows.append({\n",
        "            'actor_path': str(actor),\n",
        "            'critic_path': str(critic),\n",
        "            'checkpoint_prefix': prefix,\n",
        "            'checkpoint_kind': kind,\n",
        "            'episode': eval_parse_episode(actor),\n",
        "            'step': eval_parse_step(actor),\n",
        "            'sharpe_tag': eval_parse_sharpe_from_name(actor),\n",
        "            'mtime': actor.stat().st_mtime,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def eval_select_ablation_basket(df_ckpt: pd.DataFrame):\n",
        "    picks = []\n",
        "\n",
        "    if EVAL_INCLUDE_ROOT:\n",
        "        root_df = df_ckpt[df_ckpt['checkpoint_kind'] == 'root'].copy()\n",
        "        if not root_df.empty:\n",
        "            picks.append(root_df.sort_values('mtime', ascending=False).head(2))\n",
        "\n",
        "    hw_df = df_ckpt[df_ckpt['checkpoint_kind'] == 'high_watermark'].copy()\n",
        "    if not hw_df.empty:\n",
        "        hw_df['sharpe_rank_key'] = hw_df['sharpe_tag'].fillna(-np.inf)\n",
        "        picks.append(hw_df.sort_values(['sharpe_rank_key', 'episode', 'mtime'], ascending=[False, False, False]).head(EVAL_TOP_HW))\n",
        "\n",
        "    periodic_df = df_ckpt[df_ckpt['checkpoint_kind'] == 'periodic_step'].copy()\n",
        "    if not periodic_df.empty:\n",
        "        picks.append(periodic_df.sort_values(['step', 'mtime'], ascending=[False, False]).head(EVAL_TOP_PERIODIC))\n",
        "\n",
        "    if EVAL_INCLUDE_RARE:\n",
        "        rare_df = df_ckpt[df_ckpt['checkpoint_kind'] == 'rare'].copy()\n",
        "        if not rare_df.empty:\n",
        "            rare_df['sharpe_rank_key'] = rare_df['sharpe_tag'].fillna(-np.inf)\n",
        "            picks.append(rare_df.sort_values(['sharpe_rank_key', 'episode', 'mtime'], ascending=[False, False, False]).head(3))\n",
        "\n",
        "    if not picks:\n",
        "        return pd.DataFrame(columns=df_ckpt.columns)\n",
        "\n",
        "    out = pd.concat(picks, ignore_index=True)\n",
        "    out = out.drop_duplicates(subset=['checkpoint_prefix']).reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "eval_ckpt_df = eval_discover_actor_files(EVAL_RESULTS_ROOT)\n",
        "if eval_ckpt_df.empty:\n",
        "    raise RuntimeError(f'No valid actor+critic checkpoint pairs found under {EVAL_RESULTS_ROOT}')\n",
        "\n",
        "print('All checkpoint pairs:', len(eval_ckpt_df))\n",
        "print(eval_ckpt_df['checkpoint_kind'].value_counts().to_dict())\n",
        "\n",
        "eval_ablation_ckpts = eval_select_ablation_basket(eval_ckpt_df)\n",
        "print('Selected for ablation:', len(eval_ablation_ckpts))\n",
        "display(eval_ablation_ckpts[['checkpoint_kind', 'episode', 'step', 'sharpe_tag', 'actor_path']].head(30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Evaluate ablation basket (deterministic + stochastic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_run_one_checkpoint(eval_cfg, phase1_data, ckpt_prefix, seed=42):\n",
        "    stub = create_experiment6_result_stub(\n",
        "        random_seed=seed,\n",
        "        use_covariance=True,\n",
        "        architecture=eval_cfg['agent_params']['actor_critic_type'],\n",
        "        checkpoint_path=ckpt_prefix,\n",
        "        base_agent_params=eval_cfg.get('agent_params'),\n",
        "    )\n",
        "\n",
        "    return evaluate_experiment6_checkpoint(\n",
        "        experiment6=stub,\n",
        "        phase1_data=phase1_data,\n",
        "        config=eval_cfg,\n",
        "        random_seed=seed,\n",
        "        checkpoint_path_override=ckpt_prefix,\n",
        "        deterministic_eval_mode=EVAL_DETERMINISTIC_MODE,\n",
        "        num_eval_runs=EVAL_NUM_STOCHASTIC_RUNS,\n",
        "        stochastic_episode_length_limit=EVAL_STOCHASTIC_EPISODE_LIMIT,\n",
        "        save_eval_logs=EVAL_SAVE_LOGS,\n",
        "        save_eval_artifacts=EVAL_SAVE_ARTIFACTS,\n",
        "    )\n",
        "\n",
        "\n",
        "eval_evaluations = {}\n",
        "\n",
        "for i, row in eval_ablation_ckpts.iterrows():\n",
        "    label = f\"{row['checkpoint_kind']}__ep{row['episode']}__step{row['step']}\"\n",
        "    prefix = row['checkpoint_prefix']\n",
        "    print(f\"\n",
        "[{i+1}/{len(eval_ablation_ckpts)}] Evaluating: {label}\")\n",
        "    try:\n",
        "        ev = eval_run_one_checkpoint(eval_config, eval_phase1_data, prefix, seed=EVAL_RANDOM_SEED)\n",
        "        eval_evaluations[label] = ev\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed {label}: {type(e).__name__}: {e}\")\n",
        "\n",
        "print('‚úÖ Completed evaluations:', len(eval_evaluations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Ablation table and leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not eval_evaluations:\n",
        "    raise RuntimeError('No successful evaluations to summarize.')\n",
        "\n",
        "eval_ablation_table = build_ablation_table(eval_evaluations)\n",
        "\n",
        "display(eval_ablation_table.head(30))\n",
        "\n",
        "# Deterministic-first leaderboard view\n",
        "eval_leaderboard = eval_ablation_table.copy()\n",
        "eval_leaderboard['risk_adjusted_score'] = (\n",
        "    eval_leaderboard['det_sharpe'].fillna(-999)\n",
        "    - 0.5 * eval_leaderboard['det_max_drawdown'].fillna(1.0)\n",
        "    - 0.1 * eval_leaderboard['det_turnover'].fillna(1.0)\n",
        ")\n",
        "eval_leaderboard = eval_leaderboard.sort_values(['risk_adjusted_score', 'det_sharpe'], ascending=False).reset_index(drop=True)\n",
        "\n",
        "print('Top by risk-adjusted score:')\n",
        "display(eval_leaderboard.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Build industry baseline returns (equal-weight and cash)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_identify_asset_column(df: pd.DataFrame):\n",
        "    candidates = ['Ticker', 'ticker', 'tic', 'asset', 'Asset', 'symbol', 'Symbol']\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "\n",
        "def eval_identify_return_column(df: pd.DataFrame):\n",
        "    candidates = ['LogReturn_1d', 'log_return_1d', 'Return_1d', 'return_1d', 'daily_return']\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "\n",
        "def eval_fetch_sp500_returns(start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Fetch S&P500 daily simple returns for benchmark comparison.\n",
        "    Primary source: yfinance '^GSPC'.\n",
        "    Fallback: empty series if fetch fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import yfinance as yf\n",
        "    except Exception:\n",
        "        try:\n",
        "            !pip -q install yfinance\n",
        "            import yfinance as yf\n",
        "        except Exception:\n",
        "            print('‚ö†Ô∏è Could not install/import yfinance; SP500 benchmark disabled.')\n",
        "            return pd.Series(dtype=float)\n",
        "\n",
        "    try:\n",
        "        df = yf.download('^GSPC', start=str(start_date.date()), end=str((end_date + pd.Timedelta(days=1)).date()), auto_adjust=True, progress=False)\n",
        "        if df is None or df.empty or 'Close' not in df.columns:\n",
        "            print('‚ö†Ô∏è SP500 download returned empty data.')\n",
        "            return pd.Series(dtype=float)\n",
        "        close = pd.Series(df['Close']).dropna()\n",
        "        ret = close.pct_change().dropna().astype(float)\n",
        "        ret.index = pd.to_datetime(ret.index)\n",
        "        return ret\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è SP500 fetch failed: {type(e).__name__}: {e}')\n",
        "        return pd.Series(dtype=float)\n",
        "\n",
        "\n",
        "def eval_build_baselines_from_phase1(phase1_data):\n",
        "    test_df = phase1_data.test_df.copy()\n",
        "    if 'Date' not in test_df.columns:\n",
        "        raise ValueError('test_df must contain Date column')\n",
        "\n",
        "    ret_col = eval_identify_return_column(test_df)\n",
        "    if ret_col is None:\n",
        "        raise ValueError('Could not identify return column in test_df')\n",
        "\n",
        "    if 'LogReturn' in ret_col or 'log' in ret_col.lower():\n",
        "        test_df['_simple_ret'] = np.expm1(test_df[ret_col].astype(float))\n",
        "    else:\n",
        "        test_df['_simple_ret'] = test_df[ret_col].astype(float)\n",
        "\n",
        "    # Industry baseline 1: equal-weight over available assets each day\n",
        "    eqw = (\n",
        "        test_df.groupby('Date')['_simple_ret']\n",
        "        .mean()\n",
        "        .sort_index()\n",
        "        .astype(float)\n",
        "    )\n",
        "\n",
        "    # Industry baseline 2: cash (0% daily return)\n",
        "    cash = pd.Series(np.zeros(len(eqw)), index=pd.to_datetime(eqw.index), name='cash')\n",
        "\n",
        "    # Industry baseline 3: S&P 500 (^GSPC)\n",
        "    dt_index = pd.to_datetime(eqw.index)\n",
        "    sp500_ret = eval_fetch_sp500_returns(dt_index.min(), dt_index.max())\n",
        "    if not sp500_ret.empty:\n",
        "        # align to model dates; missing market holidays become 0 return for alignment stability\n",
        "        sp500_ret = sp500_ret.reindex(dt_index).fillna(0.0)\n",
        "    else:\n",
        "        sp500_ret = pd.Series(dtype=float)\n",
        "\n",
        "    # reset to plain 0..n index for compare_agent_vs_baseline\n",
        "    eqw = eqw.reset_index(drop=True)\n",
        "    cash = cash.reset_index(drop=True)\n",
        "    sp500 = sp500_ret.reset_index(drop=True) if not sp500_ret.empty else pd.Series(dtype=float)\n",
        "    return eqw, cash, sp500\n",
        "\n",
        "\n",
        "eval_baseline_eqw, eval_baseline_cash, eval_baseline_sp500 = eval_build_baselines_from_phase1(eval_phase1_data)\n",
        "print('Baseline lengths | EQW:', len(eval_baseline_eqw), 'Cash:', len(eval_baseline_cash), 'SP500:', len(eval_baseline_sp500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Benchmark each evaluated checkpoint vs baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark_rows = []\n",
        "for label, ev in eval_evaluations.items():\n",
        "    try:\n",
        "        cmp_eqw = compare_agent_vs_baseline(ev, eval_baseline_eqw)\n",
        "    except Exception as e:\n",
        "        cmp_eqw = {'error': str(e)}\n",
        "\n",
        "    try:\n",
        "        cmp_cash = compare_agent_vs_baseline(ev, eval_baseline_cash)\n",
        "    except Exception as e:\n",
        "        cmp_cash = {'error': str(e)}\n",
        "\n",
        "    try:\n",
        "        if len(eval_baseline_sp500) > 0:\n",
        "            cmp_sp500 = compare_agent_vs_baseline(ev, eval_baseline_sp500)\n",
        "        else:\n",
        "            cmp_sp500 = {'error': 'SP500 baseline unavailable'}\n",
        "    except Exception as e:\n",
        "        cmp_sp500 = {'error': str(e)}\n",
        "\n",
        "    row = {\n",
        "        'label': label,\n",
        "        'det_sharpe': (ev.deterministic_metrics or {}).get('sharpe_ratio', np.nan),\n",
        "        'det_return': (ev.deterministic_metrics or {}).get('annualized_return', np.nan),\n",
        "        'det_mdd': (ev.deterministic_metrics or {}).get('max_drawdown_abs', np.nan),\n",
        "        'det_turnover': (ev.deterministic_metrics or {}).get('turnover', np.nan),\n",
        "    }\n",
        "\n",
        "    for prefix, comp in [('eqw', cmp_eqw), ('cash', cmp_cash), ('sp500', cmp_sp500)]:\n",
        "        if isinstance(comp, dict) and 'error' not in comp:\n",
        "            for k, v in comp.items():\n",
        "                row[f'{prefix}_{k}'] = v\n",
        "        else:\n",
        "            row[f'{prefix}_error'] = comp.get('error', 'unknown') if isinstance(comp, dict) else 'unknown'\n",
        "\n",
        "    benchmark_rows.append(row)\n",
        "\n",
        "eval_benchmark_df = pd.DataFrame(benchmark_rows)\n",
        "\n",
        "display(eval_benchmark_df.sort_values('det_sharpe', ascending=False).head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Champion selection (production candidate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if eval_benchmark_df.empty:\n",
        "    raise RuntimeError('No benchmark rows available.')\n",
        "\n",
        "# Balanced production-style objective: reward risk-adjusted return, penalize drawdown/turnover.\n",
        "eval_benchmark_df['selection_score'] = (\n",
        "    eval_benchmark_df['det_sharpe'].fillna(-999)\n",
        "    + 0.2 * eval_benchmark_df['det_return'].fillna(0.0)\n",
        "    - 0.7 * eval_benchmark_df['det_mdd'].fillna(1.0)\n",
        "    - 0.1 * eval_benchmark_df['det_turnover'].fillna(1.0)\n",
        ")\n",
        "\n",
        "champion_row = eval_benchmark_df.sort_values('selection_score', ascending=False).iloc[0]\n",
        "EVAL_CHAMPION_LABEL = champion_row['label']\n",
        "EVAL_CHAMPION = eval_evaluations[EVAL_CHAMPION_LABEL]\n",
        "\n",
        "print('üèÜ Champion label:', EVAL_CHAMPION_LABEL)\n",
        "print(champion_row[['det_sharpe', 'det_return', 'det_mdd', 'det_turnover', 'selection_score']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Regime-sliced performance (champion vs equal-weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_regime_tag(dates: pd.Series):\n",
        "    d = pd.to_datetime(dates)\n",
        "    conds = [\n",
        "        (d <= pd.Timestamp('2020-02-19')),\n",
        "        (d >= pd.Timestamp('2020-02-20')) & (d <= pd.Timestamp('2020-06-30')),\n",
        "        (d >= pd.Timestamp('2020-07-01')) & (d <= pd.Timestamp('2021-12-31')),\n",
        "        (d >= pd.Timestamp('2022-01-01')) & (d <= pd.Timestamp('2023-12-31')),\n",
        "        (d >= pd.Timestamp('2024-01-01')),\n",
        "    ]\n",
        "    labels = ['pre_covid', 'covid_crash', 'post_covid_recovery', 'inflation_rates', 'recent']\n",
        "    out = np.select(conds, labels, default='other')\n",
        "    return pd.Series(out)\n",
        "\n",
        "\n",
        "def eval_sharpe(x):\n",
        "    x = pd.Series(x).dropna()\n",
        "    if len(x) < 2:\n",
        "        return np.nan\n",
        "    std = x.std(ddof=1)\n",
        "    if std <= 1e-12:\n",
        "        return np.nan\n",
        "    return np.sqrt(252.0) * x.mean() / std\n",
        "\n",
        "# Build aligned daily return series for champion and baselines\n",
        "champ_port = np.array(EVAL_CHAMPION.deterministic_portfolio)\n",
        "champ_ret = pd.Series(np.diff(champ_port) / champ_port[:-1]).reset_index(drop=True)\n",
        "eqw_ret = eval_baseline_eqw.reset_index(drop=True)\n",
        "sp500_ret = eval_baseline_sp500.reset_index(drop=True) if len(eval_baseline_sp500) > 0 else pd.Series(dtype=float)\n",
        "\n",
        "n_core = min(len(champ_ret), len(eqw_ret), len(eval_phase1_data.test_df['Date'].drop_duplicates()) - 1)\n",
        "if len(sp500_ret) > 0:\n",
        "    n = min(n_core, len(sp500_ret))\n",
        "else:\n",
        "    n = n_core\n",
        "\n",
        "dates = pd.to_datetime(eval_phase1_data.test_df['Date'].drop_duplicates().sort_values()).reset_index(drop=True).iloc[1:n+1]\n",
        "\n",
        "reg_df = pd.DataFrame({\n",
        "    'Date': dates.reset_index(drop=True),\n",
        "    'champion_ret': champ_ret.iloc[:n].reset_index(drop=True),\n",
        "    'eqw_ret': eqw_ret.iloc[:n].reset_index(drop=True),\n",
        "})\n",
        "if len(sp500_ret) > 0:\n",
        "    reg_df['sp500_ret'] = sp500_ret.iloc[:n].reset_index(drop=True)\n",
        "else:\n",
        "    reg_df['sp500_ret'] = np.nan\n",
        "\n",
        "reg_df['regime'] = eval_regime_tag(reg_df['Date'])\n",
        "\n",
        "regime_rows = []\n",
        "for regime, g in reg_df.groupby('regime'):\n",
        "    row = {\n",
        "        'regime': regime,\n",
        "        'n_days': len(g),\n",
        "        'champion_sharpe': eval_sharpe(g['champion_ret']),\n",
        "        'eqw_sharpe': eval_sharpe(g['eqw_ret']),\n",
        "        'champion_total_return': float((1.0 + g['champion_ret']).prod() - 1.0),\n",
        "        'eqw_total_return': float((1.0 + g['eqw_ret']).prod() - 1.0),\n",
        "    }\n",
        "    if g['sp500_ret'].notna().any():\n",
        "        row['sp500_sharpe'] = eval_sharpe(g['sp500_ret'])\n",
        "        row['sp500_total_return'] = float((1.0 + g['sp500_ret'].fillna(0.0)).prod() - 1.0)\n",
        "    else:\n",
        "        row['sp500_sharpe'] = np.nan\n",
        "        row['sp500_total_return'] = np.nan\n",
        "    regime_rows.append(row)\n",
        "\n",
        "eval_regime_df = pd.DataFrame(regime_rows).sort_values('regime').reset_index(drop=True)\n",
        "display(eval_regime_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) Statistical confidence: bootstrap Sharpe difference (champion - equal-weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_block_bootstrap_sharpe_diff(agent_ret, base_ret, n_boot=2000, block=20, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    a = np.asarray(agent_ret, dtype=float)\n",
        "    b = np.asarray(base_ret, dtype=float)\n",
        "    n = min(len(a), len(b))\n",
        "    a = a[:n]\n",
        "    b = b[:n]\n",
        "\n",
        "    def _sharpe(x):\n",
        "        x = pd.Series(x).dropna()\n",
        "        if len(x) < 2:\n",
        "            return np.nan\n",
        "        s = x.std(ddof=1)\n",
        "        if s <= 1e-12:\n",
        "            return np.nan\n",
        "        return np.sqrt(252.0) * x.mean() / s\n",
        "\n",
        "    diffs = []\n",
        "    n_blocks = int(np.ceil(n / block))\n",
        "    max_start = max(1, n - block + 1)\n",
        "\n",
        "    for _ in range(n_boot):\n",
        "        idx = []\n",
        "        for __ in range(n_blocks):\n",
        "            st = int(rng.integers(0, max_start))\n",
        "            idx.extend(range(st, min(st + block, n)))\n",
        "        idx = np.asarray(idx[:n])\n",
        "        d = _sharpe(a[idx]) - _sharpe(b[idx])\n",
        "        if np.isfinite(d):\n",
        "            diffs.append(float(d))\n",
        "\n",
        "    if not diffs:\n",
        "        return {'n_boot_eff': 0, 'mean': np.nan, 'ci_low': np.nan, 'ci_high': np.nan, 'p_le_zero': np.nan}\n",
        "\n",
        "    diffs = np.asarray(diffs)\n",
        "    return {\n",
        "        'n_boot_eff': int(len(diffs)),\n",
        "        'mean': float(np.mean(diffs)),\n",
        "        'ci_low': float(np.quantile(diffs, 0.025)),\n",
        "        'ci_high': float(np.quantile(diffs, 0.975)),\n",
        "        'p_le_zero': float(np.mean(diffs <= 0.0)),\n",
        "    }\n",
        "\n",
        "bootstrap_eqw = eval_block_bootstrap_sharpe_diff(\n",
        "    reg_df['champion_ret'].values,\n",
        "    reg_df['eqw_ret'].values,\n",
        "    n_boot=2000,\n",
        "    block=20,\n",
        "    seed=EVAL_RANDOM_SEED,\n",
        ")\n",
        "\n",
        "if reg_df['sp500_ret'].notna().any():\n",
        "    bootstrap_sp500 = eval_block_bootstrap_sharpe_diff(\n",
        "        reg_df['champion_ret'].values,\n",
        "        reg_df['sp500_ret'].fillna(0.0).values,\n",
        "        n_boot=2000,\n",
        "        block=20,\n",
        "        seed=EVAL_RANDOM_SEED,\n",
        "    )\n",
        "else:\n",
        "    bootstrap_sp500 = {'n_boot_eff': 0, 'mean': np.nan, 'ci_low': np.nan, 'ci_high': np.nan, 'p_le_zero': np.nan}\n",
        "\n",
        "print('Bootstrap Sharpe diff (Champion - EQW):')\n",
        "print(bootstrap_eqw)\n",
        "print('Bootstrap Sharpe diff (Champion - SP500):')\n",
        "print(bootstrap_sp500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15) Training-diagnostics quality checks from CSV metrics\n",
        "Uses saved CSVs to report KL stability, turnover drivers, and execution quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diag_report = {}\n",
        "\n",
        "if eval_latest_episodes_csv and Path(eval_latest_episodes_csv).exists():\n",
        "    ep = pd.read_csv(eval_latest_episodes_csv)\n",
        "    diag_report['episodes_rows'] = len(ep)\n",
        "\n",
        "    if 'approx_kl' in ep.columns:\n",
        "        kl = pd.to_numeric(ep['approx_kl'], errors='coerce').dropna()\n",
        "        if len(kl):\n",
        "            diag_report['approx_kl_mean'] = float(kl.mean())\n",
        "            diag_report['approx_kl_p50'] = float(kl.quantile(0.50))\n",
        "            diag_report['approx_kl_p90'] = float(kl.quantile(0.90))\n",
        "\n",
        "    if {'episode_turnover_pct', 'approx_kl'}.issubset(ep.columns):\n",
        "        x = pd.to_numeric(ep['episode_turnover_pct'], errors='coerce')\n",
        "        y = pd.to_numeric(ep['approx_kl'], errors='coerce')\n",
        "        valid = x.notna() & y.notna()\n",
        "        if valid.any():\n",
        "            diag_report['corr_turnoverpct_kl'] = float(np.corrcoef(x[valid], y[valid])[0, 1])\n",
        "\n",
        "if eval_latest_step_diag_csv and Path(eval_latest_step_diag_csv).exists():\n",
        "    sd = pd.read_csv(eval_latest_step_diag_csv)\n",
        "    diag_report['step_diag_rows'] = len(sd)\n",
        "\n",
        "    for col in ['l1_w_delta', 'turnover_penalty_contrib', 'tx_cost_contrib_reward_pts', 'action_realization_l1']:\n",
        "        if col in sd.columns:\n",
        "            s = pd.to_numeric(sd[col], errors='coerce').dropna()\n",
        "            if len(s):\n",
        "                diag_report[f'{col}_mean'] = float(s.mean())\n",
        "                diag_report[f'{col}_p90'] = float(s.quantile(0.90))\n",
        "\n",
        "print(json.dumps(diag_report, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16) Save final evaluation package\n",
        "Exports leaderboard, benchmark table, regime table, diagnostics, and champion metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_out_dir = EVAL_RESULTS_ROOT / 'logs'\n",
        "eval_out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "eval_ablation_path = eval_out_dir / f'final_eval_ablation_{ts}.csv'\n",
        "eval_benchmark_path = eval_out_dir / f'final_eval_benchmark_{ts}.csv'\n",
        "eval_regime_path = eval_out_dir / f'final_eval_regime_{ts}.csv'\n",
        "eval_diag_path = eval_out_dir / f'final_eval_diagnostics_{ts}.json'\n",
        "eval_meta_path = eval_out_dir / f'final_eval_champion_{ts}.json'\n",
        "\n",
        "eval_ablation_table.to_csv(eval_ablation_path, index=False)\n",
        "eval_benchmark_df.to_csv(eval_benchmark_path, index=False)\n",
        "eval_regime_df.to_csv(eval_regime_path, index=False)\n",
        "\n",
        "with open(eval_diag_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump({\n",
        "        'bootstrap_sharpe_diff_eqw': bootstrap_eqw,\n",
        "        'bootstrap_sharpe_diff_sp500': bootstrap_sp500,\n",
        "        'diagnostics': diag_report,\n",
        "        'metadata_path': str(EVAL_METADATA_PATH),\n",
        "    }, f, indent=2)\n",
        "\n",
        "with open(eval_meta_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump({\n",
        "        'champion_label': EVAL_CHAMPION_LABEL,\n",
        "        'selection_row': champion_row.to_dict(),\n",
        "        'deterministic_metrics': EVAL_CHAMPION.deterministic_metrics,\n",
        "        'checkpoint_description': EVAL_CHAMPION.checkpoint_description,\n",
        "    }, f, indent=2, default=str)\n",
        "\n",
        "print('‚úÖ Saved evaluation package:')\n",
        "print('-', eval_ablation_path)\n",
        "print('-', eval_benchmark_path)\n",
        "print('-', eval_regime_path)\n",
        "print('-', eval_diag_path)\n",
        "print('-', eval_meta_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
