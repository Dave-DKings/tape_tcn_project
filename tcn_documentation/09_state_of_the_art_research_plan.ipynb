{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TAPE-TCN State-of-the-Art Research Plan (Project Guide)\n",
        "\n",
        "**Project**: Adaptive Portfolio RL (TCN-only track)  \n",
        "**Scope**: End-to-end guide for ongoing research, engineering, and publication workflow  \n",
        "**Status**: Living plan aligned to current implementation in `adaptive_portfolio_rl/src`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Purpose of This Notebook\n",
        "\n",
        "This notebook is the master research guide for the TCN program. It is designed to:\n",
        "\n",
        "- keep experiments scientifically consistent,\n",
        "- connect implementation choices to theory,\n",
        "- enforce reproducible logging and evaluation,\n",
        "- provide a publication-ready structure for methods and results.\n",
        "\n",
        "It is intentionally practical: each research pillar maps to code, metrics, checkpoints, and decision criteria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Current Project Baseline (Code-Aligned)\n",
        "\n",
        "From current implementation:\n",
        "\n",
        "- **Assets**: 10 equities + cash (`MSFT, GOOGL, JPM, JNJ, XOM, PG, NEE, LIN, CAT, UNH`).\n",
        "- **Train/Test split**: train ends `2019-12-31`, OOS starts `2020-01-01`.\n",
        "- **Policy**: Dirichlet actor-critic with TCN variants (`TCN`, `TCN_ATTENTION`, `TCN_FUSION`).\n",
        "- **Reward**: Three-component TAPE reward with DSR/PBRS + turnover proximity + drawdown dual controller + terminal TAPE scaling.\n",
        "- **Current TCN defaults**: `tcn_filters=[32,64,64]`, `kernel=5`, `dilations=[2,4,8]`, `sequence_length=60`.\n",
        "- **Current PPO defaults**: `max_total_timesteps=100000`, `timesteps_per_ppo_update=250`, `ppo_epochs=5`, `batch_size=256`, `lr=7e-4`, `entropy_coef=0.01`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) The Seven Core Pillars\n",
        "\n",
        "1. **Risk-aware Reward Engineering (TAPE Core)**  \n",
        "2. **Actuarial Drawdown Intelligence**  \n",
        "3. **Dirichlet Simplex Policy Design**  \n",
        "4. **Temporal Modeling with TCN Family**  \n",
        "5. **Multi-source Feature System (TI + Macro + Fundamentals + Cross-sectional)**  \n",
        "6. **Execution Realism and Turnover Governance**  \n",
        "7. **Robust Evaluation, Reproducibility, and Deployment Readiness**\n",
        "\n",
        "These seven pillars are the backbone of the research narrative and should be preserved in all write-ups.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Mathematical System Summary\n",
        "\n",
        "### 4.1 Step Reward (Conceptual)\n",
        "\n",
        "\\[\n",
        "r_t = r_t^{\text{base}} + r_t^{\text{DSR/PBRS}} + r_t^{\text{turnover}} - r_t^{\text{drawdown-penalty}}\n",
        "\\]\n",
        "\n",
        "with terminal scaling term:\n",
        "\n",
        "\\[\n",
        "r_t \\leftarrow r_t \\cdot \\left(1 + \\kappa_{\text{terminal}}\\,\text{TAPE}_{\text{episode}}\right)\n",
        "\\]\n",
        "\n",
        "### 4.2 Turnover Proximity Target\n",
        "\n",
        "\\[\n",
        "\\Delta_t = |\tau_t - \tau^*|,\\quad\n",
        "r_t^{\text{turnover}} = \u0007lpha_{\tau}\\,\\max\\left(0, 1 - \frac{\\Delta_t}{b_{\tau}}\right)\n",
        "\\]\n",
        "\n",
        "where \\(\tau^*\\) is target turnover, \\(b_{\tau}\\) is tolerance band.\n",
        "\n",
        "### 4.3 Drawdown Dual Penalty\n",
        "\n",
        "\\[\n",
        "\text{DD}_t = 1 - \frac{V_t}{\\max_{s\\le t}V_s},\\quad\n",
        "r_t^{\text{drawdown-penalty}} = \\lambda_t\\,\\max(0, \text{DD}_t - d_{\text{trigger}})\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "\\lambda_{t+1} = \\Pi_{[\\lambda_{\\min},\\lambda_{\\max}]}\\left(\\lambda_t + \\eta_\\lambda\\,g_t\right)\n",
        "\\]\n",
        "\n",
        "### 4.4 Dirichlet Policy\n",
        "\n",
        "\\[\n",
        "\\mathbf{w}_t \\sim \text{Dirichlet}(\boldsymbol{\u0007lpha}_t),\\quad\n",
        "\u0007lpha_i > 0\n",
        "\\]\n",
        "\n",
        "Current alpha map is activation-controlled (e.g., `elu`, `softplus`, etc.), with dynamic epsilon floor:\n",
        "\n",
        "\\[\n",
        "\\epsilon_t = \\max(\\epsilon_{\\min},\\epsilon_{\\max}(1-\text{progress}_t))\n",
        "\\]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Pillar 1: Risk-aware Reward Engineering (TAPE Core)\n",
        "\n",
        "**Research Question**: Can a multi-objective reward improve risk-adjusted return stability over long OOS windows?  \n",
        "**Hypothesis**: TAPE will dominate raw-return reward on Sharpe/Sortino/MDD tradeoff.\n",
        "\n",
        "**Implementation anchors**:\n",
        "- `src/environment_tape_rl.py`\n",
        "- `src/reward_utils.py`\n",
        "- `src/profile_manager.py`\n",
        "\n",
        "**Primary KPIs**:\n",
        "- OOS Sharpe, OOS Sortino, OOS MDD\n",
        "- Turnover and win-rate stability\n",
        "- TAPE score consistency across checkpoints\n",
        "\n",
        "**Failure signs**:\n",
        "- TAPE high but Sharpe low in OOS,\n",
        "- reward-term dominance imbalance,\n",
        "- unstable dual variable dynamics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Pillar 2: Actuarial Drawdown Intelligence\n",
        "\n",
        "**Research Question**: Do actuarial drawdown-state features improve crisis robustness?  \n",
        "**Hypothesis**: Actuarial features reduce tail-risk and improve recovery behavior under stress regimes.\n",
        "\n",
        "**Implementation anchors**:\n",
        "- `src/actuarial.py`\n",
        "- `src/data_utils.py::add_actuarial_features`\n",
        "- `src/config.py::ACTUARIAL_PARAMS`\n",
        "\n",
        "**Actuarial feature family**:\n",
        "- `Actuarial_Expected_Recovery`\n",
        "- `Actuarial_Prob_30d`\n",
        "- `Actuarial_Prob_60d`\n",
        "- `Actuarial_Reserve_Severity`\n",
        "\n",
        "**KPIs**:\n",
        "- Drawdown depth/frequency,\n",
        "- recovery time from local troughs,\n",
        "- MDD stability across random start dates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Pillar 3: Dirichlet Simplex Policy Design\n",
        "\n",
        "**Research Question**: Does native-simplex action modeling improve allocation validity and optimization stability?  \n",
        "**Hypothesis**: Dirichlet actor reduces projection artifacts and yields cleaner weight dynamics.\n",
        "\n",
        "**Implementation anchors**:\n",
        "- `src/agents/actor_critic_tf.py`\n",
        "- `src/agents/ppo_agent_tf.py`\n",
        "\n",
        "**Decision knobs**:\n",
        "- `dirichlet_alpha_activation` (`elu` / `softplus` / alternatives)\n",
        "- `dirichlet_epsilon` annealing (`max`, `min`)\n",
        "- deterministic evaluation modes (`mode`, `mean`)\n",
        "\n",
        "**KPIs**:\n",
        "- action diversity (`action_uniques`),\n",
        "- concentration behavior (`argmax_alpha_uniques`),\n",
        "- turnover-quality relationship.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Pillar 4: Temporal Modeling with TCN Family\n",
        "\n",
        "**Research Question**: Which TCN variant best handles multi-regime portfolio control?  \n",
        "**Hypothesis**: Fusion and attention variants can improve regime sensitivity if regularized correctly.\n",
        "\n",
        "**Variants in code**:\n",
        "- `TCN`\n",
        "- `TCN_ATTENTION`\n",
        "- `TCN_FUSION`\n",
        "\n",
        "**Current receptive field guide** (causal stack):\n",
        "\n",
        "\\[\n",
        "R = 1 + \\sum_{b=1}^{B} 2(k-1)d_b\n",
        "\\]\n",
        "\n",
        "For \\(k=5\\), \\(d=[2,4,8]\\), \\(B=3\\):\n",
        "\n",
        "\\[\n",
        "R = 1 + 2\\cdot4\\cdot(2+4+8) = 113\n",
        "\\]\n",
        "\n",
        "This is larger than sequence length 60, so context saturation should be monitored.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Pillar 5: Multi-source Feature System\n",
        "\n",
        "**Research Question**: Can richer, economically grounded features improve cross-asset differentiation?  \n",
        "**Hypothesis**: Cross-sectional and fundamental features improve signal quality relative to pure technical inputs.\n",
        "\n",
        "**Feature domains**:\n",
        "- Technical indicators,\n",
        "- Macro/FRED features,\n",
        "- Fundamentals (quarterly aligned),\n",
        "- Regime features,\n",
        "- Cross-sectional rankings/z-scores,\n",
        "- Actuarial features.\n",
        "\n",
        "**Implementation anchors**:\n",
        "- `src/data_utils.py`\n",
        "- `src/build_fundamentals_from_csv.py`\n",
        "- `src/generate_fundamental_features.py`\n",
        "\n",
        "**Data integrity controls**:\n",
        "- warm-up and NaN handling before split,\n",
        "- train-only scaler fitting,\n",
        "- disabled-feature hard drop (no zombie columns).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Pillar 6: Execution Realism and Turnover Governance\n",
        "\n",
        "**Research Question**: Can the policy stay tradable under realistic frictions while preserving alpha?  \n",
        "**Hypothesis**: Turnover-aware reward with position and cash bounds improves deployability.\n",
        "\n",
        "**Execution controls**:\n",
        "- transaction cost model,\n",
        "- max single position,\n",
        "- minimum cash position,\n",
        "- turnover target + band + curriculum.\n",
        "\n",
        "**KPIs**:\n",
        "- daily turnover distribution,\n",
        "- turnover-adjusted Sharpe,\n",
        "- concentration drift,\n",
        "- net-vs-gross performance gap.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Pillar 7: Robust Evaluation and Reproducibility\n",
        "\n",
        "**Research Question**: Is the strategy robust across decision mode, start date, and checkpoint selection?  \n",
        "**Hypothesis**: Robust models retain rank across `det_mode`, `det_mean`, and stochastic tracks.\n",
        "\n",
        "**Required protocol**:\n",
        "- deterministic track 1: `det_mode`\n",
        "- deterministic track 2: `det_mean`\n",
        "- stochastic multi-run track: sampled actions\n",
        "\n",
        "**Mandatory logs**:\n",
        "- metadata JSON per run,\n",
        "- per-episode CSV,\n",
        "- summary CSV,\n",
        "- weight/action/alpha artifacts per evaluation track.\n",
        "\n",
        "**Selection rule (recommended)**:\n",
        "- choose by OOS metrics first,\n",
        "- use TAPE threshold as secondary filter,\n",
        "- avoid in-sample-only checkpoint selection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Workplan (Execution Roadmap)\n",
        "\n",
        "### Stage A \u2014 System Lock (Now)\n",
        "- Freeze baseline config and code hashes.\n",
        "- Validate data pipeline integrity and feature inventory.\n",
        "- Validate train/test date boundaries and scaler behavior.\n",
        "\n",
        "### Stage B \u2014 Variant Training Campaign\n",
        "- Run `TCN`, `TCN_ATTENTION`, `TCN_FUSION` under aligned budgets.\n",
        "- Keep reward and data controls fixed.\n",
        "- Capture unified logs and metadata.\n",
        "\n",
        "### Stage C \u2014 Robustness Campaign\n",
        "- Deterministic dual-mode + stochastic runs.\n",
        "- Random-start evaluations over full OOS.\n",
        "- Check horizon sensitivity and drawdown dynamics.\n",
        "\n",
        "### Stage D \u2014 Reporting Pack\n",
        "- Build result tables with placeholders replaced only by verified runs.\n",
        "- Produce portfolio behavior diagnostics (weights, alphas, turnover, drawdown paths).\n",
        "- Finalize manuscript-ready charts and methods narrative.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Decision Gates (Go / No-Go)\n",
        "\n",
        "A run is **Go** only if all hold:\n",
        "\n",
        "- OOS Sharpe meets target band,\n",
        "- OOS MDD stays under risk budget,\n",
        "- turnover within execution budget,\n",
        "- no leakage flags,\n",
        "- stable behavior across det/stochastic tracks.\n",
        "\n",
        "If any gate fails, freeze deployment claims and iterate model controls before further conclusions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) References Map from `related_works`\n",
        "\n",
        "This plan should cite and align with the project\u2019s local corpus in:\n",
        "`notebooks/documentation/related_works/`\n",
        "\n",
        "### Reward shaping and safe RL\n",
        "- *Policy Invariance under reward transformations - Theory and application to reward shaping.pdf*\n",
        "- *Potential-Based Reward Shaping in Reinforcement Learning.pdf*\n",
        "- *Improving the Effectiveness of Potential-Based Reward Shaping in Reinforcement Learning.pdf*\n",
        "- *Belief Reward Shaping in Reinforcement Learning.pdf*\n",
        "- *Benchmarking Potential Based Rewards for Learning Humanoid Locomotion.pdf*\n",
        "- *NeurIPS-2020-learning-to-utilize-shaping-rewards-a-new-approach-of-reward-shaping-Paper.pdf*\n",
        "- *NeurIPS-2022-exploration-guided-reward-shaping-for-reinforcement-learning-under-sparse-rewards-Paper-Conference.pdf*\n",
        "\n",
        "### Portfolio RL and risk-aware optimization\n",
        "- *Deep Reinforcement Learning for Optimal Portfolio Allocation-A Comparative Study with Mean-Variance Optimization.pdf*\n",
        "- *Risk-Adjusted Deep Reinforcement Learning for Portfolio Optimization A Multi-reward Approach.pdf*\n",
        "- *Risk-Sensitive Deep Reinforcement Learning for Portfolio Optimization.pdf*\n",
        "- *Deep Reinforcement Learning for Automated Stock Trading-An Ensemble Strategy.pdf*\n",
        "- *A Self-Rewarding Mechanism in Deep Reinforcement Learning for Trading Strategy Optimization.pdf*\n",
        "- *portfolio-optimization-through-a-multimodal-deep-reinforcement-learning-framework.pdf*\n",
        "\n",
        "### Dirichlet policy and simplex allocation\n",
        "- *Dirichlet policies for reinforced factor portfolios.pdf*\n",
        "- *A prescriptive Dirichlet power allocation policy with deep reinforcement Learning.pdf*\n",
        "- *A Selective Portfolio Management Algorithm with Off-Policy Reinforcement Learning Using Dirichlet Distribution.pdf*\n",
        "\n",
        "### TCN / attention / fusion architecture references\n",
        "- *TTSNet- Transformer\u2013Temporal Convolutional Network\u2013Self-Attention with Feature Fusion for Prediction.pdf*\n",
        "- *Graph convolutional recurrent networks for reward shaping in RL.pdf*\n",
        "\n",
        "### Activation design references\n",
        "- *An overview of the activation functions used in deep learning algorithms.pdf*\n",
        "- *Parametric RSigELU--a new trainable activation function for deep learning.pdf*\n",
        "- *AB-Swish-- A Novel Activation Function for Neural Networks in Hybrid Portfolio Optimization Frameworks.pdf*\n",
        "\n",
        "### Curriculum and training strategy\n",
        "- *curriculum learning.pdf*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15) Lame-Man Summary (Simple Version)\n",
        "\n",
        "- You are teaching a portfolio agent not just to make money, but to survive bad markets and trade realistically.\n",
        "- The project stands on 7 pillars: smart reward, actuarial safety, valid allocation math, strong TCN memory, rich features, real-world trading constraints, and strict evaluation.\n",
        "- Every experiment should be judged by **risk-adjusted performance and reliability**, not return alone.\n",
        "- If a model looks good only in one mode or one window, it is not ready.\n",
        "- This notebook is your operating manual: update it each time you change core settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Quick project-alignment check (run optionally)\n",
        "from src.config import PHASE1_CONFIG\n",
        "\n",
        "print('Architecture:', PHASE1_CONFIG['agent_params']['actor_critic_type'])\n",
        "print('TCN filters:', PHASE1_CONFIG['agent_params']['tcn_filters'])\n",
        "print('Kernel:', PHASE1_CONFIG['agent_params']['tcn_kernel_size'])\n",
        "print('Dilations:', PHASE1_CONFIG['agent_params']['tcn_dilations'])\n",
        "print('Dirichlet activation:', PHASE1_CONFIG['agent_params']['dirichlet_alpha_activation'])\n",
        "print('Train timesteps:', PHASE1_CONFIG['training_params']['max_total_timesteps'])\n",
        "print('PPO step/update:', PHASE1_CONFIG['training_params']['timesteps_per_ppo_update'])\n",
        "print('Tickers:', PHASE1_CONFIG['ASSET_TICKERS'])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}