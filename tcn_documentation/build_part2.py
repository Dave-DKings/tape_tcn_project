"""Clean and complete the TCN documentation"""
import json

# Load current notebook
nb_path = r"c:\Users\Owner\new_project\adaptive_portfolio_rl\tcn_documentation\03_tcn_architectures_and_variants_EXPANDED.ipynb"
with open(nb_path, 'r', encoding='utf-8') as f:
    nb = json.load(f)

# Remove duplicate cells (keep only first 4 cells: title, TOC, sec1, sec2, sec3)
nb["cells"] = nb["cells"][:4]

# Now add the remaining sections 4-10
# Section 4: TCN Variants
nb["cells"].append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## 4. TCN Variants Taxonomy <a id='section4'></a>\n",
        "\n",
        "This project implements **three TCN variants**, each with different complexity-expressiveness tradeof fs.\n",
        "\n",
        "### 4.1 Variant A: Plain TCN (`TCNActor`, `TCNCritic`)\n",
        "\n",
        "**Architecture**:\n",
        "```\n",
        "Input (batch, timesteps, features)\n",
        "  ↓\n",
        "TCNBlock_1 (filters=32, dilation=2)\n",
        "  ↓\n",
        "TCNBlock_2 (filters=64, dilation=4)\n",
        "  ↓\n",
        "TCNBlock_3 (filters=64, dilation=8)\n",
        "  ↓\n",
        "GlobalAveragePooling1D → (batch, 64)\n",
        "  ↓\n",
        "Dense(num_actions) → Dirichlet α\n",
        "```\n",
        "\n",
        "**Configuration**:\n",
        "- `actor_critic_type = \"TCN\"`\n",
        "- `tcn_filters = [32, 64, 64]`\n",
        "- `tcn_kernel_size = 5`\n",
        "- `tcn_dilations = [2, 4, 8]`\n",
        "\n",
        "**Best for**: Baseline, computational efficiency, interpretability\n",
        "\n",
        "**Receptive Field**: RF = 113 timesteps (exceeds sequence_length=60)\n",
        "\n",
        "### 4.2 Variant B: TCN + Attention (`TCNAttentionActor`, `TCNAttentionCritic`)\n",
        "\n",
        "**Architecture**:\n",
        "```\n",
        "Input (batch, timesteps, features)\n",
        "  ↓\n",
        "TCN Blocks (same as Plain TCN)\n",
        "  ↓\n",
        "Projection → (batch, timesteps, attention_dim=64)\n",
        "  ↓\n",
        "Multi-Head Self-Attention (4 heads)\n",
        "  ↓\n",
        "GlobalAveragePooling1D\n",
        "  ↓\n",
        "Dense → Dirichlet α\n",
        "```\n",
        "\n",
        "**Configuration**:\n",
        "- `actor_critic_type = \"TCN\"` + `use_attention = True` OR `actor_critic_type = \"TCN_ATTENTION\"`\n",
        "- `attention_heads = 4`\n",
        "- `attention_dim = 64`\n",
        "\n",
        "**Best for**: Learning temporal importance weighting, regime-dependent allocation\n",
        "\n",
        "**Tradeoff**: +15% parameters, +attention overhead, +interpretability (attention weights)\n",
        "\n",
        "### 4.3 Variant C: TCN + Fusion (`TCNFusionActor`, `TCNFusionCritic`)\n",
        "\n",
        "**Architecture** (Dual Pathway):\n",
        "```\n",
        "Input (batch, timesteps, features)\n",
        "  ↓\n",
        "┌──────────────────────┬──────────────────────┐\n",
        "│ Per-Asset Pathway    │ Global Pathway       │\n",
        "│                      │                      │\n",
        "│ Reshape by assets    │ (no reshape)         │\n",
        "│  ↓                   │  ↓                   │\n",
        "│ Shared TCN on each   │ TCN on full input    │\n",
        "│  ↓                   │  ↓                   │\n",
        "│ Time pooling         │ Time pooling         │\n",
        "│  ↓                   │  ↓                   │\n",
        "│ Project → embed_dim  │ Project → embed_dim  │\n",
        "│  ↓                   │                      │\n",
        "│ Cross-Asset Attention│                      │\n",
        "│  ↓                   │                      │\n",
        "│ Asset pooling        │                      │\n",
        "└──────────────────────┴──────────────────────┘\n",
        "                ↓\n",
        "        Gated Fusion:\n",
        "        gate = σ(W · [h_asset, h_global])\n",
        "        h = gate ⊙ h_asset + (1-gate) ⊙ h_global\n",
        "                ↓\n",
        "        Dense → Dirichlet α\n",
        "```\n",
        "\n",
        "**Configuration**:\n",
        "- `actor_critic_type = \"TCN\"` + `use_fusion = True`\n",
        "- `fusion_embed_dim = 128`\n",
        "- `fusion_attention_heads = 4`\n",
        "\n",
        "**Best for**: Capturing cross-asset relationships + global market context\n",
        "\n",
        "**Complexity**: Highest parameter count (~2x Plain TCN), most expressive\n",
        "\n",
        "### 4.4 Variant Comparison\n",
        "\n",
        "| Variant | Parameters | FLOPs/Step | Interpretability | Use Case |\n",
        "|---------|------------|------------|------------------|----------|\n",
        "| Plain TCN | Baseline | Baseline | Medium | Fast prototyping, baseline |\n",
        "| TCN+Attention | +15% | +20% | High (attn weights) | Regime detection |\n",
        "| TCN+Fusion | +100% | +150% | High (asset relationships) | Cross-asset strategy |\n",
        "\n",
        "**Implementation**: `src/agents/actor_critic_tf.py`\n",
        "\n",
        "**References**: Li et al. (2025) for fusion architecture [li2025ttsnet], André & Coqueret (2021) for Dirichlet portfolios [andre2021dirichlet]"
    ]
})

# Section 5: Multi-Head Self-Attention
nb["cells"].append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## 5. Multi-Head Self-Attention <a id='section5'></a>\n",
        "\n",
        "### 5.1 Attention Mechanism\n",
        "\n",
        "After TCN feature extraction, **multi-head self-attention** learns to weight timesteps by importance.\n",
        "\n",
        "**Query-Key-Value**:\n",
        "$$\n",
        "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
        "$$\n",
        "\n",
        "**Scaled Dot-Product Attention**:\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "**Multi-Head**:\n",
        "$$\n",
        "\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W_O\n",
        "$$\n",
        "\n",
        "where each head $i$ operates on a subspace:\n",
        "$$\n",
        "\\text{head}_i = \\text{Attention}(XW_Q^i, XW_K^i, XW_V^i)\n",
        "$$\n",
        "\n",
        "### 5.2 Why Attention After TCN?\n",
        "\n",
        "**TCN strengths**: Efficient temporal encoding with large receptive fields \n",
        "\n",
        "**TCN limitation**: Equal weighting across receptive field (via pooling)\n",
        "\n",
        "**Attention benefit**: Learn which timesteps matter most for current decision\n",
        "\n",
        "**Example**: During market crash, attention may focus on recent extreme moves; during calm periods, focus on longer-term trends.\n",
        "\n",
        "### 5.3 Implementation\n",
        "\n",
        "From `src/agents/actor_critic_tf.py::MultiHeadSelfAttention`:\n",
        "\n",
        "- **Heads**: 4 (default)\n",
        "- **Dimension**: 64 (default)\n",
        "- **Dropout**: 0.1\n",
        "- **Scale factor**: $1/\\sqrt{d_k}$ where $d_k = d_{\\text{model}} / h = 64/4 = 16$\n",
        "\n",
        "**Positional information**: Implicit via TCN's causal structure (no explicit positional encoding needed)\n",
        "\n",
        "**References**: Vaswani et al. (2017) for transformers, Li et al. (2025) for TCN-attention fusion [li2025ttsnet]"
    ]
})

# Save
with open(nb_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=2, ensure_ascii=False)

print(f"Cleaned duplicates. Total cells: {len(nb['cells'])}")
print("Added sections 4-5")
