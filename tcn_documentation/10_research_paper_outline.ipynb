{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Research Paper Outline: The TAPE-TCN Framework\n",
        "\n",
        "**Working Title**: *TAPE-TCN: Horizon-Agnostic Portfolio Optimization via Temporal Convolutional Networks and Actuarial Drawdown Control*\n",
        "\n",
        "**Target Venue**: NeurIPS (FinAI Workshop), ICAIF, or Quantitative Finance (Journal)\n",
        "**Status**: Draft Structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Abstract <a id='abstract'></a>\n",
        "\n",
        "**Context**: Deep Reinforcement Learning (DRL) for portfolio optimization often struggles with two key issues: (1) the credit-assignment problem in long-horizon financial returns, and (2) path-dependent tail risks (drawdowns).\n",
        "\n",
        "**Innovation**: We propose **TAPE-TCN**, a framework combining:\n",
        "1. **Temporal Convolutional Networks (TCN)**: Dilated causal convolutions for multi-scale sequence modeling.\n",
        "2. **TAPE Reward System**: A three-component step reward (Base + DSR/PBRS + Turnover proximity) with drawdown dual-penalty control and terminal TAPE utility.\n",
        "3. **Actuarial State Augmentation**: Survival-analysis-inspired drawdown recovery features.\n",
        "\n",
        "**Evaluation Plan**: We evaluate across deterministic (`mode`, `mean`) and stochastic tracks on a strict out-of-sample window (2020-01-01 to 2025-11-30), with all final performance metrics reported after full TCN variant runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction <a id='intro'></a>\n",
        "\n",
        "### 1.1 The Problem\n",
        "- Financial markets are noisy, non-stationary, and have low signal-to-noise ratios.\n",
        "- Standard RL objectives (maximize discounted cumulative return) under-handle **path-dependent risks** like drawdowns.\n",
        "- Practical deployment requires balancing return, risk, and transaction realism.\n",
        "\n",
        "### 1.2 The TAPE-TCN Solution\n",
        "- **TCN**: Parallelizable sequence modeling with stable gradients and controllable receptive field.\n",
        "- **Actuarial Intelligence**: Explicit drawdown recovery-state variables in the observation space.\n",
        "- **TAPE**: Daily reward shaping + terminal utility alignment for long-horizon portfolio health.\n",
        "\n",
        "### 1.3 Contributions\n",
        "1. Actuarial survival-style drawdown features integrated into a portfolio RL state pipeline.\n",
        "2. Three-component TAPE reward with drawdown dual-control and terminal alignment.\n",
        "3. Comparative study across TCN variants (`TCN`, `TCN_ATTENTION`, `TCN_FUSION`) under unified evaluation protocol.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Methodology: Temporal Convolutional Architecture <a id='method-tcn'></a>\n",
        "\n",
        "### 2.1 Dilated Causal Convolutions\n",
        "- **Causal**: Output at time $t$ depends only on inputs $x_{0:t}$.\n",
        "- **Dilated**: Filter $f$ applied with spacing $d$, enabling large temporal context.\n",
        "\n",
        "$$ (F *_d X)(x_t) = \\sum_{i=0}^{k-1} f(i) \\cdot x_{t - d i} $$\n",
        "\n",
        "### 2.2 Receptive Field Math (Current Stack)\n",
        "With two causal conv layers per residual block, receptive field is:\n",
        "\n",
        "$$ R = 1 + \\sum_b 2(k-1)d_b $$\n",
        "\n",
        "For kernel size $k=5$ and dilation stack $[2,4,8]$:\n",
        "\n",
        "$$ R = 1 + 2\\cdot(5-1)\\cdot(2+4+8) = 113 $$\n",
        "\n",
        "This provides broad temporal coverage for medium/long regime dynamics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Methodology: TAPE Reward System <a id='method-tape'></a>\n",
        "\n",
        "We decompose reward into dense step terms plus terminal utility.\n",
        "\n",
        "### 3.1 Step Reward (Dense)\n",
        "\n",
        "$$ r_t = r_t^{\text{base}} + r_t^{\text{DSR/PBRS}} + r_t^{\text{turnover}} - r_t^{\text{drawdown}} $$\n",
        "\n",
        "where drawdown term is dual-controlled:\n",
        "\n",
        "$$ r_t^{\text{drawdown}} = \\lambda_t \\cdot \\max(0, \text{DD}_t - d_{\text{trigger}}), \\quad\n",
        "\\lambda_{t+1}=\\Pi_{[\\lambda_{\\min},\\lambda_{\\max}]}\\left(\\lambda_t + \\eta_\\lambda g_t\right) $$\n",
        "\n",
        "- **DSR/PBRS**: Differential Sharpe-based potential shaping.\n",
        "- **Turnover proximity**: Reward near target turnover band; suppress excessive churn.\n",
        "- **Drawdown dual control**: Adaptive penalty intensity under drawdown stress.\n",
        "\n",
        "### 3.2 Terminal Utility (Sparse)\n",
        "At episode end, compute aggregate TAPE utility $S \\in [0,1]$ and apply:\n",
        "\n",
        "$$ r_T \\leftarrow r_T + \\Lambda \\cdot S_{\text{TAPE}} $$\n",
        "\n",
        "This aligns local actions with episode-level portfolio quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Methodology: Actuarial State Augmentation <a id='method-actuarial'></a>\n",
        "\n",
        "We inject survival-analysis-inspired drawdown features into state $s_t$.\n",
        "\n",
        "### 4.1 Core Idea\n",
        "Estimate recovery characteristics from historical drawdown episodes and expose them as real-time risk context.\n",
        "\n",
        "### 4.2 Implemented Features\n",
        "- `Actuarial_Expected_Recovery`\n",
        "- `Actuarial_Prob_30d`\n",
        "- `Actuarial_Prob_60d`\n",
        "- `Actuarial_Reserve_Severity`\n",
        "\n",
        "**Hypothesis**: These features help distinguish recoverable dips from persistent stress regimes, improving risk-aware allocation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Experimental Design <a id='experiments'></a>\n",
        "\n",
        "### 5.1 Data and Splits (Implementation-Aligned)\n",
        "- **Assets**: 10 US equities + cash.\n",
        "- **Training window**: 2011-01-01 to 2019-12-31.\n",
        "- **Out-of-sample (OOS)**: 2020-01-01 to 2025-11-30.\n",
        "\n",
        "### 5.2 Model Families in Scope\n",
        "1. **TCN** (core)\n",
        "2. **TCN_ATTENTION**\n",
        "3. **TCN_FUSION**\n",
        "\n",
        "External/legacy comparisons (if reported) should be clearly labeled as non-core implementation baselines.\n",
        "\n",
        "### 5.3 Planned Ablation Axes (for later execution)\n",
        "1. **Architecture**: TCN vs TCN_ATTENTION vs TCN_FUSION\n",
        "2. **Reward**: TAPE full vs return-only / reduced components\n",
        "3. **Features**: with vs without actuarial block\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results and Discussion <a id='results'></a>\n",
        "\n",
        "*(Placeholders only until full TCN variant campaign is completed.)*\n",
        "\n",
        "### 6.1 Performance Table (Template)\n",
        "| Model | Sharpe | Sortino | Max DD | Turnover |\n",
        "|-------|--------|---------|--------|----------|\n",
        "| TCN | TBD | TBD | TBD | TBD |\n",
        "| TCN_ATTENTION | TBD | TBD | TBD | TBD |\n",
        "| TCN_FUSION | TBD | TBD | TBD | TBD |\n",
        "\n",
        "### 6.2 Behavioral Analysis (To Fill)\n",
        "- Allocation concentration dynamics (weights/alphas)\n",
        "- Turnover governance behavior versus target band\n",
        "- Crisis-window behavior (2020, 2022, 2025 segments)\n",
        "- Deterministic (`mode`, `mean`) vs stochastic robustness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusion <a id='conclusion'></a>\n",
        "\n",
        "This study defines a risk-aware TCN framework for portfolio RL that integrates actuarial drawdown context, shaped reward engineering, and execution constraints. Final empirical conclusions will be made only after the full TCN variant and robustness campaign is completed under the unified logging protocol.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "Use canonical citations in `paper/references.bib`, and align with project-local reading set in:\n",
        "\n",
        "`notebooks/documentation/related_works/`\n",
        "\n",
        "Core citation groups:\n",
        "- Reward shaping / PBRS / safe RL\n",
        "- Portfolio RL and risk-sensitive optimization\n",
        "- Dirichlet policy for simplex allocation\n",
        "- TCN / attention / fusion sequence modeling\n",
        "- Activation function studies for stable alpha mapping\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}