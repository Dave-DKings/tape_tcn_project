{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN Architectures and Variants (Current)\n",
    "\n",
    "This notebook documents the TCN variants and their mathematical foundations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) TCN Block in This Codebase\n",
    "\n",
    "Each `TCNBlock` has two causal dilated Conv1D layers plus residual connection.\n",
    "\n",
    "For input sequence $x_t$:\n",
    "\n",
    "$$\n",
    "y_t^{(1)} = \\sigma\\left(\\sum_{i=0}^{k-1} W_i^{(1)}x_{t-id} + b^{(1)}\\right),\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t^{(2)} = \\sigma\\left(\\sum_{i=0}^{k-1} W_i^{(2)}y_{t-id}^{(1)} + b^{(2)}\\right),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{out}_t = \\text{ReLU}(y_t^{(2)} + r_t).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Available TCN Variants\n",
    "\n",
    "### Variant A: Plain TCN\n",
    "\n",
    "- `actor_critic_type = \"TCN\"`\n",
    "- `use_attention = False`\n",
    "- `use_fusion = False`\n",
    "- Actor/Critic: `TCNActor`, `TCNCritic`\n",
    "\n",
    "### Variant B: TCN + Attention (toggle path)\n",
    "\n",
    "- `actor_critic_type = \"TCN\"`\n",
    "- `use_attention = True`\n",
    "- Actor/Critic: `TCNAttentionActor`, `TCNAttentionCritic`\n",
    "\n",
    "### Variant C: TCN + Fusion (toggle path)\n",
    "\n",
    "- `actor_critic_type = \"TCN\"`\n",
    "- `use_fusion = True`\n",
    "- Uses fusion pathway implemented in current TCN stack.\n",
    "\n",
    "### Variant D: TCN_ATTENTION (explicit alias)\n",
    "\n",
    "- `actor_critic_type = \"TCN_ATTENTION\"`\n",
    "- Same attention actor/critic classes as Variant B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Attention Mathematics\n",
    "\n",
    "After TCN feature extraction and projection, multi-head self-attention is applied:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Receptive Field Formula\n",
    "\n",
    "This implementation has two Conv1D layers per block. With kernel size $k$ and dilation list $\\{d_j\\}$:\n",
    "\n",
    "$$\n",
    "\\text{RF} = 1 + 2(k-1)\\sum_j d_j.\n",
    "$$\n",
    "\n",
    "Current Phase 1 values:\n",
    "\n",
    "- $k = 5$\n",
    "- $d = [2, 4, 8]$\n",
    "\n",
    "$$\n",
    "\\text{RF} = 1 + 2(5-1)(2+4+8) = 113.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Dilation Implementation Detail\n",
    "\n",
    "Blocks are created via:\n",
    "\n",
    "- `zip(tcn_filters, tcn_dilations)`\n",
    "\n",
    "So block count is:\n",
    "\n",
    "$$\n",
    "N_{\\text{blocks}} = \\min\\left(|\\text{tcn\\_filters}|, |\\text{tcn\\_dilations}|\\right).\n",
    "$$\n",
    "\n",
    "Current active values produce 3 blocks (`[32,64,64]` with `[2,4,8]`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Dirichlet Head for TCN Variants\n",
    "\n",
    "All TCN variants output Dirichlet concentration parameters $\\alpha$ with configurable activation.\n",
    "\n",
    "Current default:\n",
    "\n",
    "- `dirichlet_alpha_activation = \"elu\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.config import PHASE1_CONFIG\n",
    "ap = PHASE1_CONFIG['agent_params']\n",
    "k = ap['tcn_kernel_size']\n",
    "d = ap['tcn_dilations']\n",
    "rf = 1 + 2*(k-1)*sum(d)\n",
    "print('Current TCN setup')\n",
    "print('  filters:', ap['tcn_filters'])\n",
    "print('  kernel:', k)\n",
    "print('  dilations:', d)\n",
    "print('  sequence_length:', ap['sequence_length'])\n",
    "print('  theoretical RF:', rf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
